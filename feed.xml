<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="https://snawarhussain.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://snawarhussain.com/" rel="alternate" type="text/html" /><updated>2025-04-30T08:31:15+00:00</updated><id>https://snawarhussain.com/feed.xml</id><title type="html">Snawar Hussain</title><subtitle></subtitle><author><name>Snawar Hussain</name></author><entry><title type="html">Policy Optimization with REINFORCE: A Deep Dive into Policy Gradients and related concepts</title><link href="https://snawarhussain.com/educational/reinforcement%20learning/Policy-Gradient-Methods-in-ReInforcement-Learning-Log-trick-for-gradient-computation/" rel="alternate" type="text/html" title="Policy Optimization with REINFORCE: A Deep Dive into Policy Gradients and related concepts" /><published>2024-08-30T00:00:00+00:00</published><updated>2024-08-30T00:00:00+00:00</updated><id>https://snawarhussain.com/educational/reinforcement%20learning/Policy-Gradient-Methods-in-ReInforcement-Learning-Log-trick-for-gradient-computation</id><content type="html" xml:base="https://snawarhussain.com/educational/reinforcement%20learning/Policy-Gradient-Methods-in-ReInforcement-Learning-Log-trick-for-gradient-computation/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>The REINFORCE algorithm is a classic method in policy gradient reinforcement learning. It directly optimizes the policy by maximizing the expected cumulative reward. To understand how REINFORCE uses stochastic gradient ascent to update policy parameters, we need to delve into the problem of computing gradients of the expected reward and the derivation of the policy gradient theorem.</p>

<p>The development of the REINFORCE algorithm and the associated policy gradient theorem was driven by a need to optimize policies in reinforcement learning (RL) environments where the outcome (reward) is uncertain and depends on a sequence of actions. The challenges in computing gradients directly from the expected reward and the eventual adoption of the log-derivative trick stem from deep insights into how learning from rewards works in probabilistic environments.</p>

<h2 id="historical-context-and-intuition">Historical Context and Intuition</h2>

<h3 id="the-challenge-of-policy-optimization-in-rl">The Challenge of Policy Optimization in RL</h3>

<p>Before policy gradient methods like REINFORCE were developed, many reinforcement learning approaches focused on value-based methods, like Q-learning, where the objective is to learn a value function that estimates the expected reward for a given state-action pair. However, these methods don’t directly optimize the policy itself.</p>

<p>Researchers realized that directly optimizing the policy—learning the parameters that dictate which actions to take—could offer a more direct approach to solving RL problems. The policy \(\pi_\theta(a \mid s)\) is a function that gives the probability of taking action \(a\) given state \(s\), and it is parameterized by \(\theta\).</p>

<p>The goal is to find the optimal policy parameters \(\theta^*\) that maximize the expected cumulative reward:</p>

\[J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ R(\tau) \right]\]

<p>Where:</p>

<ul>
  <li>\(\tau\) represents a trajectory (sequence of states, actions, and rewards).</li>
  <li>\(R(\tau)\) is the total reward of the trajectory \(\tau\).</li>
  <li>\(\pi_\theta\) is the policy parameterized by \(\theta\).</li>
  <li>\(\mathbb{E}_{\tau \sim \pi_\theta}\) denotes the expectation over trajectories sampled from the policy \(\pi_\theta\).</li>
</ul>

<p>The challenge is that \(J(\theta)\) is an expectation, and directly computing its gradient \(\nabla_\theta J(\theta)\) is difficult because the expectation involves a distribution over trajectories, which depends on the policy parameters \(\theta\). The trajectory distribution itself is a complex, implicit function of \(\theta\), making it non-trivial to compute the gradient directly.</p>

<p><a href="/gif/why-why-oh-thats-why-meme-sheldon-NLL1QB" title="why why oh that's why meme sheldon"><img src="https://i.makeagif.com/media/6-29-2021/NLL1QB.gif" alt="why why oh that's why meme sheldon" /></a></p>

<p>Ok, here I said a lot of things regarding the challenges of computing gradients of an expectation but what does it actually mean? Let’s break it down.</p>

<h2 id="but-why-is-computing-the-gradient-of-the-expected-reward-difficult">But Why is Computing the Gradient of the Expected Reward Difficult?</h2>

<p>The primary challenge is that the expected reward \(J(\theta)\) is an expectation over a distribution of trajectories, which are indirectly dependent on the policy parameters \(\theta\).</p>

<ol>
  <li><strong>Complex Dependency</strong>:
    <ul>
      <li>The expectation \(\mathbb{E}_{\tau \sim \pi_\theta}\) involves summing over all possible trajectories \(\tau\), each of which is a sequence of states and actions influenced by the policy \(\pi_\theta\).</li>
      <li>The distribution over trajectories itself depends on the policy parameters \(\theta\), making the expectation highly non-linear and difficult to differentiate directly.</li>
    </ul>
  </li>
  <li><strong>Infeasibility of Direct Differentiation</strong>:
    <ul>
      <li>To compute the gradient \(\nabla_\theta J(\theta)\) directly, you would need to compute how changes in \(\theta\) affect the entire distribution of trajectories and how these changes in turn affect the expected reward.</li>
      <li>For most practical problems, directly calculating this gradient is infeasible due to the combinatorial explosion of possible trajectories and the implicit, complex relationship between the policy parameters and the trajectories.</li>
    </ul>
  </li>
</ol>

<!-- gif centered -->
<iframe src="https://giphy.com/embed/3ELtfmA4Apkju" width="330" style="align-items: center;
justify-content: center" frameborder="0"></iframe>
<p>Okkkk….? But let’s take another step back and try to understand in more depth:</p>

<h3 id="understanding-the-infeasibility-of-direct-gradient-calculation">Understanding the Infeasibility of Direct Gradient Calculation</h3>

<h4 id="1-combinatorial-explosion-of-trajectories">1. Combinatorial Explosion of Trajectories</h4>

<p>Just as mentioned earlier, in reinforcement learning, a trajectory \(\tau\) refers to a sequence of states, actions, and rewards experienced by the agent from the start to the end of an episode. For example, a trajectory might look like this:</p>

\[\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \dots, s_T, a_T, r_T).\]

<ul>
  <li>
    <p><strong>Combinatorial Explosion</strong>: The number of possible trajectories grows exponentially with the length of the episode and the number of possible actions at each state. If an environment has \(S\) states, \(A\) actions, and episodes of length \(T\), the number of possible trajectories could be as large as \((S \times A)^T\). This rapid growth in the number of trajectories is what we mean by combinatorial explosion.</p>

    <ul>
      <li><strong>Example</strong>: Suppose an environment has 100 states, 10 possible actions per state, and an episode length of 50 steps. The number of possible trajectories is \((100 \times 10)^{50} = 10^{100}\), an astronomically large number.</li>
    </ul>
  </li>
  <li>
    <p><strong>Implicit Dependence on Policy Parameters</strong>: Each trajectory’s probability depends on the policy parameters \(\theta\). Changing \(\theta\) alters the probabilities of taking specific actions in given states, which in turn alters the distribution of trajectories. The relationship between \(\theta\) and the trajectories is implicit and complex because small changes in \(\theta\) can lead to different sequences of states and actions, which accumulate over time.</p>

    <ul>
      <li><strong>Implication</strong>: Calculating the gradient of the expected reward directly would require summing over all possible trajectories, each weighted by its probability under the current policy. This would involve evaluating and differentiating an enormous number of terms, which is computationally infeasible.</li>
    </ul>
  </li>
</ul>

<iframe src="https://giphy.com/embed/3CXVJrUB2cJfbDi7DF" width="400" height="480" style="" frameborder="0" class="giphy-embed" allowfullscreen=""></iframe>
<!-- put png image -->
<p>Now… I hope things are a bit more clear as to why computing the gradient of the expected reward is difficult. But how do we actually go about solving this problem? This is where the log-derivative trick and the REINFORCE algorithm come into play.</p>

<h2 id="the-log-derivative-trick">The Log-Derivative Trick</h2>

<p>When early researchers realized the challenge of directly computing \(\nabla_\theta J(\theta)\), they sought an alternative way to express the gradient that would be easier to work with. They turned to the field of statistical estimation, where the log-derivative (likelihood ratio) trick was already known.</p>

<iframe src="https://giphy.com/embed/iNQ2cIve8rUqI" width="400" style="" frameborder="0" class="giphy-embed" allowfullscreen=""></iframe>

<h3 id="derivation-of-the-log-derivative-trick">Derivation of the Log-Derivative Trick</h3>

<p>We’re starting with the goal of computing the gradient of the expected reward \(\mathbb{E}[R_t \mid \pi_\theta]\) with respect to the policy parameters \(\theta\).</p>

<p>The key steps in the derivation are:</p>

<ol>
  <li><strong>Expectation of the Reward</strong>:
    <ul>
      <li>
\[\nabla_\theta \mathbb{E}[R_t \mid \pi_\theta] = \nabla_\theta \sum_a \pi_\theta(a) \mathbb{E}[R_t \mid A_t = a]\]
      </li>
      <li>This expression shows the expectation as a sum over all possible actions \(a\), weighted by the probability \(\pi_\theta(a)\) of taking action \(a\) under the current policy.</li>
    </ul>
  </li>
  <li><strong>Introducing the \(q(a)\) Function</strong>:
    <ul>
      <li>\(q(a) = \mathbb{E}[R_t \mid A_t = a]\) is the expected reward when taking action \(a\).</li>
      <li>Substituting this into the original expression gives us:</li>
    </ul>

\[\nabla_\theta \sum_a \pi_\theta(a) q(a)\]
  </li>
  <li><strong>Gradient Expansion</strong>:
    <ul>
      <li>Using the product rule (since \(\pi_\theta(a)\) depends on \(\theta\)), we expand the gradient:
\(\nabla_\theta \sum_a \pi_\theta(a) q(a) = \sum_a q(a) \nabla_\theta \pi_\theta(a)\)</li>
    </ul>
  </li>
  <li><strong>Rewriting the Gradient Using \(\pi_\theta(a)\)</strong>:
    <ul>
      <li>Notice that \(\nabla_\theta \pi_\theta(a) = \pi_\theta(a) \frac{\nabla_\theta \pi_\theta(a)}{\pi_\theta(a)}\).</li>
      <li>This allows us to rewrite the gradient as:
 \(\sum_a q(a) \nabla_\theta \pi_\theta(a) = \sum_a \pi_\theta(a) q(a) \frac{\nabla_\theta \pi_\theta(a)}{\pi_\theta(a)}\)</li>
    </ul>
  </li>
  <li><strong>Factoring out \(\pi_\theta(a)\)</strong>:
    <ul>
      <li>The expression simplifies to:
\(\sum_a \pi_\theta(a) q(a) \frac{\nabla_\theta \pi_\theta(a)}{\pi_\theta(a)}\)</li>
      <li>
        <p>Here, we can see that \(\frac{\nabla_\theta \pi_\theta(a)}{\pi_\theta(a)}\) is the score function, often denoted as \(\nabla_\theta \log \pi_\theta(a)\).</p>
      </li>
      <li>This is still a sum over all possible actions \(a\). The term \(\pi_\theta(a) q(a)\) represents the contribution of action \(a\) to the expected reward, weighted by the likelihood of taking that action under the policy \(\pi_\theta\).</li>
    </ul>
  </li>
  <li>
    <p><strong>Transition to Expectation</strong>:</p>

    <ul>
      <li>Recognize that \(\pi_\theta(a)\) is the probability of taking action \(a\). Therefore, summing over all actions can be viewed as taking an expectation over actions sampled from the policy distribution \(\pi_\theta\).</li>
      <li>We replace the sum with an expectation over the distribution of actions \(A_t\) under policy \(\pi_\theta\):
  \(\sum_a \pi_\theta(a) \left[ q(a) \frac{\nabla_\theta \pi_\theta(a)}{\pi_\theta(a)} \right] = \mathbb{E}_{A_t \sim \pi_\theta} \left[ q(A_t) \frac{\nabla_\theta \pi_\theta(A_t)}{\pi_\theta(A_t)} \right]\)
      - Here, \(q(A_t) = R_t\) is the reward associated with taking action \(A_t\).</li>
    </ul>
  </li>
  <li><strong>Finally:</strong>
\(\mathbb{E}_{A_t \sim \pi_\theta} \left[ R_t \frac{\nabla_\theta \pi_\theta(A_t)}{\pi_\theta(A_t)} \right]\)</li>
</ol>

<p>The log-derivative trick allows us to express the gradient of the expectation in a different form that is more tractable:</p>

\[\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ R(\tau) \nabla_\theta \log \pi_\theta(\tau) \right]\]

<h3 id="benefits-of-this-transformation">Benefits of This Transformation</h3>

<ol>
  <li><strong>Sampling-Based Estimation</strong>:
    <ul>
      <li><strong>Original Problem</strong>: Directly computing \(\nabla_\theta J(\theta)\) would require summing over all possible trajectories and computing their derivatives, which is infeasible.</li>
      <li>
        <p><strong>Transformed Problem</strong>: The new expression is an expectation that we can estimate by sampling trajectories from the policy. Instead of needing to consider all possible trajectories, we can generate a few sample trajectories \(\tau \sim \pi_\theta\) and use these samples to approximate the gradient. We turn the problem of computing a complicated expectation over an enormous set into a simpler problem of estimating that expectation by sampling. This is much more feasible in practice, especially in environments where the state and action spaces are large.</p>
      </li>
      <li><strong>Formally</strong>: By expressing the gradient as an expectation, we can approximate it using Monte Carlo methods. We only need to sample actions from the policy, compute the reward, and update the policy based on those samples.</li>
    </ul>
  </li>
  <li><strong>Gradient Computation Becomes Tractable</strong>:
    <ul>
      <li>The transformed expression \(\mathbb{E}_{\tau \sim \pi_\theta} \left[ R(\tau) \nabla_\theta \log \pi_\theta(\tau) \right]\) can be computed using the chain rule on the policy network. This is because \(\nabla_\theta \log \pi_\theta(\tau)\) is just the gradient of the log-probability of the trajectory, which is directly computable if we know the policy \(\pi_\theta\).
Therefore, we can do <strong>stochastic gradient ascent</strong> in the space of policy parameters.</li>
    </ul>
  </li>
</ol>

<h2 id="bonus-lesson-on-monte-carlo-and-gradient-estimation">Bonus Lesson on Monte Carlo and Gradient Estimation</h2>

<p>Monte Carlo (MC) methods are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. In the context of reinforcement learning (RL) and specifically in policy gradient methods like REINFORCE, Monte Carlo methods are used to estimate the expectation of a function with respect to a probability distribution.</p>

<h3 id="what-happens-when-you-sample">What Happens When You Sample?</h3>

<ol>
  <li><strong>Expectation of a Function</strong>:
    <ul>
      <li>Suppose you have a random variable \(X\) with a probability distribution \(p(X)\), and you want to compute the expected value of a function \(f(X)\) under this distribution:
\(\mathbb{E}_{X \sim p(X)}[f(X)] = \int f(X) p(X) dX\)</li>
      <li>In practice, for complex functions or high-dimensional spaces, computing this expectation analytically is difficult or impossible.</li>
    </ul>
  </li>
  <li><strong>Monte Carlo Estimation</strong>:
    <ul>
      <li>Instead of calculating the integral directly, you can estimate the expectation by taking random samples \(X_i\) from the distribution \(p(X)\) and averaging the function values:
\(\mathbb{E}_{X \sim p(X)}[f(X)] \approx \frac{1}{N} \sum_{i=1}^N f(X_i)\)</li>
      <li>As \(N\) (the number of samples) increases, the average converges to the true expectation due to the Law of Large Numbers.</li>
    </ul>
  </li>
</ol>

<h3 id="applying-this-to-policy-gradients">Applying This to Policy Gradients</h3>

<p>In the context of the policy gradient theorem:</p>

<ol>
  <li><strong>Gradient of the Expected Reward</strong>:
    <ul>
      <li>The gradient of the expected reward can be expressed as:
\(\nabla_\theta J(\theta) = \mathbb{E}_{A_t \sim \pi_\theta} \left[ R_t \frac{\nabla_\theta \pi_\theta(A_t)}{\pi_\theta(A_t)} \right]\)</li>
      <li>This expectation is over the distribution of actions \(A_t\) given by the policy \(\pi_\theta\).</li>
    </ul>
  </li>
  <li><strong>Sampling from the Policy</strong>:
    <ul>
      <li>To estimate this gradient, you don’t need to evaluate every possible action (which would be computationally infeasible in large or continuous action spaces).</li>
      <li>Instead, you draw samples of actions \(A_t\) from the policy \(\pi_\theta\), compute the associated reward \(R_t\) for those actions, and then compute the policy gradient for those specific samples.</li>
    </ul>
  </li>
  <li><strong>Monte Carlo Estimation of the Gradient</strong>:
    <ul>
      <li>The policy gradient can be approximated by averaging the gradients computed for each sample:
\(\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N R_{t,i} \frac{\nabla_\theta \pi_\theta(A_{t,i})}{\pi_\theta(A_{t,i})}\)</li>
      <li>Here, \(A_{t,i}\) are the actions sampled from the policy \(\pi_\theta\), and \(R_{t,i}\) are the corresponding rewards.</li>
      <li>As you draw more samples, the average of these gradients converges to the true gradient of the expected reward, allowing you to use this approximation to update the policy parameters \(\theta\).</li>
    </ul>
  </li>
</ol>

<h2 id="key-takeaways">Key Takeaways:</h2>
<ul>
  <li>
    <p><strong>Why It Works</strong>: By expressing the gradient as an expectation, you transform a potentially intractable sum (over all possible actions) into something you can approximate using a manageable number of samples. The Monte Carlo method is powerful because it allows you to estimate this expectation (and thus the gradient) by averaging over a relatively small number of samples from the policy.</p>
  </li>
  <li>
    <p><strong>Key Insight</strong>: The crucial insight is that you don’t need to consider every possible action at every step. Instead, by randomly sampling actions according to the policy and averaging the results, you get an unbiased estimate of the true gradient. This is what enables policy gradient methods to scale to complex, high-dimensional action spaces.</p>
  </li>
  <li>
    <p><strong>Implication for Learning</strong>: This approach is why we can effectively use stochastic gradient ascent to optimize policies in reinforcement learning. Even though each step in the optimization process is based on a noisy estimate of the gradient, as long as you sample enough and follow the gradient on average, the policy will improve over time.</p>
  </li>
</ul>

<h2 id="code-implementation">Code Implementation</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gymnasium</span> <span class="k">as</span> <span class="n">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="kn">import</span> <span class="n">Categorical</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">relu</span><span class="p">,</span> <span class="n">softmax</span>
<span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="kn">import</span> <span class="n">Categorical</span>


<span class="k">class</span> <span class="nc">PolicyNetwork</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Policy Network that defines the policy π_θ(a | s) as a neural network.
    This network takes in the state as input and outputs a probability distribution
    over actions.
    
    Args:
    - state_dim: Dimensionality of the state space.
    - action_dim: Dimensionality of the action space.
    - hidden_dim: Number of hidden units in the hidden layer.
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PolicyNetwork</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="c1"># First fully connected layer
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="c1"># Second fully connected layer (output layer)
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="s">"""
        Forward pass through the network. The output is a probability distribution
        over actions, given the input state.

        This corresponds to computing π_θ(a | s).
        """</span>
        <span class="c1"># Apply ReLU activation to the first layer's output
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="c1"># Compute action probabilities using softmax
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Output π_θ(a | s)
</span>
    <span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policy_net</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="s">"""
        Select an action based on the current policy. This involves sampling from the
        distribution π_θ(a | s) and returning both the action and its log-probability.

        Args:
        - policy_net: The policy network (self).
        - state: The current state of the environment.

        Returns:
        - action: The action sampled from the policy.
        - log_prob: The log-probability of the selected action.
        """</span>
        <span class="c1"># Convert state to tensor
</span>        <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">state</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span>
        <span class="c1"># Get action probabilities from the policy network
</span>        <span class="n">probs</span> <span class="o">=</span> <span class="n">policy_net</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="c1"># Create a categorical distribution over actions
</span>        <span class="n">m</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
        <span class="c1"># Sample an action from the distribution
</span>        <span class="n">action</span> <span class="o">=</span> <span class="n">m</span><span class="p">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="c1"># Return the action and its log-probability (log π_θ(a | s))
</span>        <span class="k">return</span> <span class="n">action</span><span class="p">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">m</span><span class="p">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">compute_returns</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">):</span>
        <span class="s">"""
        Compute the return R(τ) for each time step in a trajectory. This involves
        calculating the cumulative discounted reward.

        Args:
        - rewards: List of rewards obtained during the episode.
        - gamma: Discount factor.

        Returns:
        - returns: List of discounted returns for each time step.
        """</span>
        <span class="n">R</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">returns</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Iterate over rewards in reverse order to calculate returns
</span>        <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">rewards</span><span class="p">):</span>
            <span class="n">R</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">R</span>
            <span class="n">returns</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">R</span><span class="p">)</span>  <span class="c1"># Insert at the beginning
</span>        <span class="k">return</span> <span class="n">returns</span>  <span class="c1"># This is R(τ) in the theory
</span>
<span class="c1"># Set up the environment
</span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">'CartPole-v1'</span><span class="p">)</span>

<span class="c1"># Set the seed for reproducibility
</span><span class="n">torch</span><span class="p">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Initialize the policy network with state and action dimensions
</span><span class="n">state_dim</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">action_dim</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span>
<span class="n">policy_net</span> <span class="o">=</span> <span class="n">PolicyNetwork</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">policy_net</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">num_episodes</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">):</span>
    <span class="s">"""
    Train the policy network using the REINFORCE algorithm.

    Args:
    - policy_net: The policy network to be trained.
    - optimizer: The optimizer to update the network parameters.
    - num_episodes: Number of episodes to train the network.
    - gamma: Discount factor for future rewards.
    """</span>
    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>
        <span class="c1"># Reset the environment to get the initial state
</span>        <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">log_probs</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Store log π_θ(a | s)
</span>        <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Store rewards
</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1500</span><span class="p">):</span>  <span class="c1"># Cap episode length
</span>            <span class="c1"># Select action and get its log-probability
</span>            <span class="n">action</span><span class="p">,</span> <span class="n">log_prob</span> <span class="o">=</span> <span class="n">policy_net</span><span class="p">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">policy_net</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
            <span class="c1"># Take the action in the environment
</span>            <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="o">*</span><span class="n">res</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="c1"># Store log-probability and reward
</span>            <span class="n">log_probs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_prob</span><span class="p">)</span>
            <span class="n">rewards</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                <span class="k">break</span>  <span class="c1"># End episode if done
</span>
        <span class="c1"># Compute returns R(τ) from rewards
</span>        <span class="n">returns</span> <span class="o">=</span> <span class="n">policy_net</span><span class="p">.</span><span class="n">compute_returns</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
        <span class="n">returns</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">returns</span><span class="p">)</span>

        <span class="c1"># Normalize returns (optional but helps with stability)
</span>        <span class="n">returns</span> <span class="o">=</span> <span class="p">(</span><span class="n">returns</span> <span class="o">-</span> <span class="n">returns</span><span class="p">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">returns</span><span class="p">.</span><span class="n">std</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>

        <span class="c1"># Calculate the loss: -sum(log π_θ(a | s) * R(τ))
</span>        <span class="c1"># MAXIMIZING SOMETHING = MINIMIZING ITS NEGATIVE 
</span>        <span class="c1">#(hence we are still doing gradient ascent)
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">log_probs</span><span class="p">)</span> <span class="o">*</span> <span class="n">returns</span><span class="p">)</span>

        <span class="c1"># Perform backpropagation to compute gradients
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>  <span class="c1"># Clear existing gradients
</span>        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># Compute gradients w.r.t. policy parameters
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># Update policy parameters θ
</span>
        <span class="c1"># Log progress every 100 episodes
</span>        <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Episode </span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s">, Total Reward: </span><span class="si">{</span><span class="nb">sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    
    <span class="c1"># Save the trained policy network
</span>    <span class="n">torch</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">policy_net</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s">'policy_net.pth'</span><span class="p">)</span>
    

<span class="c1"># Initialize optimizer (Adam) with learning rate
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">policy_net</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Train the policy network using REINFORCE
</span><span class="n">train</span><span class="p">(</span><span class="n">policy_net</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">num_episodes</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">test_policy</span><span class="p">(</span><span class="n">policy_net</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">num_episodes</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="s">"""
    Test the trained policy network by running it in the environment.

    Args:
    - policy_net: The trained policy network.
    - env: The environment to test in.
    - num_episodes: Number of episodes to run the policy.
    """</span>
    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>
        <span class="c1"># Reset environment for each episode
</span>        <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">env</span><span class="p">.</span><span class="n">render</span><span class="p">()</span>  <span class="c1"># Render the environment
</span>            <span class="c1"># Select action based on the trained policy
</span>            <span class="n">action</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">policy_net</span><span class="p">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">policy_net</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
            <span class="c1"># Take the action and observe the outcome
</span>            <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Test Episode </span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s">, Total Reward: </span><span class="si">{</span><span class="n">total_reward</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="n">env</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>

<span class="c1"># Reinitialize the environment for testing with rendering
</span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">'CartPole-v1'</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="s">'human'</span><span class="p">)</span>
<span class="n">test_policy</span><span class="p">(</span><span class="n">policy_net</span><span class="p">,</span> <span class="n">env</span><span class="p">)</span>
</code></pre></div></div>
<!-- gif of the trained model that can balance cartpole -->
<figure class="styled-figure">
  <img src="/assets/images/reinforce/cartpole.gif" height="400" />
  <figcaption>The neural network manages to learn a policy to map state into actions that can balance the CartPole</figcaption>
</figure>]]></content><author><name>Snawar Hussain</name></author><category term="Educational" /><category term="Reinforcement Learning" /><category term="REINFORCE" /><category term="Policy Gradients" /><category term="Monte Carlo Methods" /><category term="Stochastic Gradient Ascent" /><category term="Reinforcement Learning" /><category term="Machine Learning" /><category term="Deep Learning" /><category term="AI" /><summary type="html"><![CDATA[Discover how the REINFORCE algorithm leverages policy gradients, the log-trick, and Monte Carlo sampling to optimize decision-making in reinforcement learning environments. This guide offers a detailed walkthrough for understanding and implementing REINFORCE with Python and PyTorch.]]></summary></entry><entry><title type="html">Taming the LLaMA: A Guide to Herding LLaMA 3.1 with Hugging Face</title><link href="https://snawarhussain.com/educational/llms/Taming-the-LLaMA-3.1/" rel="alternate" type="text/html" title="Taming the LLaMA: A Guide to Herding LLaMA 3.1 with Hugging Face" /><published>2024-08-14T00:00:00+00:00</published><updated>2024-08-14T00:00:00+00:00</updated><id>https://snawarhussain.com/educational/llms/Taming-the-LLaMA-3.1</id><content type="html" xml:base="https://snawarhussain.com/educational/llms/Taming-the-LLaMA-3.1/"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>The Meta Llama 3.1 is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out).</p>

<p><strong>Model Release Date:</strong> July 23, 2024.</p>

<p>When it comes to loading and fine-tuning LLMs using various tools and libraries. Hugging Face, with its extensive API, offers multiple methods to load and interact with these LLMs, each tailored to different needs. However, this flexibility can also be confusing.</p>

<figure class="styled-figure">
  <img src="/assets/images/llama/llama_arch.png" height="400" />
  <figcaption>LLama model architecture: The Llama 2, 3 and 3.1 share the same model architecture but differ in parameters values i.e context length, vocab size, etc. Source:https://github.com/hkproj/pytorch-llama-notes </figcaption>
</figure>

<link rel="stylesheet" href="/assets/css/atten_mech/style.css" />

<h2 id="using-hugging-faces-pipelines">Using Hugging Face’s <code class="language-plaintext highlighter-rouge">Pipelines</code></h2>

<p>This is the simplest and easiest way when you need to get up and running with LLaMA 3.1 as quickly as possible, Hugging Face’s pipeline API is your go-to. This method abstracts away most of the complexity, allowing you to focus on the task at hand, whether it’s text generation, sentiment analysis, or translation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="s">"meta-llama/Meta-Llama-3.1-8B"</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">transformers</span><span class="p">.</span><span class="n">pipeline</span><span class="p">(</span>
    <span class="s">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_id</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s">"torch_dtype"</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">},</span> <span class="n">device_map</span><span class="o">=</span><span class="s">"auto"</span>
<span class="p">)</span>

<span class="n">pipeline</span><span class="p">(</span><span class="s">"The reason why bigbang happend is"</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Loading checkpoint shards: 100%|██████████| 4/4 [00:01&lt;00:00,  2.95it/s],
"[{'generated_text': 'The reason why bigbang happend is because of the big bang theory. The big bang theory is a theory that explains the origin of the universe. The theory was first proposed by the Belgian priest Georges Lemaître in the 1920s. The big bang theory is based on the idea that the universe began as a small, hot, dense point and then expanded and cooled to form the universe we see today.\\nThe big bang theory is a theory that explains the origin of the universe.'}]"
</code></pre></div></div>
<p>The pipeline API automatically handles model loading, tokenization, and inference, making it ideal for developers who need results fast without diving into the nitty-gritty details of model configuration.</p>

<p><strong>Class Used:</strong> <code class="language-plaintext highlighter-rouge">transformers.pipeline</code>
<strong>Ideal For:</strong> Quick prototyping, minimal setup, and rapid testing.
<strong>Limitations:</strong> Limited flexibility in customizing model behavior. Not suited for fine-tuning or large-scale training.</p>

<p>HuggingFace will automatically download the model weights and everything necessary,
the model weights are usually stored in <code class="language-plaintext highlighter-rouge">~/.cache/huggingface/hub/</code> directory.</p>

<h2 id="using-automodelforcausallm">Using <code class="language-plaintext highlighter-rouge">AutoModelForCausalLM</code>:</h2>

<p>When it comes to fine-tuning a pre-trained model like LLaMA 3.1, the <code class="language-plaintext highlighter-rouge">AutoModelForCausalLM</code> class from Hugging Face can be regarded as a good choice. This class is designed to handle causal language models, and it makes the process of loading and configuring the model straightforward. One of the key advantages of using <code class="language-plaintext highlighter-rouge">AutoModelForCausalLM</code> is that it automatically reads the model’s JSON configuration file, setting up the architecture based on the parameters defined during pre-training.</p>

<h3 id="why-use-automodelforcausallm-for-fine-tuning">Why Use <code class="language-plaintext highlighter-rouge">AutoModelForCausalLM</code> for Fine-Tuning?</h3>

<ul>
  <li><strong>Automatic Configuration</strong>: Hugging Face’s <code class="language-plaintext highlighter-rouge">AutoModelForCausalLM</code> automatically loads the model’s architecture from the JSON configuration. This means you don’t have to manually specify the model’s layers, attention heads, or other parameters; it’s all taken care of for you.</li>
  <li><strong>Flexibility for Fine-Tuning</strong>: Since the model is loaded with all its pre-trained parameters intact, you can easily fine-tune it on new data with minimal changes. This makes it a great choice when you need to adapt LLaMA 3.1 to a specific task or domain.</li>
</ul>

<h3 id="example-of-loading-with-automodelforcausallm">Example of Loading with <code class="language-plaintext highlighter-rouge">AutoModelForCausalLM</code></h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="s">"meta-llama/Meta-Llama-3.1-8B"</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span>
 <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">,</span>
<span class="n">attn_implementation</span><span class="o">=</span><span class="s">"flash_attention_2"</span><span class="p">,)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>

<span class="c1"># Preparing input text for fine-tuning
</span><span class="n">input_text</span> <span class="o">=</span> <span class="s">"Fine-tuning LLaMA on new data is straightforward with AutoModelForCausalLM."</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">).</span><span class="n">input_ids</span>

<span class="c1"># Model is now ready for fine-tuning
</span>
</code></pre></div></div>
<p>This model is now ready for fine-tuning on new data. The <code class="language-plaintext highlighter-rouge">AutoModelForCausalLM</code> class has loaded the pre-trained LLaMA 3.1 model with the specified configuration, and the tokenizer is set up to process input text for inference or fine-tuning. One can write their own training loop utilizing packages like <code class="language-plaintext highlighter-rouge">PEFT</code> and <code class="language-plaintext highlighter-rouge">accelerate</code> for efficient model training and fine-tuning.</p>

<h2 id="quantized-loading-with-automodelforcausallm-and-bitsandbytes-efficient-inference">Quantized Loading with <code class="language-plaintext highlighter-rouge">AutoModelForCausalLM</code> and <code class="language-plaintext highlighter-rouge">BitsAndBytes</code>: Efficient Inference</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="s">"meta-llama/Meta-Llama-3.1-8B"</span>

<span class="n">quant_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span><span class="n">load_in_8bit</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_id</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">quant_config</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="n">attn_implementation</span><span class="o">=</span><span class="s">"flash_attention_2"</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards: 100%|██████████| 4/4 [00:03&lt;00:00,  1.11it/s]
</code></pre></div></div>
<p>We can print the model architecture.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear8bitLt(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
</code></pre></div></div>

<p>For running inference with the quantized model, we can use the similar code as before.:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">input_text</span> <span class="o">=</span> <span class="s">"The reason why bigbang happend is "</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">generation_config</span><span class="p">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token_id</span>

<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">).</span><span class="n">input_ids</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span><span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The reason why bigbang happend is 1. gravity 2. mass 3. energy 4. light 5. space 6. time 7. matter 8. dark matter 9. dark energy 10. space-time 11. mass-energy 12. mass-energy-time 13. mass-energy-time-space 14. mass-energy-time-space-dark matter 15. mass-energy-time-space-dark matter-dark energy 16. mass-energy-time-space-dark matter-dark
</code></pre></div></div>
<p>you can read the documentations for more details on the <code class="language-plaintext highlighter-rouge">generate</code> method and its parameters.</p>

<p>Since we are using the pre-trained LLama 3.1 which was not fine-tuned for anything specific, the output might not be coherent or relevant to the input text.</p>

<blockquote>
  <p>The quantized model is now ready for efficient inference with reduced memory usage and faster execution times.</p>
</blockquote>

<div style="border-left: 5px solid #007bff; padding: 10px; margin: 20px 0;">
Here in these examples we are using <b>Meta-Llama-3.1-8B</b> model and not the <b>Meta-Llama-3.1-8B-intstruct</b> model, the latter one is fine-tuned with RLHF and is more suitable for answering questions and generating coherent responses. The former is mainly for fine-tuning on your own data and tasks since it is not fine-tuned for any specific task.
</div>

<h2 id="loading-llama-with-local-weights">Loading LLama with local weights:</h2>

<p>If you have model weight available locally, for example you downloaded the orignal .pth file from the Meta repo for LLama 3.1 and converted them into Hugging face weights locally and would like to load from there. Then you can use the <code class="language-plaintext highlighter-rouge">from_pretrained</code> method with the <code class="language-plaintext highlighter-rouge">local_files_only</code> parameter set to <code class="language-plaintext highlighter-rouge">True</code>. This will ensure that the model is loaded from the local directory and not downloaded from the Hugging Face model hub.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">local_model_path</span> <span class="o">=</span> <span class="s">"~/.cache/huggingface/hub/DirectorytoModelWeights"</span>
<span class="n">local_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">local_model_path</span><span class="p">,</span>
                                                   <span class="n">local_files_only</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                                   <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">,)</span>

</code></pre></div></div>

<h2 id="using-llamaconfig-and--llamaforcausallm">Using <code class="language-plaintext highlighter-rouge">LlamaConfig</code> and  <code class="language-plaintext highlighter-rouge">LlamaForCausalLM</code>:</h2>
<p>LlamaConfig is a configuration class that allows you to define the architecture and parameters of the LLaMA model before loading it. This class gives you the flexibility to customize aspects like the number of layers, attention heads, and more.</p>

<p>Why Use It?
Customization: Allows for tailored model architectures and hyperparameters.
Research and Experimentation: Ideal for exploring different model configurations. Adding enhancement or modifications to the model architecture.</p>

<p>The famous example of using <code class="language-plaintext highlighter-rouge">LlamaConfig</code> is the the repo <a href="https://github.com/jquesnelle/yarn.git">YARN</a> where authors loaded LLama model with <code class="language-plaintext highlighter-rouge">LlamaConfig</code> and <code class="language-plaintext highlighter-rouge">LlamaForCausalLM</code> with pretrained weights and changed the RoPe (Rotary Positional Embeddings) with their proposed YARN (Yet Another Rotary Positional Embedding) to improve the context window of the model.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">LlamaConfig</span><span class="p">,</span> <span class="n">LlamaForCausalLM</span><span class="p">,</span><span class="n">AutoTokenizer</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="n">params</span><span class="o">=</span> <span class="p">{</span>

  <span class="s">"attention_bias"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
  <span class="s">"attention_dropout"</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
  <span class="s">"bos_token_id"</span><span class="p">:</span> <span class="mi">128000</span><span class="p">,</span>
  <span class="s">"eos_token_id"</span><span class="p">:</span> <span class="mi">128001</span><span class="p">,</span>
  <span class="s">"hidden_act"</span><span class="p">:</span> <span class="s">"silu"</span><span class="p">,</span>
  <span class="s">"hidden_size"</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span>
  <span class="s">"initializer_range"</span><span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>
  <span class="s">"intermediate_size"</span><span class="p">:</span> <span class="mi">14336</span><span class="p">,</span>
  <span class="s">"max_position_embeddings"</span><span class="p">:</span> <span class="mi">131072</span><span class="p">,</span>
  <span class="s">"mlp_bias"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
  <span class="s">"model_type"</span><span class="p">:</span> <span class="s">"llama"</span><span class="p">,</span>
  <span class="s">"num_attention_heads"</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span> <span class="c1"># 32-&gt;16
</span>  <span class="s">"num_hidden_layers"</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span> <span class="c1"># 32-&gt;16
</span>  <span class="s">"num_key_value_heads"</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
  <span class="s">"pretraining_tp"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
  <span class="s">"rms_norm_eps"</span><span class="p">:</span> <span class="mf">1e-05</span><span class="p">,</span>
  <span class="s">"rope_scaling"</span><span class="p">:</span> <span class="p">{</span>
    <span class="s">"factor"</span><span class="p">:</span> <span class="mf">8.0</span><span class="p">,</span>
    <span class="s">"low_freq_factor"</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="s">"high_freq_factor"</span><span class="p">:</span> <span class="mf">4.0</span><span class="p">,</span>
    <span class="s">"original_max_position_embeddings"</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>
    <span class="s">"rope_type"</span><span class="p">:</span> <span class="s">"llama3"</span>
  <span class="p">},</span>
  <span class="s">"rope_theta"</span><span class="p">:</span> <span class="mf">500000.0</span><span class="p">,</span>
  <span class="s">"tie_word_embeddings"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
  <span class="s">"torch_dtype"</span><span class="p">:</span> <span class="s">"bfloat16"</span><span class="p">,</span>
  <span class="s">"transformers_version"</span><span class="p">:</span> <span class="s">"4.43.0.dev0"</span><span class="p">,</span>
  <span class="s">"use_cache"</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
  <span class="s">"vocab_size"</span><span class="p">:</span> <span class="mi">128256</span>
<span class="p">}</span>
<span class="n">congig_cls</span> <span class="o">=</span> <span class="n">LlamaConfig</span>
<span class="n">model_cls</span> <span class="o">=</span> <span class="n">LlamaForCausalLM</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">congig_cls</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">model_cls</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">local_model_path</span> <span class="o">=</span> <span class="s">"/home/snawar/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/48d6d0fc4e02fb1269b36940650a1b7233035cbb"</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">local_model_path</span><span class="p">,</span>
                                                   <span class="n">local_files_only</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                                   <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">,).</span><span class="n">to</span><span class="p">(</span><span class="s">'cuda'</span><span class="p">)</span>



<span class="n">model_id</span> <span class="o">=</span> <span class="s">"meta-llama/Meta-Llama-3.1-8B"</span>

<span class="n">input_text</span> <span class="o">=</span> <span class="s">"The reason why bigbang happend is "</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">generation_config</span><span class="p">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token_id</span>

<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">).</span><span class="n">input_ids</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span><span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████| 4/4 [00:00&lt;00:00, 24.87it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The reason why bigbang happend is 2nd law of thermodynamics.
2nd law of thermodynamics is entropy increase.
Entropy is probability of disorder.
So, there is always probability of disorder.
But, there is no probability of order.
That is, the probability of order is 0.
So, entropy can be 0.
And, when entropy is 0, temperature is infinite.
Infinite temperature is infinite energy.
So, the universe will have infinite energy.
And,
</code></pre></div></div>

<p class="notice--warning">Even though i changed the <code class="language-plaintext highlighter-rouge">num_hidden_layers</code> and <code class="language-plaintext highlighter-rouge">num_attention_heads</code> in the <code class="language-plaintext highlighter-rouge">param</code> from 32 to 16 , The model weights were still loaded.</p>

<h2 id="metas-original-implementation-the-full-llama-experience">Meta’s Original Implementation: The Full LLaMA Experience</h2>

<p>Meta’s original <a href="https://github.com/meta-llama/llama-models/tree/main">implementation</a> offers the most control, particularly for large-scale distributed training. The original LLama 3.1 architecture is purely implemented in PyTorch.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">from</span> <span class="nn">fairscale.nn.model_parallel.initialize</span> <span class="kn">import</span> <span class="n">initialize_model_parallel</span><span class="p">,</span> <span class="n">get_model_parallel_world_size</span>
<span class="kn">from</span> <span class="nn">models.llama3_1.api.args</span> <span class="kn">import</span> <span class="n">ModelArgs</span>
<span class="kn">from</span> <span class="nn">models.llama3_1.reference_impl.model</span> <span class="kn">import</span> <span class="n">Transformer</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>

<span class="c1"># Set environment variables manually if not running in a distributed launcher
</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'RANK'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'0'</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'WORLD_SIZE'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'1'</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'MASTER_ADDR'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'localhost'</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'MASTER_PORT'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'9300'</span>



<span class="c1"># Continue with the rest of the script...
</span>
<span class="c1"># Initialize the PyTorch distributed environment
</span><span class="n">dist</span><span class="p">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s">'nccl'</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Initialize the model parallel group
</span><span class="n">initialize_model_parallel</span><span class="p">(</span><span class="n">model_parallel_size_</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Define your configuration
</span><span class="n">json</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"dim"</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span>
    <span class="s">"ffn_dim_multiplier"</span><span class="p">:</span> <span class="mf">1.3</span><span class="p">,</span>
    <span class="s">"multiple_of"</span><span class="p">:</span> <span class="mi">1024</span><span class="p">,</span>
    <span class="s">"n_heads"</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s">"n_kv_heads"</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
    <span class="s">"n_layers"</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s">"norm_eps"</span><span class="p">:</span> <span class="mf">1e-05</span><span class="p">,</span>
    <span class="s">"rope_theta"</span><span class="p">:</span> <span class="mf">500000.0</span><span class="p">,</span>
    <span class="s">"use_scaled_rope"</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
    <span class="s">"vocab_size"</span><span class="p">:</span> <span class="mi">128256</span>
<span class="p">}</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">ModelArgs</span>
<span class="n">param</span> <span class="o">=</span> <span class="n">config</span><span class="p">(</span><span class="o">**</span><span class="n">json</span><span class="p">)</span>

<span class="c1"># Initialize the model
</span><span class="k">print</span><span class="p">(</span><span class="s">"loading model ...."</span><span class="p">)</span>
<span class="n">llama_model</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">llama_model</span><span class="p">)</span>

<span class="c1"># load the .pth model weigths with model.load_state_dict(torch.load("path/to/your/model.pth"))
# Now you can proceed with your training or inference tasks
</span></code></pre></div></div>
<p>This method is geared towards advanced users who are comfortable managing distributed systems and need to train or fine-tune LLaMA 3.1 at scale.</p>

<p>The repo for LLama 3.1 itself does not contain the scripts for traning, finetuning and inference but there is a separate repo for that called <a href="https://github.com/meta-llama/llama-recipes.git">llama-recipies</a> which containes several useful scripts.</p>

<h4 id="why-use-it">Why Use It?</h4>
<p><strong>Full Control:</strong> Best for complex setups requiring model parallelism and distributed training.</p>

<p><strong>Large-Scale Training:</strong> Ideal for scenarios where you need to fully leverage your hardware resources.</p>]]></content><author><name>Snawar Hussain</name></author><category term="Educational" /><category term="LLMs" /><category term="LLaMA" /><category term="Generative Models" /><category term="Model Loading" /><category term="Hugging Face" /><category term="Transformers" /><category term="Causal Attention" /><category term="Self-Attention" /><category term="Machine Learning" /><category term="Deep Learning" /><category term="AI" /><summary type="html"><![CDATA[Exploring the different methods to load and work with the LLaMA 3.1 model using Hugging Face's APIs and Meta's original implementation. Learn which approach best suits your needs for inference, fine-tuning, or training from scratch.]]></summary></entry><entry><title type="html">GPT model Causal Multi-Head Attention Mechanism: Pure and Simple</title><link href="https://snawarhussain.com/educational/llms/Causal-Attention-Mechanism-Pure-and-Simple/" rel="alternate" type="text/html" title="GPT model Causal Multi-Head Attention Mechanism: Pure and Simple" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://snawarhussain.com/educational/llms/Causal-Attention-Mechanism-Pure-and%20Simple</id><content type="html" xml:base="https://snawarhussain.com/educational/llms/Causal-Attention-Mechanism-Pure-and-Simple/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>I have always found a disconnect between the theoretical understanding of attention mechanisms, the illustrations and their practical code implementation in models like GPT and wasn’t able to merge all the knowledge while I was trying to understand multi-head attention mechanism in-depth. Since transformers work on batched input, they things change up a bit and it is always confusion to relate the illustration and theory to the code implementation. In this post, I aim to bridge this gap by providing a simple and intuitive explanation of the vanilla as well as causal attention mechanism in GPT models.</p>

<p>Let’s break it down step-by-step, starting from the input sequence and moving through the entire process.</p>

<h2 id="mathematical-representation-of-attention-mechanism">Mathematical Representation of Attention Mechanism</h2>

<p>We all know the attention mechanism in transformers can be mathematically represented as follows:</p>

\[Y = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V\]

<p>here \(Q\), \(K\), and \(V\) are the query, key, and value matrices, respectively. The softmax function is applied to the scaled dot-product of \(Q\) and \(K\), divided by the square root of the dimension of the key matrix. The output is then multiplied by the value matrix to obtain the final output \(Y\). 
Here i am interested in the dimensions of the input data as well as that of \(Q, K\) and \(V\) and how they are used in the attention mechanism. and what dimension the operations in the attention mechanism are performed on.</p>

<h2 id="input-data-representation">Input Data Representation</h2>
<p>as we know the input data to the transformer model is represented as a sequence of tokens. Each token is embedded into a vector of dimension <code class="language-plaintext highlighter-rouge">n_embd</code>. The input is represented as a matrix X of shape (<code class="language-plaintext highlighter-rouge">batch_size, seq_length,  n_embd</code>). This matrix is then passed through linear transformations to obtain the query, key, and value matrices \(Q\), \(K\), and \(V\), respectively.</p>

<p>Given that we have a trained tokenizer, tokenized the input sequence and passed it through an input embedding layer (<code class="language-plaintext highlighter-rouge">nn.embedding()</code>), we can represent the input data as a matrix \(X\) of shape (<code class="language-plaintext highlighter-rouge">batch_size, seq_length, n_embd</code>). This matrix contains the embeddings of the input tokens, where each row corresponds to a token and each column represents the embedding dimension.</p>

<p>suppose we have an input <strong>X</strong> with batch size is <strong>4</strong>, with sequence tokens (<code class="language-plaintext highlighter-rouge">eq_length</code>) <strong>12</strong>, after passing it through our tokenizer and embedding layer, we end up with final input of <code class="language-plaintext highlighter-rouge">[4,12,6]</code> (<code class="language-plaintext highlighter-rouge">batch_size, seq_length, n_embd</code>) The input data matrix \(X\) is represented as shown in the figure 01 below.</p>

<!-- html for putting svg figure -->
<figure class="styled-figure">
  <img src="/assets/images/atten_mech/attn_01.svg" alt="Input Data Matrix X" />
  <figcaption>Figure 01: Input Data Matrix X: where each row corresponds to a token and each column represents the embedding dimension</figcaption>
</figure>

<p>In the figure 01 above, the \(12 \times 6\) matrix represents a sinlge batch of the input data matrix \(X\), where each row corresponds to a token and each column represents the embedding dimension. we have a total of 4 batches of the input data matrix \(X\).</p>

<p><!--  highlight this part of the blog --></p>

<h3 id="multi-head-self-attention">Multi-Head Self-Attention</h3>

<blockquote>
  <p>Many blogs and articles mention that the input X is projected into queries, keys, and values of equal dimension, which are then used to compute attention based on a given equation. They often state that this process is repeated in parallel through multi-head attention.</p>
</blockquote>

<div style="border-left: 5px solid #007bff; padding: 10px; margin: 20px 0;">


However, in practice, multi-head attention is implemented more cleverly. The primary goal is to allow the model to attend to information from different representation subspaces simultaneously at different positions. This approach enables the model to learn richer and more diverse representations of the input data.

</div>

<p>This concept will become clearer once we delve into the implementation details of multi-head attention.</p>

<p>Since each token as a an embedding of dimension <code class="language-plaintext highlighter-rouge">n_embd</code> which is 6 in our case, what is done is we further divide the each batch of \(12 \times 6\) (<code class="language-plaintext highlighter-rouge">seq_length, n_embd</code>) matrix into a ([<code class="language-plaintext highlighter-rouge">seq_length, num_h ,h_size</code>])
so embedding dimension is divided into <code class="language-plaintext highlighter-rouge">num_h</code> heads, where each head has a dimension of <code class="language-plaintext highlighter-rouge">h_size</code>. This is done by reshaping the input data matrix \(X\) into a tensor of shape (<code class="language-plaintext highlighter-rouge">batch_size, seq_length, num_h, h_size</code>). and for each batch we reshape it into [<code class="language-plaintext highlighter-rouge">num_h, seq_length, h_size</code>]</p>

<p>so in order for multi-head attention to work, the embedding dimension <code class="language-plaintext highlighter-rouge">n_embd</code> should be divisible by the number of heads <code class="language-plaintext highlighter-rouge">num_h</code>.  in our case the embedding dimension is 6 and we have decide number of heads to be 3, so we end up with a tensor of shape <code class="language-plaintext highlighter-rouge">[3,12,2]</code> (<code class="language-plaintext highlighter-rouge">num_h, seq_length,  h_size</code>) for each batch. as shown in figure 02 below.</p>

<figure class="styled-figure">
  <img src="/assets/images/atten_mech/attn_02.svg" height="600" />
  <figcaption>Figure 02:the embedding dimension for each batch is divided into multiple heads, allowing the model to attend to different aspects of the tokens simultaneously</figcaption>
</figure>

<p>For each batch of the input data matrix X of shape <code class="language-plaintext highlighter-rouge">[12,6]</code> passed through linear transformations to obtain the query, key, and value matrices Q, K, and V, each of shape <code class="language-plaintext highlighter-rouge">[12,6]</code> respectively. \(QKV\) are then reshaped it into a tensor of shape <code class="language-plaintext highlighter-rouge">[3,12,2]</code> (<code class="language-plaintext highlighter-rouge">num_h, seq_length,  h_size</code>).</p>

<p>Did you notice something ? Instead of projecting the input data X into multiple \(QKV\) matrices for each head , we do the projection once and divide the embedding dimension into multiple heads.</p>

<p class="notice--info"><strong>Info:</strong> Did you notice something ? Instead of projecting the input data \(X\) into multiple \(QKV\) matrices for each head , we do the projection once and divide the embedding dimension into multiple heads. The reason for this is that it allows the model to attend to information from different representation subspaces simultaneously at different positions. This approach enables the model to learn richer and more diverse representations of the input data.</p>

<blockquote>
  <p>Since embeddings of each token represent a different aspect of the token, dividing the embedding dimension into multiple heads allows the model to attend to different aspects (sub embedding space) of the token simultaneously.</p>
</blockquote>

<blockquote>
  <p>This way, One aspect of a token might be more attentive to a certain aspect of an other token while a different aspect of the same token might learn to pay attention to certain aspect of a different token in a different head.  This is the essence of multi-head attention.</p>
</blockquote>

<p>We project each batch into \(QKV\) matrices, each of shape <code class="language-plaintext highlighter-rouge">[12,6]</code> (<code class="language-plaintext highlighter-rouge">seq_length,  n_embd</code>). The \(QKV\) matrices are then reshaped into a tensor of shape <code class="language-plaintext highlighter-rouge">[3,12,2]</code> (<code class="language-plaintext highlighter-rouge">num_h, seq_length,  h_size</code>) for each batch.</p>

<p>and inside each head, we first compute the scaled dot-product attention between \(Q\) <code class="language-plaintext highlighter-rouge">[12,2]</code> and \(K\) <code class="language-plaintext highlighter-rouge">[12,2]</code> to get the attention scores of shape <code class="language-plaintext highlighter-rouge">[12,12]</code> as shown in figure 03 below.</p>

<figure class="styled-figure">
  <img src="/assets/images/atten_mech/attn_03.svg" alt="Input Data Matrix X" />
  <figcaption>Figure 03: Scaled Dot-Product Attention Q@K for Each Head</figcaption>
</figure>

<p>basically – because we chopped up the original token embeddings of each token into multiple heads of size <code class="language-plaintext highlighter-rouge">h_size</code>– for each head we get different attention scores since we are attending to different aspects of each token in each head. as highlighted in the figure 04 below.</p>

<figure class="styled-figure">
  <img src="/assets/images/atten_mech/attn_05.svg" alt="Input Data Matrix X" />
  <figcaption>Figure 04: Different aspect of the tokens give rise to unique attention score matrices</figcaption>
</figure>

<p>The attention scores are then passed through the softmax function to obtain the attention weights. that are then multiplied by the value matrix \(V\) <code class="language-plaintext highlighter-rouge">[12, 2]</code> to obtain the output of the head \(y\) of shape <code class="language-plaintext highlighter-rouge">[12,2]</code> (figure 04).</p>

<figure class="styled-figure">
  <img src="/assets/images/atten_mech/attn_04.svg" alt="Input Data Matrix X" />
  <figcaption>Figure 05: Attention score is multiplied with V to aggregate context between concepts (sub-embedding of each token in that head)  </figcaption>
</figure>

<p>This process is done for each of 3 heads, and the outputs are concatenated and reshaped to ge the final output of the multi-head attention mechanism as shown in figure 06 below.</p>

<figure class="styled-figure">
  <img src="/assets/images/atten_mech/attn_06.svg" alt="Input Data Matrix X" />
  <figcaption>Figure 06: partial Context aggregation at sub-embedding level of each head are combined to get the final context aware embeddings of tokens</figcaption>
</figure>

<h3 id="causal-attention-mechanism">Causal Attention Mechanism</h3>

<p>The causal attention mechanism is a variant of the multi-head attention mechanism that restricts the model from attending to tokens that come after the current token. This is achieved by masking the attention scores of the tokens that come after the current token (Figure 07). The masking is done by setting the attention scores to negative infinity <code class="language-plaintext highlighter-rouge">-inf</code> before passing them through the softmax function and multiplying it with the \(V\) matrix.</p>

<figure class="styled-figure">
  <img src="/assets/images/atten_mech/attn_07.svg" alt="Input Data Matrix X" />
  <figcaption>Figure 07: Each token (in rows) only have the attention score for tokens before it to prevent looking into the future </figcaption>
</figure>

<h3 id="code-implementation">Code Implementation</h3>

<p>The code implementation of the multi-head attention mechanism in PyTorch is shown below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CausalSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span><span class="n">GPTConfig</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span> <span class="o">%</span> <span class="n">config</span><span class="p">.</span><span class="n">n_head</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">c_attn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="o">*</span><span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">c_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">n_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_embd</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s">'bias'</span><span class="p">,</span>
                             <span class="n">tril</span><span class="p">(</span><span class="n">ones</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">)).</span><span class="n">view</span><span class="p">(</span>
                                    <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> 
                                    <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">,</span>
                                    <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">))</span> <span class="c1"># create a lower triangular matrix of ones
</span>        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>   <span class="c1"># B: batch size, T: sequence length, C: n_embd [4, 12,6]
</span>        
        <span class="n">qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">c_attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># [4, 12, 6] -&gt; [4, 12, 6*3] -&gt; [4, 12, 18]
</span>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># [4, 12, 6], [4, 12, 6], [4, 12, 6]
</span>        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">C</span><span class="o">//</span><span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># [4, 12, 3, 2] transpose-&gt; [4, 3, 12, 2] 
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">C</span><span class="o">//</span><span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># [4, 12, 3, 2] transpose-&gt; [4, 3, 12, 2] 
</span>        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">C</span><span class="o">//</span><span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="c1"># [4, 12, 3, 2] transpose-&gt; [4, 3, 12, 2] 
</span>        <span class="n">att</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">k</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span> <span class="c1"># [4, 3, 12, 12]
</span>        <span class="n">att</span> <span class="o">=</span> <span class="n">att</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">[:,:,:</span><span class="n">T</span><span class="p">,:</span><span class="n">T</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span> <span class="c1"># [4, 3, 12, 12]replace zero with -inf
</span>        <span class="n">att</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">att</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># [4, 3, 12, 12] softmax
</span>        <span class="n">y</span> <span class="o">=</span> <span class="n">att</span> <span class="o">@</span> <span class="n">v</span> <span class="c1"># [4, 3, 12, 12] @ [4, 3, 12, 2] -&gt; [4, 3, 12, 2]
</span>        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="n">contiguous</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span> <span class="c1"># [4, 3, 12, 2] transpose-&gt; [4, 12, 3, 2] view-&gt; [4, 12, 6]
</span>        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">c_proj</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># [4, 12, 6] -&gt; [4, 12, 6] learnable linear layer
</span>        <span class="k">return</span> <span class="n">y</span>   <span class="c1"># [4, 12, 6]
</span></code></pre></div></div>

<h4 id="initialization">initialization</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CausalSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">GPTConfig</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span> <span class="o">%</span> <span class="n">config</span><span class="p">.</span><span class="n">n_head</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">c_attn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">c_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">n_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_embd</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s">'bias'</span><span class="p">,</span>
                             <span class="n">tril</span><span class="p">(</span><span class="n">ones</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">)).</span><span class="n">view</span><span class="p">(</span>
                                    <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">))</span>
</code></pre></div></div>

<p><span style="color:#d36135; font-weight: bold">Asserting Divisibility:</span></p>

<p>Ensure that the embedding dimension (n_embd) is divisible by the number of heads (n_head). This is important for splitting the embeddings into multiple heads.
Linear Layers for Projections:</p>

<p><span style="color:#d36135; font-weight: bold">self.c_attn:</span></p>

<p>A linear layer to project the input into queries, keys, and values. The output dimension is three times the embedding dimension to accommodate Q, K, and V.</p>

<p><span style="color:#d36135; font-weight: bold">self.c_proj:</span></p>

<p>A linear layer to project the concatenated outputs of the multi-head attention back to the original embedding dimension.</p>

<p><span style="color:#d36135; font-weight: bold">Registering the Causal Mask:</span></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="p">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s">'bias'</span><span class="p">,</span> <span class="n">tril</span><span class="p">(</span><span class="n">ones</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">)).</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">))</span>
</code></pre></div></div>
<p>Creates a lower triangular matrix of ones using <code class="language-plaintext highlighter-rouge">tril(ones(config.block_size, config.block_size))</code> to serve as a causal mask. 
The name is said to ‘bias’ to match the naming scheme of GPT to lead pre-trained weight ^_~</p>

<p>The mask is reshaped and registered as a buffer, which means it won’t be updated during training but is persistent in the model’s state. ( some mumbo jumbo for leading GPT weights more on this in the next post or )</p>

<h4 id="forward-pass">Forward Pass</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>
    
    <span class="n">qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">c_attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">att</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">k</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">att</span> <span class="o">=</span> <span class="n">att</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">T</span><span class="p">,</span> <span class="p">:</span><span class="n">T</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span>
    <span class="n">att</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">att</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">att</span> <span class="o">@</span> <span class="n">v</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="n">contiguous</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">c_proj</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>
</code></pre></div></div>

<p><span style="color:#d36135; font-weight: bold">Input Dimensions:</span></p>

<p>B, T, C = x.size(): Extract the batch size (B), sequence length (T), and embedding dimension (C) from the input tensor x.</p>

<p><span style="color:#d36135; font-weight: bold">Linear Projection to Q, K, V:</span></p>

<p><em>qkv = self.c_attn(x):</em> Apply the linear layer to project the input into queries (q), keys (k), and values (v).
<em>q, k, v = qkv.split(C, dim=2):</em> Split the concatenated qkv tensor into separate q, k, and v tensors.
Reshaping and Transposing for Multi-Head Attention:</p>

<p><em>k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2):</em> Reshape k to [B, T, nh, hs] and then transpose to [B, nh, T, hs].</p>

<p>Similar operations are performed for q and v.</p>

<p><span style="color:#d36135; font-weight: bold">Scaled Dot-Product Attention:</span></p>

<p><em>att = (q @ k.transpose(-2, -1)) * (k.size(-1) ** -0.5):</em> Compute the attention scores using the dot product of q and k, scaled by the square root of the head size.
<em>att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(‘-inf’)):</em> Apply the causal mask to ensure that each position can only attend to previous positions.
<em>att = nn.functional.softmax(att, dim=-1):</em> Apply softmax to obtain the attention weights.</p>

<p><span style="color:#d36135; font-weight: bold">Apply Attention Weights to Values:</span></p>

<p><em>y = att @ v:</em> Compute the weighted sum of the values using the attention weights.</p>

<p><span style="color:#d36135; font-weight: bold">Combining Heads:</span></p>

<p><em>y = y.transpose(1, 2).contiguous().view(B, T, C):</em> Transpose and reshape the output to combine the heads back into the original embedding dimension.</p>

<p><span style="color:#d36135; font-weight: bold">Final Linear Projection:</span></p>

<p><em>y = self.c_proj(y):</em> Apply the final linear projection to produce the output of the attention mechanism.</p>

<link rel="stylesheet" href="/assets/css/atten_mech/style.css" />]]></content><author><name>Snawar Hussain</name></author><category term="Educational" /><category term="LLMs" /><category term="GPT" /><category term="Generative Models" /><category term="Attention Mechanism" /><category term="Multi-Head Attention" /><category term="Transformers" /><category term="Causal Attention" /><category term="Self-Attention" /><category term="Machine Learning" /><category term="Deep Learning" /><category term="AI" /><summary type="html"><![CDATA[How does the multi-head attention mechanism work in transformers? Let's break it down step-by-step, starting from the input sequence and moving through the entire process.]]></summary></entry><entry><title type="html">Getting TensorFlow to Work with GPU in Conda Environment on Linux or WSL</title><link href="https://snawarhussain.com/blog/linux/linux-tensorflow-with-cuda-in-conda-environment/" rel="alternate" type="text/html" title="Getting TensorFlow to Work with GPU in Conda Environment on Linux or WSL" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://snawarhussain.com/blog/linux/linux-tensorflow-with-cuda-in-conda-environment</id><content type="html" xml:base="https://snawarhussain.com/blog/linux/linux-tensorflow-with-cuda-in-conda-environment/"><![CDATA[<p>Let’s admit it, installing TensorFlow with CUDA support is a pain in the neck and doesn’t work right away on the first attempt 99% of the time. Many of us have faced the frustration of seeing TensorFlow fail to utilize the GPU even though <code class="language-plaintext highlighter-rouge">nvidia-smi</code> confirms it’s there. If you’re running Linux or WSL and have installed TensorFlow in a Conda environment but are struggling to get it to use your GPU, this guide is for you. Follow these steps to ensure TensorFlow can utilize the CUDA and cuDNN libraries installed within your Conda environment, rather than relying on a global installation that might be outdated or incompatible with your version of tensorflow.</p>

<h2 id="prerequisites">Prerequisites</h2>

<p>Before starting, ensure you have the following:</p>
<ol>
  <li>A working installation of Conda.</li>
  <li>TensorFlow installed in a Conda environment.</li>
  <li>NVIDIA drivers installed and verified with <code class="language-plaintext highlighter-rouge">nvidia-smi</code> command.</li>
  <li>CUDA and cuDNN installed within your Conda environment.</li>
</ol>

<h2 id="step-1-verify-your-environment">Step 1: Verify Your Environment</h2>

<p>First, verify that you can run <code class="language-plaintext highlighter-rouge">nvidia-smi</code> and that it correctly shows your GPU:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvidia-smi
</code></pre></div></div>
<p>This command should display information about your NVIDIA GPU. If it doesn’t, you may need to install the NVIDIA drivers or check your hardware configuration.</p>

<p>Next, activate your Conda environment and check if TensorFlow is installed:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda activate &lt;your_environment&gt;
python <span class="nt">-c</span> <span class="s2">"import tensorflow as tf; print(tf.__version__)"</span>
</code></pre></div></div>
<p>if TensorFlow is installed, you should see the version number printed. If not, install TensorFlow with cuda in your Conda environment:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>tensorflow[and-cuda]
</code></pre></div></div>
<p>on papers this should be enough to have your tensorflow up and running with CUDA support, but in reality it often doesn’t work as expected. especially if you don’t have a system wide installation of CUDA and cuDNN or if it’s not compatible with the version of TensorFlow you’re using.</p>

<p>For the sake of sanity check let’s run a simple TensorFlow code to see if it’s already using the GPU:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-c</span> <span class="s2">"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"</span>
</code></pre></div></div>
<p>if you see something like <code class="language-plaintext highlighter-rouge">[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]</code> then you’re good to go, otherwise follow the next steps.</p>

<h2 id="step-2-identify-the-cuda-and-cudnn-paths">Step 2: Identify the CUDA and cuDNN Paths</h2>

<p>We want tensorflow to use the CUDA and cuDNN libraries installed within the Conda environment that we installed with the above <code class="language-plaintext highlighter-rouge">pip install tensorflow[and-cuda]</code>, rather than relying on a global installation that might be outdated.
 Make sure to activate your conda environment where tensorflow with cuda is installed In linux terminal run</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-c</span> <span class="s2">"import nvidia.cudnn; print(nvidia.cudnn.__file__)"</span>
</code></pre></div></div>
<p>if it prints out something like:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/home/username/miniconda3/envs/tf/lib/python3.11/site-packages/nvidia/cudnn/__init__.py
</code></pre></div></div>
<p>then that means we do have a cuDNN library installed within the conda environment but now we just need to set the path so that tensorflow can see it.</p>

<p>save the cudnn paths in a variable:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">CUDNN_PATH</span><span class="o">=</span><span class="si">$(</span><span class="nb">dirname</span> <span class="si">$(</span>python <span class="nt">-c</span> <span class="s2">"import nvidia.cudnn; print(nvidia.cudnn.__file__)"</span><span class="si">))</span>
<span class="nb">echo</span> <span class="nv">$CUDNN_PATH</span>
</code></pre></div></div>
<p>this will save the path to the cuDNN library in the <code class="language-plaintext highlighter-rouge">CUDNN_PATH</code> variable and print it out to the terminal.</p>

<h2 id="step-3-set-ld_library_path-environment-variable">Step 3: Set <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> Environment Variable:</h2>

<p>Now we need to set the <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> environment variable to include the paths to the CUDA and cuDNN libraries within the Conda environment. This will allow TensorFlow to find and use these libraries when running on the GPU.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="k">${</span><span class="nv">CUDNN_PATH</span><span class="k">}</span>/lib:<span class="nv">$LD_LIBRARY_PATH</span>
<span class="nb">echo</span> <span class="nv">$LD_LIBRARY_PATH</span>
</code></pre></div></div>
<p>This adds the cuDNN library path to the LD_LIBRARY_PATH.</p>

<h2 id="step-4-test-tensorflow-with-cuda">Step 4: Test TensorFlow with CUDA</h2>

<p>Now test again if TensorFlow can see the GPU and use CUDA and cuDNN libraries:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-c</span> <span class="s2">"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"</span>
</code></pre></div></div>
<p>if you see <code class="language-plaintext highlighter-rouge">[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]</code> then you’re good to go, otherwise you might need to check the paths again and make sure they’re correct.</p>

<h2 id="step-5-presist-the-changes">Step 5: Presist the changes</h2>

<p>To make sure the changes persist across terminal sessions, we want to update the <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> each time our conda environment is activated. To do this, we need to add some lines to the <code class="language-plaintext highlighter-rouge">activate</code> script of the conda environment:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nano <span class="nv">$CONDA_PREFIX</span>/etc/conda/activate.d/env_vars.sh
</code></pre></div></div>
<p>Add the following lines to the file:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/sh</span>
<span class="nb">export </span><span class="nv">CUDNN_PATH</span><span class="o">=</span><span class="si">$(</span><span class="nb">dirname</span> <span class="si">$(</span>python <span class="nt">-c</span> <span class="s2">"import nvidia.cudnn; print(nvidia.cudnn.__file__)"</span><span class="si">))</span>
<span class="nb">export </span><span class="nv">OLD_LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>
<span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="k">${</span><span class="nv">CUDNN_PATH</span><span class="k">}</span>/lib:<span class="nv">$LD_LIBRARY_PATH</span>
</code></pre></div></div>

<p>Save the file and exit the editor. Now, whenever you activate your conda environment, the <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> will be updated to include the paths to the CUDA and cuDNN libraries within the conda environment.</p>

<h2 id="create-deactivation-script">Create Deactivation Script</h2>

<p>To ensure that the <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> is reset when you deactivate the conda environment, create a <code class="language-plaintext highlighter-rouge">deactivate</code> script that unsets the <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> variable. To do this, run the following command:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nano <span class="nv">$CONDA_PREFIX</span>/etc/conda/deactivate.d/env_vars.sh
</code></pre></div></div>

<p>Add the following lines to the file:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/sh</span>
<span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$OLD_LD_LIBRARY_PATH</span>
<span class="nb">unset </span>OLD_LD_LIBRARY_PATH
<span class="nb">unset </span>CUDNN_PATH
</code></pre></div></div>
<p>That’s it! Now, whenever you deactivate your conda environment, the <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> will be reset to its original value.</p>

<p class="notice--info"><strong>Note:</strong> Tensorflow by default looks for cudnn in the environment variable <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code>.Although it seems like we are just running bunch of commands in a shell without understanding what they do. The main reason we are doing this is first to find cudnn installed within our conda environment that is accessible to python and then set the default path <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> to where the cudnn is installed. So that tensorflow can use it.And we can update the <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> each time our conda environment is activated by adding the commands to the <code class="language-plaintext highlighter-rouge">activate</code> script of the conda environment. This way we don’t have to run the commands each time we activate the environment.</p>]]></content><author><name>Snawar Hussain</name></author><category term="Blog" /><category term="Linux" /><category term="TensorFlow" /><category term="GPU" /><category term="CUDA" /><category term="cuDNN" /><category term="Conda" /><category term="Linux" /><category term="WSL2" /><category term="WSL" /><category term="Deep Learning" /><category term="AI" /><summary type="html"><![CDATA[Guide to set-up TensorFlow to use GPU in a Conda environment.Follow these steps to ensure TensorFlow leverages CUDA and cuDNN installed in your Conda environment.]]></summary></entry><entry><title type="html">Navigating K-space in MRI: The Role of Gradient Fields</title><link href="https://snawarhussain.com/educational/mri%20technology/EPGs-For_Dummies/" rel="alternate" type="text/html" title="Navigating K-space in MRI: The Role of Gradient Fields" /><published>2024-04-12T00:00:00+00:00</published><updated>2024-04-12T00:00:00+00:00</updated><id>https://snawarhussain.com/educational/mri%20technology/EPGs-For_Dummies</id><content type="html" xml:base="https://snawarhussain.com/educational/mri%20technology/EPGs-For_Dummies/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>In Magnetic Resonance Imaging (MRI), gradient fields, among many other things, are used to encode spatial information. This post aims to clarify the connection between gradient fields and spatial frequencies in the K-space, and how gradients walks us through K-space. Blending mathematical theory with practical code examples, the goal is to enhance basic understanding of MR Physics and Simulation.</p>

<h2 id="mathematical-relationship-between-k-space-and-gradient-fields">Mathematical relationship between K-space and Gradient fields</h2>

<p>Recall from MRI theory that the gradients are used to change the mangetic field strenght of the B0 field in a particular direction. 
For example a gradient in the x direction will change the magnetic field strength in the x direction.And similar is the case with gradients in the y or z direction.This change in magnetic field strength is represented by the following equation:</p>

\[B(x,y,z) = (B_0 + G_x.x + G_z.y+ G_z.z)_z\]

<p>It is important to note that the overall magnetic field  is still pointing in the z direction. The gradients only change the strenght of magnetic field in space and not the direction. That is why the right side of the above equation is subscripted with z.
visually we can represent this change in magnetic field strength as shown in the following interactive plot:
by changing slider values we can change the magnetic field strength in the x, y and z directions. the overall magnetic field is still pointing in the z direction. The gradients only change the strenght of magnetic field shown in colorcoded arrows all pointing in the z direction.</p>

<div class="container">
<!-- add heigh and width to the div cube -->
    <div id="grid"></div>
    <input type="range" id="gx-slider" min="-100" max="100" value="0" />
    <input type="range" id="gy-slider" min="-100" max="100" value="0" />
    <input type="range" id="gz-slider" min="-100" max="100" value="0" />
</div>

<h2 id="rotating-vs-labratory-frame-of-reference">Rotating vs Labratory Frame of reference</h2>
<p>Consider a simple scenario where follwing an RF field and a slice selection gradient, we have a 2D slice of the spin magnetic moments knocked into the transverse plane all rotating at larmor frequency in phase.</p>

<!-- <div class="frames-container">
    <div class="frame" id="lab-frame">
        <h2>Labratory Frame</h2>
        <div class="arrow_grid" id="lab-cube"></div>
    </div>
    <div class="frame" id="rotational-frame">
        <h2>Rotational Frame</h2>
        <div class="arrow_grid" id="rotational-cube"></div>
    </div>
</div> -->

<p align="center">
<img src="/assets/images/ks_grad/lab_rotat.gif" height="300" />
 <figcaption>Fig 1: Labratory vs Rotational Frame of reference</figcaption>
</p>

<p>In the laboratory frame of reference, the spin magnetic moments are precessing at the Larmor frequency and are initially in phase. However, when we shift our perspective to the rotating frame of reference, these spin magnetic moments appear stationary. This stationary appearance results from the rotating frame moving synchronously with the spins’ Larmor precession. This concept is at the core of what we call the rotating frame of reference.</p>

<p>Opting for the rotating frame of reference in our subsequent discussions offers a significant advantage. It simplifies the visualization of gradient effects on spin magnetic moments, allowing us to observe these effects without the complexity introduced by their rapid precession in the laboratory frame. Essentially, it provides a clearer and more stable viewpoint, making it easier to comprehend the changes induced by the gradients without the ‘dizzying’ effects of spin motion seen in the lab frame.</p>

<h2 id="gradient-fields-and-spin-magnetic-moments">Gradient Fields and Spin Magnetic Moments</h2>

<p>When a gradient field is introduced in any direction, it alters the magnetic field strength along that direction. This variation in magnetic field strength causes the spin magnetic moments to precess at different frequencies in that specific direction. The reason for this is that the precession frequency of spin magnetic moments is directly proportional to the magnetic field strength, as described by the equation:</p>

\[\omega = \gamma B\]

<p>Here, \(\omega\) represents the precession frequency, \(\gamma\) is the gyromagnetic ratio, and \(B\) signifies the magnetic field strength.</p>

<p>In a rotating frame of reference, this differential in precession frequencies translates to spin dephasing along the spatial gradient. This dephasing occurs because spins are not precessing synchronously with the rotating frame.</p>

<p>For instance, a Gx gradient introduces spin magnetic moment dephasing along the x-axis. Spins located on one side of the x-axis origin dephase in the opposite direction to those on the other side, while spins precisely at the x-axis origin do not dephase. Analogous effects occur with Gy and Gz gradients, affecting spin dephasing along the y-axis and z-axis, respectively.</p>

<h2 id="2d-fourier-transform-and-mri">2D Fourier Transform and MRI</h2>

<p>In our previous discussion on <a href="/educational/mri%20technology/data%20analysis/2D-Fourier-Transform-K-space-and-MRI/">2D Fourier Transform</a> we simplified the concept of image decomposition using the principles of the 2D Fourier Transform. To avoid the complexities of mathematical rigor, let’s recall the key idea: any two-dimensional (2D) spatial signal, such as an image, can be decomposed into components resembling 2D sinusoids. These sinusoidal patterns represent variations in both spatial frequency and phase orientation. By gathering enough amount of these 2D sinusoids, varying in their spatial frequencies and orientations, we can reconstruct the original image. This process forms the crux of the inverse 2D Fourier Transform.</p>

<p>Each of these 2D sinusoids, characterized by specific oscillations along the x and y axes, correlates to a unique point in the 2D Fourier Transform plane, denoted as \(F(k_x, k_y)\). This plane is, in fact, what is known in MRI terminology as the K-space. K-space is a conceptual framework used for understanding and processing the data acquired by MRI scanners to produce images. It represents the spatial frequencies of the object being imaged, with each point in K-space contributing to the overall image’s formation.</p>

<h2 id="stepping-through-k-space-using-gradients">Stepping Through K-space using Gradients</h2>

<p>The role of gradients in MRI is pivotal for navigating through K-space, and to demonstrate this, I have created an interactive graph. Imagine a 2D slice with spin magnetic moments initially aligned in the transverse plane and rotating at the Larmor frequency, in phase within the rotating frame of reference.</p>

<p>In our interactive plot, you’ll find horizontal and vertical sliders representing the application of gradient fields. As you adjust these sliders, you are effectively changing the frequency and phase of the spin magnetic moments. This adjustment simulates the effects of frequency and phase encoding in MRI.</p>

<p>The horizontal slider corresponds to the frequency encoding gradient. Moving this slider alters the precession frequency of the spins along one axis, thus varying their position along the frequency-encoded direction in K-space.</p>

<p>The vertical slider, on the other hand, represents the phase encoding gradient. Adjusting this slider changes the phase of the spins, thereby moving them along the phase-encoded direction in K-space.</p>

<p>As you interact with these sliders, observe the changes in a 2D grid of arrows on the plot. These arrows symbolize the spin magnetic moments and their orientation in response to the applied gradients. Were you able to see it ??</p>

<div class="threejs">
<!-- add heigh and width to the div cube -->
    <div id="cube"></div>
    <input type="range" id="horizontal-slider" min="-360" max="360" value="0" />
    <input type="range" id="vertical-slider" min="-360" max="360" value="0" />

</div>

<p>By turing on the gradients in the x and y direction we are varying the frequency and phase of the spins in the x and y direction based on
their location in space and in-turn creating our own 2D sinosoids corresponding to a certain point in the 2D K-space.</p>

<p>This is exactly what we do in MRI. We collect 2D sinosoids (of spin magnetic moments of different tissues) varying in spatial frequency and phase and once we have collected enough of them we can reconstruct the original image back using the inverse Fourier Transform.</p>

<p>Mathematically, the relationship between the gradient fields and their representation in K-space can be expressed as follows:</p>

\[k(x,y,z) = \gamma \int G(x,y,z) dt\]

<p>In this equation, \(k(x,y,z)\) represents the position in K-space , \(\gamma\) is the gyromagnetic ratio, \(G(x,y,z)\) is the gradient applied, and \(dt\) denotes the integration over time. This formulation illustrates that the position in K-space is directly proportional to the time integral of the gradient field applied in a arbitrary direction.</p>

<h3 id="the-influence-of-k_x-and-k_y-on-sinusoidal-patterns">The Influence of \(k_x\) and \(k_y\) on Sinusoidal Patterns</h3>

<p>Recall that the number of wiggles or oscillations in 2D sinosoid in x and y direction correlates directly to a specific point in the \(F(k_x, k_y)\) plane aka K-space. For instance, a sinusoid with 3 complete cycles  in the x-direction and 3 in the y-direction would correspond to a point in \(F(k_x, k_y)\) with \(k_x =3\) value and \(k_y =3\) value. This mapping is fundamental to how spatial frequencies are represented and manipulated in the Fourier Transform, particularly in applications like MRI, where precise spatial information is crucial.</p>

<p>Here’s a part of animation that demonstrates the relation between point in K-Space and the 2D sinosoid it generates and the effect of phasor assosiated with each of the 2D sinosoid (spatical frequency) and how it scales and shifts it.</p>

<p align="center">
<img src="/assets/images/2D_FT/FT_2D.gif" height="600" />
 <figcaption>Fig 2: 2D Sinosoid with changing phase  </figcaption>
</p>

<h2 id="conclusion">Conclusion</h2>

<p>In summary, the intricacies of MRI technology, particularly the role of gradient fields in navigating through K-space, underscore the sophistication of this imaging modality. Through the careful manipulation of gradients, MRI is capable of encoding spatial information into the spin magnetic moments, which are then mapped onto K-space. The fundamental equation \(k = \gamma \int G dt\) elegantly captures this relationship, demonstrating how the position in K-space is determined by the integrated effect of the applied gradients over time.</p>

<p>This deep interplay between physical principles and technological implementation not only facilitates the creation of detailed anatomical images but also opens avenues for advanced imaging techniques. The interactive tools and visualizations we discussed serve as potent means for understanding these complex concepts, making the abstract principles of MRI more tangible and comprehensible.</p>

<p>As we continue to refine and advance MRI technology, the potential for improved diagnostics and research expands. The journey through K-space, facilitated by gradient fields, is not just a cornerstone of MRI but a testament to the ingenuity and continual evolution of medical imaging technology.</p>

<!-- Include necessary scritps -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r121/three.min.js"></script>

<!-- Include OrbitControls.js from jsDelivr -->
<script src="https://cdn.jsdelivr.net/npm/three@0.121.0/examples/js/controls/OrbitControls.js"></script>

<script src="/assets/js/arrowgrid.js"></script>

<link rel="stylesheet" href="/assets/css/arrowgrid.css" />

<script src="https://cdn.jsdelivr.net/npm/chroma-js@2.1.0/chroma.min.js"></script>

<script src="/assets/js/threejs.js"></script>

<link rel="stylesheet" href="/assets/css/threejs.css" />]]></content><author><name>Snawar Hussain</name></author><category term="Educational" /><category term="MRI Technology" /><category term="MRI" /><category term="K-space" /><category term="Gradient Fields" /><category term="Fourier Transform" /><category term="Spin Magnetic Moments" /><category term="Image Reconstruction" /><category term="Medical Imaging" /><summary type="html"><![CDATA[Exploring the crucial role of gradient fields in MRI for stepping through K-space.]]></summary></entry><entry><title type="html">Navigating K-space in MRI: The Role of Gradient Fields</title><link href="https://snawarhussain.com/educational/mri%20technology/Kspace-walk-using-encoding-gradients-in-MRI/" rel="alternate" type="text/html" title="Navigating K-space in MRI: The Role of Gradient Fields" /><published>2024-01-25T00:00:00+00:00</published><updated>2024-01-25T00:00:00+00:00</updated><id>https://snawarhussain.com/educational/mri%20technology/Kspace-walk-using-encoding-gradients-in-MRI</id><content type="html" xml:base="https://snawarhussain.com/educational/mri%20technology/Kspace-walk-using-encoding-gradients-in-MRI/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>In Magnetic Resonance Imaging (MRI), gradient fields, among many other things, are used to encode spatial information. This post aims to clarify the connection between gradient fields and spatial frequencies in the K-space, and how gradients walks us through K-space. Blending mathematical theory with practical code examples, the goal is to enhance basic understanding of MR Physics and Simulation.</p>

<h2 id="mathematical-relationship-between-k-space-and-gradient-fields">Mathematical relationship between K-space and Gradient fields</h2>

<p>Recall from MRI theory that the gradients are used to change the mangetic field strenght of the B0 field in a particular direction. 
For example a gradient in the x direction will change the magnetic field strength in the x direction.And similar is the case with gradients in the y or z direction.This change in magnetic field strength is represented by the following equation:</p>

\[B(x,y,z) = (B_0 + G_x.x + G_z.y+ G_z.z)_z\]

<p>It is important to note that the overall magnetic field  is still pointing in the z direction. The gradients only change the strenght of magnetic field in space and not the direction. That is why the right side of the above equation is subscripted with z.
visually we can represent this change in magnetic field strength as shown in the following interactive plot:
by changing slider values we can change the magnetic field strength in the x, y and z directions. the overall magnetic field is still pointing in the z direction. The gradients only change the strenght of magnetic field shown in colorcoded arrows all pointing in the z direction.</p>

<div class="container">
<!-- add heigh and width to the div cube -->
    <div id="grid"></div>
    <input type="range" id="gx-slider" min="-100" max="100" value="0" />
    <input type="range" id="gy-slider" min="-100" max="100" value="0" />
    <input type="range" id="gz-slider" min="-100" max="100" value="0" />
</div>

<h2 id="rotating-vs-labratory-frame-of-reference">Rotating vs Labratory Frame of reference</h2>
<p>Consider a simple scenario where follwing an RF field and a slice selection gradient, we have a 2D slice of the spin magnetic moments knocked into the transverse plane all rotating at larmor frequency in phase.</p>

<!-- <div class="frames-container">
    <div class="frame" id="lab-frame">
        <h2>Labratory Frame</h2>
        <div class="arrow_grid" id="lab-cube"></div>
    </div>
    <div class="frame" id="rotational-frame">
        <h2>Rotational Frame</h2>
        <div class="arrow_grid" id="rotational-cube"></div>
    </div>
</div> -->

<p align="center">
<img src="/assets/images/ks_grad/lab_rotat.gif" height="300" />
 <figcaption>Fig 1: Labratory vs Rotational Frame of reference</figcaption>
</p>

<p>In the laboratory frame of reference, the spin magnetic moments are precessing at the Larmor frequency and are initially in phase. However, when we shift our perspective to the rotating frame of reference, these spin magnetic moments appear stationary. This stationary appearance results from the rotating frame moving synchronously with the spins’ Larmor precession. This concept is at the core of what we call the rotating frame of reference.</p>

<p>Opting for the rotating frame of reference in our subsequent discussions offers a significant advantage. It simplifies the visualization of gradient effects on spin magnetic moments, allowing us to observe these effects without the complexity introduced by their rapid precession in the laboratory frame. Essentially, it provides a clearer and more stable viewpoint, making it easier to comprehend the changes induced by the gradients without the ‘dizzying’ effects of spin motion seen in the lab frame.</p>

<h2 id="gradient-fields-and-spin-magnetic-moments">Gradient Fields and Spin Magnetic Moments</h2>

<p>When a gradient field is introduced in any direction, it alters the magnetic field strength along that direction. This variation in magnetic field strength causes the spin magnetic moments to precess at different frequencies in that specific direction. The reason for this is that the precession frequency of spin magnetic moments is directly proportional to the magnetic field strength, as described by the equation:</p>

\[\omega = \gamma B\]

<p>Here, \(\omega\) represents the precession frequency, \(\gamma\) is the gyromagnetic ratio, and \(B\) signifies the magnetic field strength.</p>

<p>In a rotating frame of reference, this differential in precession frequencies translates to spin dephasing along the spatial gradient. This dephasing occurs because spins are not precessing synchronously with the rotating frame.</p>

<p>For instance, a Gx gradient introduces spin magnetic moment dephasing along the x-axis. Spins located on one side of the x-axis origin dephase in the opposite direction to those on the other side, while spins precisely at the x-axis origin do not dephase. Analogous effects occur with Gy and Gz gradients, affecting spin dephasing along the y-axis and z-axis, respectively.</p>

<h2 id="2d-fourier-transform-and-mri">2D Fourier Transform and MRI</h2>

<p>In our previous discussion on <a href="/educational/mri%20technology/data%20analysis/2D-Fourier-Transform-K-space-and-MRI/">2D Fourier Transform</a> we simplified the concept of image decomposition using the principles of the 2D Fourier Transform. To avoid the complexities of mathematical rigor, let’s recall the key idea: any two-dimensional (2D) spatial signal, such as an image, can be decomposed into components resembling 2D sinusoids. These sinusoidal patterns represent variations in both spatial frequency and phase orientation. By gathering enough amount of these 2D sinusoids, varying in their spatial frequencies and orientations, we can reconstruct the original image. This process forms the crux of the inverse 2D Fourier Transform.</p>

<p>Each of these 2D sinusoids, characterized by specific oscillations along the x and y axes, correlates to a unique point in the 2D Fourier Transform plane, denoted as \(F(k_x, k_y)\). This plane is, in fact, what is known in MRI terminology as the K-space. K-space is a conceptual framework used for understanding and processing the data acquired by MRI scanners to produce images. It represents the spatial frequencies of the object being imaged, with each point in K-space contributing to the overall image’s formation.</p>

<h2 id="stepping-through-k-space-using-gradients">Stepping Through K-space using Gradients</h2>

<p>The role of gradients in MRI is pivotal for navigating through K-space, and to demonstrate this, I have created an interactive graph. Imagine a 2D slice with spin magnetic moments initially aligned in the transverse plane and rotating at the Larmor frequency, in phase within the rotating frame of reference.</p>

<p>In our interactive plot, you’ll find horizontal and vertical sliders representing the application of gradient fields. As you adjust these sliders, you are effectively changing the frequency and phase of the spin magnetic moments. This adjustment simulates the effects of frequency and phase encoding in MRI.</p>

<p>The horizontal slider corresponds to the frequency encoding gradient. Moving this slider alters the precession frequency of the spins along one axis, thus varying their position along the frequency-encoded direction in K-space.</p>

<p>The vertical slider, on the other hand, represents the phase encoding gradient. Adjusting this slider changes the phase of the spins, thereby moving them along the phase-encoded direction in K-space.</p>

<p>As you interact with these sliders, observe the changes in a 2D grid of arrows on the plot. These arrows symbolize the spin magnetic moments and their orientation in response to the applied gradients. Were you able to see it ??</p>

<div class="threejs">
<!-- add heigh and width to the div cube -->
    <div id="cube"></div>
    <input type="range" id="horizontal-slider" min="-360" max="360" value="0" />
    <input type="range" id="vertical-slider" min="-360" max="360" value="0" />

</div>

<p>By turing on the gradients in the x and y direction we are varying the frequency and phase of the spins in the x and y direction based on
their location in space and in-turn creating our own 2D sinosoids corresponding to a certain point in the 2D K-space.</p>

<p>This is exactly what we do in MRI. We collect 2D sinosoids (of spin magnetic moments of different tissues) varying in spatial frequency and phase and once we have collected enough of them we can reconstruct the original image back using the inverse Fourier Transform.</p>

<p>Mathematically, the relationship between the gradient fields and their representation in K-space can be expressed as follows:</p>

\[k(x,y,z) = \gamma \int G(x,y,z) dt\]

<p>In this equation, \(k(x,y,z)\) represents the position in K-space , \(\gamma\) is the gyromagnetic ratio, \(G(x,y,z)\) is the gradient applied, and \(dt\) denotes the integration over time. This formulation illustrates that the position in K-space is directly proportional to the time integral of the gradient field applied in a arbitrary direction.</p>

<h3 id="the-influence-of-k_x-and-k_y-on-sinusoidal-patterns">The Influence of \(k_x\) and \(k_y\) on Sinusoidal Patterns</h3>

<p>Recall that the number of wiggles or oscillations in 2D sinosoid in x and y direction correlates directly to a specific point in the \(F(k_x, k_y)\) plane aka K-space. For instance, a sinusoid with 3 complete cycles  in the x-direction and 3 in the y-direction would correspond to a point in \(F(k_x, k_y)\) with \(k_x =3\) value and \(k_y =3\) value. This mapping is fundamental to how spatial frequencies are represented and manipulated in the Fourier Transform, particularly in applications like MRI, where precise spatial information is crucial.</p>

<p>Here’s a part of animation that demonstrates the relation between point in K-Space and the 2D sinosoid it generates and the effect of phasor assosiated with each of the 2D sinosoid (spatical frequency) and how it scales and shifts it.</p>

<p align="center">
<img src="/assets/images/2D_FT/FT_2D.gif" height="600" />
 <figcaption>Fig 2: 2D Sinosoid with changing phase  </figcaption>
</p>

<h2 id="conclusion">Conclusion</h2>

<p>In summary, the intricacies of MRI technology, particularly the role of gradient fields in navigating through K-space, underscore the sophistication of this imaging modality. Through the careful manipulation of gradients, MRI is capable of encoding spatial information into the spin magnetic moments, which are then mapped onto K-space. The fundamental equation \(k = \gamma \int G dt\) elegantly captures this relationship, demonstrating how the position in K-space is determined by the integrated effect of the applied gradients over time.</p>

<p>This deep interplay between physical principles and technological implementation not only facilitates the creation of detailed anatomical images but also opens avenues for advanced imaging techniques. The interactive tools and visualizations we discussed serve as potent means for understanding these complex concepts, making the abstract principles of MRI more tangible and comprehensible.</p>

<p>As we continue to refine and advance MRI technology, the potential for improved diagnostics and research expands. The journey through K-space, facilitated by gradient fields, is not just a cornerstone of MRI but a testament to the ingenuity and continual evolution of medical imaging technology.</p>

<!-- Include necessary scritps -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r121/three.min.js"></script>

<!-- Include OrbitControls.js from jsDelivr -->
<script src="https://cdn.jsdelivr.net/npm/three@0.121.0/examples/js/controls/OrbitControls.js"></script>

<script src="/assets/js/arrowgrid.js"></script>

<link rel="stylesheet" href="/assets/css/arrowgrid.css" />

<script src="https://cdn.jsdelivr.net/npm/chroma-js@2.1.0/chroma.min.js"></script>

<script src="/assets/js/threejs.js"></script>

<link rel="stylesheet" href="/assets/css/threejs.css" />]]></content><author><name>Snawar Hussain</name></author><category term="Educational" /><category term="MRI Technology" /><category term="MRI" /><category term="K-space" /><category term="Gradient Fields" /><category term="Fourier Transform" /><category term="Spin Magnetic Moments" /><category term="Image Reconstruction" /><category term="Medical Imaging" /><summary type="html"><![CDATA[Exploring the crucial role of gradient fields in MRI for stepping through K-space.]]></summary></entry><entry><title type="html">Current Opinion On Animal Pose Estimation Tools A Review</title><link href="https://snawarhussain.com/Current-Opinion-on-Animal-Pose-Estimation-Tools-A-Review/" rel="alternate" type="text/html" title="Current Opinion On Animal Pose Estimation Tools A Review" /><published>2024-01-05T00:00:00+00:00</published><updated>2024-01-05T00:00:00+00:00</updated><id>https://snawarhussain.com/Current-Opinion-on-Animal-Pose-Estimation-Tools-A-Review</id><content type="html" xml:base="https://snawarhussain.com/Current-Opinion-on-Animal-Pose-Estimation-Tools-A-Review/"><![CDATA[<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Current Opinion on Animal Pose Estimation and Behavior Analysis Tools</title><style>
    /* cspell:disable-file */
    /* webkit printing magic: print all background colors */
    html {
        -webkit-print-color-adjust: exact;
    }
    * {
        box-sizing: border-box;
        -webkit-print-color-adjust: exact;
    }
    
    html,
    body {
        margin: 0;
        padding: 0;
    }
    @media only screen {
        body {
            margin: 2em auto;
            max-width: 900px;
            color: rgb(55, 53, 47);
        }
    }
    
    body {
        line-height: 1.5;
        white-space: pre-wrap;
    }
    
    a,
    a.visited {
        color: inherit;
        text-decoration: underline;
    }
    
    .pdf-relative-link-path {
        font-size: 80%;
        color: #444;
    }
    
    h1,
    h2,
    h3 {
        letter-spacing: -0.01em;
        line-height: 1.2;
        font-weight: 600;
        margin-bottom: 0;
    }
    
    .page-title {
        font-size: 2.5rem;
        font-weight: 700;
        margin-top: 0;
        margin-bottom: 0.75em;
    }
    
    h1 {
        font-size: 1.875rem;
        margin-top: 1.875rem;
    }
    
    h2 {
        font-size: 1.5rem;
        margin-top: 1.5rem;
    }
    
    h3 {
        font-size: 1.25rem;
        margin-top: 1.25rem;
    }
    
    .source {
        border: 1px solid #ddd;
        border-radius: 3px;
        padding: 1.5em;
        word-break: break-all;
    }
    
    .callout {
        border-radius: 3px;
        padding: 1rem;
    }
    
    figure {
        margin: 1.25em 0;
        page-break-inside: avoid;
    }
    
    figcaption {
        opacity: 0.5;
        font-size: 85%;
        margin-top: 0.5em;
    }
    
    mark {
        background-color: transparent;
    }
    
    .indented {
        padding-left: 1.5em;
    }
    
    hr {
        background: transparent;
        display: block;
        width: 100%;
        height: 1px;
        visibility: visible;
        border: none;
        border-bottom: 1px solid rgba(55, 53, 47, 0.09);
    }
    
    img {
        max-width: 100%;
    }
    
    @media only print {
        img {
            max-height: 100vh;
            object-fit: contain;
        }
    }
    
    @page {
        margin: 1in;
    }
    
    .collection-content {
        font-size: 0.875rem;
    }
    
    .column-list {
        display: flex;
        justify-content: space-between;
    }
    
    .column {
        padding: 0 1em;
    }
    
    .column:first-child {
        padding-left: 0;
    }
    
    .column:last-child {
        padding-right: 0;
    }
    
    .table_of_contents-item {
        display: block;
        font-size: 0.875rem;
        line-height: 1.3;
        padding: 0.125rem;
    }
    
    .table_of_contents-indent-1 {
        margin-left: 1.5rem;
    }
    
    .table_of_contents-indent-2 {
        margin-left: 3rem;
    }
    
    .table_of_contents-indent-3 {
        margin-left: 4.5rem;
    }
    
    .table_of_contents-link {
        text-decoration: none;
        opacity: 0.7;
        border-bottom: 1px solid rgba(55, 53, 47, 0.18);
    }
    
    table,
    th,
    td {
        border: 1px solid rgba(55, 53, 47, 0.09);
        border-collapse: collapse;
    }
    
    table {
        border-left: none;
        border-right: none;
    }
    
    th,
    td {
        font-weight: normal;
        padding: 0.25em 0.5em;
        line-height: 1.5;
        min-height: 1.5em;
        text-align: left;
    }
    
    th {
        color: rgba(55, 53, 47, 0.6);
    }
    
    ol,
    ul {
        margin: 0;
        margin-block-start: 0.6em;
        margin-block-end: 0.6em;
    }
    
    li > ol:first-child,
    li > ul:first-child {
        margin-block-start: 0.6em;
    }
    
    ul > li {
        list-style: disc;
    }
    
    ul.to-do-list {
        padding-inline-start: 0;
    }
    
    ul.to-do-list > li {
        list-style: none;
    }
    
    .to-do-children-checked {
        text-decoration: line-through;
        opacity: 0.375;
    }
    
    ul.toggle > li {
        list-style: none;
    }
    
    ul {
        padding-inline-start: 1.7em;
    }
    
    ul > li {
        padding-left: 0.1em;
    }
    
    ol {
        padding-inline-start: 1.6em;
    }
    
    ol > li {
        padding-left: 0.2em;
    }
    
    .mono ol {
        padding-inline-start: 2em;
    }
    
    .mono ol > li {
        text-indent: -0.4em;
    }
    
    .toggle {
        padding-inline-start: 0em;
        list-style-type: none;
    }
    
    /* Indent toggle children */
    .toggle > li > details {
        padding-left: 1.7em;
    }
    
    .toggle > li > details > summary {
        margin-left: -1.1em;
    }
    
    .selected-value {
        display: inline-block;
        padding: 0 0.5em;
        background: rgba(206, 205, 202, 0.5);
        border-radius: 3px;
        margin-right: 0.5em;
        margin-top: 0.3em;
        margin-bottom: 0.3em;
        white-space: nowrap;
    }
    
    .collection-title {
        display: inline-block;
        margin-right: 1em;
    }
    
    .page-description {
        margin-bottom: 2em;
    }
    
    .simple-table {
        margin-top: 1em;
        font-size: 0.875rem;
        empty-cells: show;
    }
    .simple-table td {
        height: 29px;
        min-width: 120px;
    }
    
    .simple-table th {
        height: 29px;
        min-width: 120px;
    }
    
    .simple-table-header-color {
        background: rgb(247, 246, 243);
        color: black;
    }
    .simple-table-header {
        font-weight: 500;
    }
    
    time {
        opacity: 0.5;
    }
    
    .icon {
        display: inline-block;
        max-width: 1.2em;
        max-height: 1.2em;
        text-decoration: none;
        vertical-align: text-bottom;
        margin-right: 0.5em;
    }
    
    img.icon {
        border-radius: 3px;
    }
    
    .user-icon {
        width: 1.5em;
        height: 1.5em;
        border-radius: 100%;
        margin-right: 0.5rem;
    }
    
    .user-icon-inner {
        font-size: 0.8em;
    }
    
    .text-icon {
        border: 1px solid #000;
        text-align: center;
    }
    
    .page-cover-image {
        display: block;
        object-fit: cover;
        width: 100%;
        max-height: 30vh;
    }
    
    .page-header-icon {
        font-size: 3rem;
        margin-bottom: 1rem;
    }
    
    .page-header-icon-with-cover {
        margin-top: -0.72em;
        margin-left: 0.07em;
    }
    
    .page-header-icon img {
        border-radius: 3px;
    }
    
    .link-to-page {
        margin: 1em 0;
        padding: 0;
        border: none;
        font-weight: 500;
    }
    
    p > .user {
        opacity: 0.5;
    }
    
    td > .user,
    td > time {
        white-space: nowrap;
    }
    
    input[type="checkbox"] {
        transform: scale(1.5);
        margin-right: 0.6em;
        vertical-align: middle;
    }
    
    p {
        margin-top: 0.5em;
        margin-bottom: 0.5em;
    }
    
    .image {
        border: none;
        margin: 1.5em 0;
        padding: 0;
        border-radius: 0;
        text-align: center;
    }
    
    .code,
    code {
        background: rgba(135, 131, 120, 0.15);
        border-radius: 3px;
        padding: 0.2em 0.4em;
        border-radius: 3px;
        font-size: 85%;
        tab-size: 2;
    }
    
    code {
        color: #eb5757;
    }
    
    .code {
        padding: 1.5em 1em;
    }
    
    .code-wrap {
        white-space: pre-wrap;
        word-break: break-all;
    }
    
    .code > code {
        background: none;
        padding: 0;
        font-size: 100%;
        color: inherit;
    }
    
    blockquote {
        font-size: 1.25em;
        margin: 1em 0;
        padding-left: 1em;
        border-left: 3px solid rgb(55, 53, 47);
    }
    
    .bookmark {
        text-decoration: none;
        max-height: 8em;
        padding: 0;
        display: flex;
        width: 100%;
        align-items: stretch;
    }
    
    .bookmark-title {
        font-size: 0.85em;
        overflow: hidden;
        text-overflow: ellipsis;
        height: 1.75em;
        white-space: nowrap;
    }
    
    .bookmark-text {
        display: flex;
        flex-direction: column;
    }
    
    .bookmark-info {
        flex: 4 1 180px;
        padding: 12px 14px 14px;
        display: flex;
        flex-direction: column;
        justify-content: space-between;
    }
    
    .bookmark-image {
        width: 33%;
        flex: 1 1 180px;
        display: block;
        position: relative;
        object-fit: cover;
        border-radius: 1px;
    }
    
    .bookmark-description {
        color: rgba(55, 53, 47, 0.6);
        font-size: 0.75em;
        overflow: hidden;
        max-height: 4.5em;
        word-break: break-word;
    }
    
    .bookmark-href {
        font-size: 0.75em;
        margin-top: 0.25em;
    }
    
    .sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
    .code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
    .serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
    .mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
    .pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
    .pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
    .pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
    .pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
    .pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
    .pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
    .pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
    .pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
    .pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
    .pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
    .pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
    .pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
    .pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
    .pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
    .pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
    .pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
    .highlight-default {
        color: rgba(55, 53, 47, 1);
    }
    .highlight-gray {
        color: rgba(120, 119, 116, 1);
        fill: rgba(120, 119, 116, 1);
    }
    .highlight-brown {
        color: rgba(159, 107, 83, 1);
        fill: rgba(159, 107, 83, 1);
    }
    .highlight-orange {
        color: rgba(217, 115, 13, 1);
        fill: rgba(217, 115, 13, 1);
    }
    .highlight-yellow {
        color: rgba(203, 145, 47, 1);
        fill: rgba(203, 145, 47, 1);
    }
    .highlight-teal {
        color: rgba(68, 131, 97, 1);
        fill: rgba(68, 131, 97, 1);
    }
    .highlight-blue {
        color: rgba(51, 126, 169, 1);
        fill: rgba(51, 126, 169, 1);
    }
    .highlight-purple {
        color: rgba(144, 101, 176, 1);
        fill: rgba(144, 101, 176, 1);
    }
    .highlight-pink {
        color: rgba(193, 76, 138, 1);
        fill: rgba(193, 76, 138, 1);
    }
    .highlight-red {
        color: rgba(212, 76, 71, 1);
        fill: rgba(212, 76, 71, 1);
    }
    .highlight-gray_background {
        background: rgba(241, 241, 239, 1);
    }
    .highlight-brown_background {
        background: rgba(244, 238, 238, 1);
    }
    .highlight-orange_background {
        background: rgba(251, 236, 221, 1);
    }
    .highlight-yellow_background {
        background: rgba(251, 243, 219, 1);
    }
    .highlight-teal_background {
        background: rgba(237, 243, 236, 1);
    }
    .highlight-blue_background {
        background: rgba(231, 243, 248, 1);
    }
    .highlight-purple_background {
        background: rgba(244, 240, 247, 0.8);
    }
    .highlight-pink_background {
        background: rgba(249, 238, 243, 0.8);
    }
    .highlight-red_background {
        background: rgba(253, 235, 236, 1);
    }
    .block-color-default {
        color: inherit;
        fill: inherit;
    }
    .block-color-gray {
        color: rgba(120, 119, 116, 1);
        fill: rgba(120, 119, 116, 1);
    }
    .block-color-brown {
        color: rgba(159, 107, 83, 1);
        fill: rgba(159, 107, 83, 1);
    }
    .block-color-orange {
        color: rgba(217, 115, 13, 1);
        fill: rgba(217, 115, 13, 1);
    }
    .block-color-yellow {
        color: rgba(203, 145, 47, 1);
        fill: rgba(203, 145, 47, 1);
    }
    .block-color-teal {
        color: rgba(68, 131, 97, 1);
        fill: rgba(68, 131, 97, 1);
    }
    .block-color-blue {
        color: rgba(51, 126, 169, 1);
        fill: rgba(51, 126, 169, 1);
    }
    .block-color-purple {
        color: rgba(144, 101, 176, 1);
        fill: rgba(144, 101, 176, 1);
    }
    .block-color-pink {
        color: rgba(193, 76, 138, 1);
        fill: rgba(193, 76, 138, 1);
    }
    .block-color-red {
        color: rgba(212, 76, 71, 1);
        fill: rgba(212, 76, 71, 1);
    }
    .block-color-gray_background {
        background: rgba(241, 241, 239, 1);
    }
    .block-color-brown_background {
        background: rgba(244, 238, 238, 1);
    }
    .block-color-orange_background {
        background: rgba(251, 236, 221, 1);
    }
    .block-color-yellow_background {
        background: rgba(251, 243, 219, 1);
    }
    .block-color-teal_background {
        background: rgba(237, 243, 236, 1);
    }
    .block-color-blue_background {
        background: rgba(231, 243, 248, 1);
    }
    .block-color-purple_background {
        background: rgba(244, 240, 247, 0.8);
    }
    .block-color-pink_background {
        background: rgba(249, 238, 243, 0.8);
    }
    .block-color-red_background {
        background: rgba(253, 235, 236, 1);
    }
    .select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
    .select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
    .select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
    .select-value-color-green { background-color: rgba(219, 237, 219, 1); }
    .select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
    .select-value-color-transparentGray { background-color: rgba(227, 226, 224, 0); }
    .select-value-color-translucentGray { background-color: rgba(255, 255, 255, 0.0375); }
    .select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
    .select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
    .select-value-color-red { background-color: rgba(255, 226, 221, 1); }
    .select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
    .select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
    .select-value-color-pageGlass { background-color: undefined; }
    .select-value-color-washGlass { background-color: undefined; }
    
    .checkbox {
        display: inline-flex;
        vertical-align: text-bottom;
        width: 16;
        height: 16;
        background-size: 16px;
        margin-left: 2px;
        margin-right: 5px;
    }
    
    .checkbox-on {
        background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
    }
    
    .checkbox-off {
        background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
    }

    
    

    .citation-container {
        height: 300px;
        position: relative;
        margin: 10px auto;
        background: #1c1d21;
        color: #9fa8b7;
        border-radius: 16px;
        padding: 10px;
        font-size: var(--rem);
        line-height: 1.6;

    }

    
    .copy-code {
        position: absolute;
        top: 10px;
        right: 10px;
        background-color: var(--primary);
        color: white;
        border: none;
        padding: 5px 10px;
        cursor: pointer;
        border-radius: 3px;
        opacity: 0;
        transition: opacity 0.2s;
    }

    .citation-container:hover .copy-code {
        opacity: 1;
    }

    .hljs {
        overflow-x: auto;
        background: #1c1d21;
        color: #9fa8b7;
        height: 300px;

    }
    .copy-code.copied {
        background-color: green;
    }
        
    </style></head><body><article id="a4821611-be9f-48c1-9cde-f8bafc7cbbba" class="page serif"><header><img class="page-cover-image" src="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/Untitled%208.png" style="object-position:center 23.340000000000003%"/><div class="page-header-icon page-header-icon-with-cover"><span class="icon">🐁</span></div><h1 class="page-title">Current Opinion on Animal Pose Estimation and Behavior Analysis Tools</h1><p class="page-description"></p></header><div class="page-body"><nav id="2e415bde-98ee-4e38-8989-bf0beca907d7" class="block-color-gray table_of_contents"><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#d324f40e-e890-4041-b6c4-0c24856f24d9">Introduction</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#6e319e68-4eec-4702-b439-058ad248a503">List of Current Technologies</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#ee931a5a-a041-42b6-9f92-b27a857e529e">Overview</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#6e725e81-913b-4010-a5c2-c797b1684282">DeepLabCut</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#fb4b7336-d05d-4ded-9e34-11a5649cd141">SLEAP</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#082f29a4-328b-47bd-9ac7-a3d395099f3f">Anipose:</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#9195be81-1d3a-4cb4-9101-9aa52a417d05">DANNCE</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#c6eb0330-2983-43e1-bf7b-1e86479cfc28">AnyMaze</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#ecf79de4-4d43-49dc-9454-2ef6c5dc92d4">DeepPoseKit</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#5c6d6547-068f-4a43-8b41-36756487fa9b">DeepEthogram</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#5c9dc06f-1944-4205-9fc4-d0c88a0f1222">MARS</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#482d6ca1-7bb5-48af-92b1-67cae7c31cc7">VAME</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#088b1512-4037-4857-bf6f-5292b5992b51">B-SOiD</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#42144c5e-8f76-45a5-98d3-f57d6ab5f304">BehaviorAtlas</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#d5e755af-5e0d-4ccd-a7c9-c36ea725caf7">Skeletal Estimation</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#9dfb7a9d-57ad-418d-9d3e-98bbc1b2ae65">AVATAR</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#60a26119-1689-426a-ba60-65e57f975607">Final Thoughts</a></div></nav><h1 id="d324f40e-e890-4041-b6c4-0c24856f24d9" class="">Introduction</h1><p id="ecf626ff-ed2c-44b8-8545-3c06daa4322e" class="">Animal pose estimation tools have become increasingly popular in recent years, as they provide a way to accurately track and measure the movement and behavior of animals. Researchers have identified a number of benefits to using these tools, including providing accurate and detailed data that can be used to study animal behavior and inform conservation efforts. However, there are still some challenges in using these tools, including accuracy and cost. In this document, we will explore current opinion on animal pose estimation tools, looking at both the advantages and disadvantages of using them.</p><p id="32d7c46d-0821-4e45-b78c-718eea8894db" class="">Video based markerless pose estimation  is a powerful tool for quantifying animal behavior, as it allows for an unobtrusive, non-invasive observation of the animal&#x27;s actions, with minimal setup requirements. Using pose estimation, it is possible to extract key points across the animal&#x27;s body, allowing for the approximation of its motion patterns. Different behaviors are associated with distinct motion patterns; for instance, a mouse&#x27;s &quot;investigative&quot; behavior may involve a sequence of actions such as &quot;stop walking, look around and sniff&quot;. Once the pose estimation of the animal is obtained across all video frames, the body points can be plotted as time series data. By further analyzing the time series data, it is possible to gain further insight into the animal&#x27;s behavior; for example, the duration or frequency of certain behaviors can be observed, which can be used to gain a better understanding of the animal&#x27;s behavior in its natural environment.</p><p id="a38aa7a9-f757-4056-b8cf-4af83e9387e9" class="">Current popular animal pose estimation tools include DeepLabCut and OpenPose. DeepLabCut uses deep learning models to accurately track and measure an animal&#x27;s pose, while OpenPose is a more general framework that can be used to track the movement of any animal. Both tools provide detailed and accurate data that can be used to study animal behavior and inform conservation efforts. However, both tools are relatively expensive and require significant computing resources, which can be a barrier for smaller organizations. Additionally, accuracy can be an issue with both tools, as they can sometimes have difficulty tracking certain animals or complex poses.</p><h1 id="6e319e68-4eec-4702-b439-058ad248a503" class="">List of Current Technologies</h1><div id="4528383a-2cbc-405d-9223-42293badf0b9" class="column-list"><div id="aa67f2de-cad5-483e-a2c7-4cfff43c4bdc" style="width:50%" class="column"><ol type="1" id="bc64a587-a451-43dc-b097-2e922225894c" class="numbered-list" start="1"><li><a href="https://github.com/DeepLabCut/DeepLabCut">DeepLabcCut</a></li></ol><ol type="1" id="77f5e803-eba1-4689-832a-bbe700aaef98" class="numbered-list" start="2"><li><a href="https://github.com/murthylab/sleap">SLEAP</a></li></ol><ol type="1" id="5910b419-be11-42f5-80e5-efd2d64384d1" class="numbered-list" start="3"><li><a href="https://github.com/lambdaloop/anipose">Anipose</a></li></ol><ol type="1" id="c0a0df38-4ba5-4ec7-b360-2f3d6392e670" class="numbered-list" start="4"><li><a href="https://github.com/spoonsso/dannce">DANNCE</a></li></ol><ol type="1" id="444c10f5-0ba7-4da7-9f48-39a2ea669fad" class="numbered-list" start="5"><li><a href="https://www.any-maze.com/features/tracking-video-capture/">AnyMaze</a></li></ol><ol type="1" id="a8ca5452-0bf5-4b15-85a8-c71bc748b5ad" class="numbered-list" start="6"><li><a href="https://github.com/jgraving/deepposekit">DeepPoseKit</a></li></ol><ol type="1" id="ce53bb81-2ee1-4ed1-b74f-e7415e3d375d" class="numbered-list" start="7"><li><a href="https://github.com/jbohnslav/deepethogram">DeepEthogram</a></li></ol></div><div id="2322aab3-6cc6-49e2-b048-f6bcd2a5f3b6" style="width:50%" class="column"><ol type="1" id="a0c68878-6abe-47c2-afa9-c114fc16eba2" class="numbered-list" start="8"><li><a href="https://neuroethology.github.io/MARS/">MARS</a></li></ol><ol type="1" id="2f3c5822-116e-4f6e-9b34-ffb879e2b2b3" class="numbered-list" start="9"><li><a href="https://github.com/LINCellularNeuroscience/VAME">VAME</a></li></ol><ol type="1" id="64bcedd3-3512-49ac-9989-52324cfeca9a" class="numbered-list" start="10"><li><a href="https://github.com/YttriLab/B-SOID">B-SOiD</a></li></ol><ol type="1" id="cea3642e-5c64-4349-9add-7aaa645f48b4" class="numbered-list" start="11"><li><a href="https://behavioratlas.tech//">BehaviorAtlas</a></li></ol><ol type="1" id="dceba7db-af61-4bc7-beec-36f3df133fd1" class="numbered-list" start="12"><li><a href="https://www.nature.com/articles/s41592-022-01634-9#code-availability">Skeletal Estimation</a></li></ol><ol type="1" id="5ab724e6-e1b7-4eaf-a52b-02a69a69858f" class="numbered-list" start="13"><li><a href="https://www.biorxiv.org/content/10.1101/2021.12.31.474634v1">AVATAR (pre-print)</a></li></ol><ol type="1" id="9b67aafa-89ad-4ae5-af10-9ed5654fd749" class="numbered-list" start="14"><li><a href="https://github.com/snawarhussain/ComputationalNeuroEthologyPapers/">and Many More….</a>🔗🌐</li></ol></div></div><p id="4766f4af-60c8-4c80-9220-768af9a26a52" class="">
    </p><p id="fbbb6926-4dcf-4252-9efc-c2debd33a1d9" class="">Below we have summarized 📚 our findings in a table that evaluate each method on the basis of </p><p id="335af203-e169-4ee0-ab1c-96db2eee68fa" class="">
    </p><div id="5f5cb6ed-d393-4a79-acc8-2b1572350d5e" class="column-list"><div id="0c683a25-bcee-4fd1-8378-59060e6e6c57" style="width:33.33333333333333%" class="column"><blockquote id="d12bd24e-b929-4c0f-ab0f-e24fcaf85661" class="block-color-orange_background">Code Availability</blockquote><blockquote id="84cb843f-8fa4-4762-abc9-c45717d01147" class="block-color-orange_background">Documentation (Doc)</blockquote><blockquote id="d45e4110-9c24-470b-b766-3978291169e5" class="block-color-orange_background">Scalable or not</blockquote><blockquote id="b476df9f-fd3f-4aca-bb00-e211eec4f874" class="block-color-orange_background">Last Time Updated</blockquote></div><div id="406f0da3-5add-4624-841f-7d1e459541c7" style="width:33.33333333333333%" class="column"><blockquote id="1caeccbb-960e-4a34-aed2-d8d154dc6d43" class="block-color-orange_background">Dataset Availability</blockquote><blockquote id="5ef620ce-246c-45c6-b807-9e5118d1c552" class="block-color-orange_background">Open Source</blockquote><blockquote id="deb64408-300e-41a4-b8ed-95f42e55cef6" class="block-color-orange_background">Real-time Performance (RP)</blockquote><p id="4ce0c946-124c-452a-a76a-c5f92980c7bc" class="">
    </p></div><div id="8f23b45d-68f0-4412-a985-20c0ef046af1" style="width:33.33333333333333%" class="column"><blockquote id="e388d6d4-be79-45e6-a36c-c4ac3d0a2e9b" class="block-color-orange_background">User-friendliness (UF)</blockquote><blockquote id="d0dc9a85-11c5-455b-8cdf-d550064e8e7f" class="block-color-orange_background">GUI Availability</blockquote><blockquote id="e196c411-7c52-40f9-811c-4c7485f221c7" class="block-color-orange_background">Reproducibility (RP)</blockquote><p id="438f415b-bf8d-4f9b-8e73-d1c6f87db6b3" class="">
    </p></div></div><div id="6f58fbd4-356c-4014-ae4d-1daf0069e3fa" class="collection-content"><h4 class="collection-title">Review</h4><table class="collection-content"><thead><tr><th><span class="icon property-icon"><svg role="graphics-symbol" viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0" class="typesTitle"><path d="M0.637695 13.1914C1.0957 13.1914 1.32812 13 1.47852 12.5215L2.24414 10.3887H6.14746L6.90625 12.5215C7.05664 13 7.2959 13.1914 7.74707 13.1914C8.22559 13.1914 8.5332 12.9043 8.5332 12.4531C8.5332 12.2891 8.50586 12.1523 8.44434 11.9678L5.41602 3.79199C5.2041 3.21777 4.82129 2.9375 4.19922 2.9375C3.60449 2.9375 3.21484 3.21777 3.0166 3.78516L-0.0322266 12.002C-0.09375 12.1797 -0.121094 12.3232 -0.121094 12.4668C-0.121094 12.918 0.166016 13.1914 0.637695 13.1914ZM2.63379 9.12402L4.17871 4.68066H4.21973L5.76465 9.12402H2.63379ZM12.2793 13.2324C13.3115 13.2324 14.2891 12.6787 14.7129 11.8037H14.7402V12.5762C14.7471 12.9863 15.0273 13.2393 15.4238 13.2393C15.834 13.2393 16.1143 12.9795 16.1143 12.5215V8.00977C16.1143 6.49902 14.9658 5.52148 13.1543 5.52148C11.7666 5.52148 10.6592 6.08887 10.2695 6.99121C10.1943 7.15527 10.1533 7.3125 10.1533 7.46289C10.1533 7.81152 10.4062 8.04395 10.7686 8.04395C11.0215 8.04395 11.2129 7.94824 11.3496 7.73633C11.7529 6.99121 12.2861 6.65625 13.1064 6.65625C14.0977 6.65625 14.6992 7.20996 14.6992 8.1123V8.67285L12.5664 8.7959C10.7686 8.8916 9.77734 9.69824 9.77734 11.0107C9.77734 12.3369 10.8096 13.2324 12.2793 13.2324ZM12.6621 12.1387C11.8008 12.1387 11.2129 11.667 11.2129 10.9561C11.2129 10.2725 11.7598 9.82129 12.7578 9.75977L14.6992 9.62988V10.3203C14.6992 11.3457 13.7969 12.1387 12.6621 12.1387Z"></path></svg></span>Tools</th><th><span class="icon property-icon"><img src="https://www.notion.so/icons/command-line_gray.svg" style="width:14px;height:14px"/></span>Code</th><th><span class="icon property-icon"><img src="https://www.notion.so/icons/database_gray.svg" style="width:14px;height:14px"/></span>Dataset</th><th><span class="icon property-icon"><img src="https://www.notion.so/icons/follow_gray.svg" style="width:14px;height:14px"/></span>UF</th><th><span class="icon property-icon"><img src="https://www.notion.so/icons/service-counter_gray.svg" style="width:14px;height:14px"/></span>Doc</th><th><span class="icon property-icon"><img src="https://www.notion.so/icons/book_gray.svg" style="width:14px;height:14px"/></span>Open-Source?</th><th><span class="icon property-icon"><img src="https://www.notion.so/icons/cursor-click_gray.svg" style="width:14px;height:14px"/></span>GUI</th><th><span class="icon property-icon"><img src="https://www.notion.so/icons/judicial-scales_gray.svg" style="width:14px;height:14px"/></span>scaleable</th><th><span class="icon property-icon"><img src="https://www.notion.so/icons/watch-analog_gray.svg" style="width:14px;height:14px"/></span>RT</th><th><span class="icon property-icon"><img src="https://www.notion.so/icons/dna_gray.svg" style="width:14px;height:14px"/></span>RP</th><th><span class="icon property-icon"><img src="https://www.notion.so/icons/arrow-up_gray.svg" style="width:14px;height:14px"/></span>Updated</th></tr></thead><tbody><tr id="dd4f62f0-2967-4a08-9832-9db134684bd8"><td class="cell-title"><a href="https://www.notion.so/DLC-dd4f62f029674a0898329db134684bd8?pvs=21">DLC</a></td><td class="cell-gBhX"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Available</span></td><td class="cell-@eqG"><span class="status-value"><div class="status-dot"></div>N/A</span></td><td class="cell-DaSw">90</td><td class="cell-_`ur">80</td><td class="cell-Ycxk"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>YES</span></td><td class="cell-{azb"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Yes</span></td><td class="cell-@z;}"><span class="status-value select-value-color-yellow"><div class="status-dot status-dot-color-yellow"></div>Scalable</span></td><td class="cell-tGcX">50</td><td class="cell-QPRV">90</td><td class="cell-BocC">2021</td></tr><tr id="7670b567-f229-442d-aac2-bdada98174b1"><td class="cell-title"><a href="https://www.notion.so/SLEAP-7670b567f229442daac2bdada98174b1?pvs=21">SLEAP</a></td><td class="cell-gBhX"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Available</span></td><td class="cell-@eqG"><span class="status-value"><div class="status-dot"></div>N/A</span></td><td class="cell-DaSw">85</td><td class="cell-_`ur">65</td><td class="cell-Ycxk"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>YES</span></td><td class="cell-{azb"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Yes</span></td><td class="cell-@z;}"><span class="status-value select-value-color-yellow"><div class="status-dot status-dot-color-yellow"></div>Scalable</span></td><td class="cell-tGcX">90</td><td class="cell-QPRV">90</td><td class="cell-BocC">2021</td></tr><tr id="01bae412-dfbb-40d8-9b14-dddfedd4ddd8"><td class="cell-title"><a href="https://www.notion.so/Anipose-01bae412dfbb40d89b14dddfedd4ddd8?pvs=21">Anipose</a></td><td class="cell-gBhX"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Available</span></td><td class="cell-@eqG"><span class="status-value"><div class="status-dot"></div>N/A</span></td><td class="cell-DaSw">80</td><td class="cell-_`ur">60</td><td class="cell-Ycxk"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>YES</span></td><td class="cell-{azb"><span class="status-value select-value-color-red"><div class="status-dot status-dot-color-red"></div>N/A</span></td><td class="cell-@z;}"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Highly</span></td><td class="cell-tGcX">10</td><td class="cell-QPRV">60</td><td class="cell-BocC">2020</td></tr><tr id="189e8db6-97e3-4d92-be44-bbfb5746703f"><td class="cell-title"><a href="https://www.notion.so/DANNCE-189e8db697e34d92be44bbfb5746703f?pvs=21">DANNCE</a></td><td class="cell-gBhX"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Available</span></td><td class="cell-@eqG"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Available</span></td><td class="cell-DaSw">60</td><td class="cell-_`ur">50</td><td class="cell-Ycxk"><span class="status-value select-value-color-yellow"><div class="status-dot status-dot-color-yellow"></div>Partially</span></td><td class="cell-{azb"><span class="status-value select-value-color-red"><div class="status-dot status-dot-color-red"></div>N/A</span></td><td class="cell-@z;}"><span class="status-value select-value-color-yellow"><div class="status-dot status-dot-color-yellow"></div>Scalable</span></td><td class="cell-tGcX">10</td><td class="cell-QPRV">50</td><td class="cell-BocC">2022</td></tr><tr id="d7e5fa99-b3a0-4fd9-89eb-cdbd8bd9a95e"><td class="cell-title"><a href="https://www.notion.so/AnyMaze-d7e5fa99b3a04fd989ebcdbd8bd9a95e?pvs=21">AnyMaze</a></td><td class="cell-gBhX"><span class="status-value"><div class="status-dot"></div>N/A</span></td><td class="cell-@eqG"><span class="status-value"><div class="status-dot"></div>N/A</span></td><td class="cell-DaSw">100</td><td class="cell-_`ur">100</td><td class="cell-Ycxk"><span class="status-value select-value-color-red"><div class="status-dot status-dot-color-red"></div>NO</span></td><td class="cell-{azb"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Yes</span></td><td class="cell-@z;}"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Highly</span></td><td class="cell-tGcX">100</td><td class="cell-QPRV">100</td><td class="cell-BocC">2022</td></tr><tr id="3ea7b5f7-47b8-4845-9967-82ec557128d8"><td class="cell-title"><a href="https://www.notion.so/DeepPoseKit-3ea7b5f747b84845996782ec557128d8?pvs=21">DeepPoseKit</a></td><td class="cell-gBhX"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Available</span></td><td class="cell-@eqG"><span class="status-value"><div class="status-dot"></div>N/A</span></td><td class="cell-DaSw">60</td><td class="cell-_`ur">60</td><td class="cell-Ycxk"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>YES</span></td><td class="cell-{azb"><span class="status-value select-value-color-blue"><div class="status-dot status-dot-color-blue"></div>Incomplete</span></td><td class="cell-@z;}"><span class="status-value select-value-color-yellow"><div class="status-dot status-dot-color-yellow"></div>Scalable</span></td><td class="cell-tGcX">90</td><td class="cell-QPRV">80</td><td class="cell-BocC">2022</td></tr><tr id="663c0d41-c565-40ef-bc73-25c9601b89d7"><td class="cell-title"><a href="https://www.notion.so/DeepEthogram-663c0d41c56540efbc7325c9601b89d7?pvs=21">DeepEthogram</a></td><td class="cell-gBhX"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Available</span></td><td class="cell-@eqG"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Available</span></td><td class="cell-DaSw">50</td><td class="cell-_`ur">50</td><td class="cell-Ycxk"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>YES</span></td><td class="cell-{azb"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Yes</span></td><td class="cell-@z;}"><span class="status-value select-value-color-yellow"><div class="status-dot status-dot-color-yellow"></div>Scalable</span></td><td class="cell-tGcX">90</td><td class="cell-QPRV">80</td><td class="cell-BocC">2021</td></tr><tr id="38fdba8a-cc3f-486a-b071-c88cf6ab0c74"><td class="cell-title"><a href="https://www.notion.so/MARS-38fdba8acc3f486ab071c88cf6ab0c74?pvs=21">MARS</a></td><td class="cell-gBhX"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Available</span></td><td class="cell-@eqG"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Available</span></td><td class="cell-DaSw">70</td><td class="cell-_`ur">80</td><td class="cell-Ycxk"><span class="status-value select-value-color-yellow"><div class="status-dot status-dot-color-yellow"></div>Partially</span></td><td class="cell-{azb"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Yes</span></td><td class="cell-@z;}"><span class="status-value select-value-color-yellow"><div class="status-dot status-dot-color-yellow"></div>Scalable</span></td><td class="cell-tGcX">80</td><td class="cell-QPRV">80</td><td class="cell-BocC">2021</td></tr><tr id="07b13337-ed93-4818-817f-cab99e7e453a"><td class="cell-title"><a href="https://www.notion.so/VAME-07b13337ed934818817fcab99e7e453a?pvs=21">VAME</a></td><td class="cell-gBhX"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Available</span></td><td class="cell-@eqG"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Available</span></td><td class="cell-DaSw">40</td><td class="cell-_`ur">70</td><td class="cell-Ycxk"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>YES</span></td><td class="cell-{azb"><span class="status-value select-value-color-red"><div class="status-dot status-dot-color-red"></div>N/A</span></td><td class="cell-@z;}"><span class="status-value select-value-color-yellow"><div class="status-dot status-dot-color-yellow"></div>Scalable</span></td><td class="cell-tGcX">20</td><td class="cell-QPRV">90</td><td class="cell-BocC">2022</td></tr><tr id="98ec3f76-47d8-4622-8c75-376d445eb4ed"><td class="cell-title"><a href="https://www.notion.so/B-SOiD-98ec3f7647d846228c75376d445eb4ed?pvs=21">B-SOiD</a></td><td class="cell-gBhX"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Available</span></td><td class="cell-@eqG"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Available</span></td><td class="cell-DaSw">80</td><td class="cell-_`ur">80</td><td class="cell-Ycxk"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>YES</span></td><td class="cell-{azb"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Yes</span></td><td class="cell-@z;}"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Highly</span></td><td class="cell-tGcX">90</td><td class="cell-QPRV">85</td><td class="cell-BocC">2021</td></tr><tr id="2e319372-e014-4875-ab31-fedd50ec680d"><td class="cell-title"><a href="https://www.notion.so/BehaviorAtlas-2e319372e0144875ab31fedd50ec680d?pvs=21">BehaviorAtlas</a></td><td class="cell-gBhX"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Available</span></td><td class="cell-@eqG"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Available</span></td><td class="cell-DaSw">40</td><td class="cell-_`ur">50</td><td class="cell-Ycxk"><span class="status-value select-value-color-yellow"><div class="status-dot status-dot-color-yellow"></div>Partially</span></td><td class="cell-{azb"><span class="status-value select-value-color-red"><div class="status-dot status-dot-color-red"></div>N/A</span></td><td class="cell-@z;}"><span class="status-value select-value-color-yellow"><div class="status-dot status-dot-color-yellow"></div>Scalable</span></td><td class="cell-tGcX">50</td><td class="cell-QPRV">30</td><td class="cell-BocC">2021</td></tr><tr id="4c701e9c-504f-402d-8d89-4fe8ec07ac4d"><td class="cell-title"><a href="https://www.notion.so/Skeletal-Estimation-4c701e9c504f402d8d894fe8ec07ac4d?pvs=21">Skeletal Estimation</a></td><td class="cell-gBhX"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Available</span></td><td class="cell-@eqG"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>Available</span></td><td class="cell-DaSw">60</td><td class="cell-_`ur">70</td><td class="cell-Ycxk"><span class="status-value select-value-color-green"><div class="status-dot status-dot-color-green"></div>YES</span></td><td class="cell-{azb"><span class="status-value select-value-color-red"><div class="status-dot status-dot-color-red"></div>N/A</span></td><td class="cell-@z;}"><span class="status-value select-value-color-yellow"><div class="status-dot status-dot-color-yellow"></div>Scalable</span></td><td class="cell-tGcX">20</td><td class="cell-QPRV">85</td><td class="cell-BocC">2023</td></tr><tr id="1de84423-38d1-486d-831a-af0347836956"><td class="cell-title"><a href="https://www.notion.so/AVATAR-1de8442338d1486d831aaf0347836956?pvs=21">AVATAR</a></td><td class="cell-gBhX"><span class="status-value"><div class="status-dot"></div>N/A</span></td><td class="cell-@eqG"><span class="status-value"><div class="status-dot"></div>N/A</span></td><td class="cell-DaSw">10</td><td class="cell-_`ur">10</td><td class="cell-Ycxk"><span class="status-value select-value-color-red"><div class="status-dot status-dot-color-red"></div>NO</span></td><td class="cell-{azb"><span class="status-value select-value-color-red"><div class="status-dot status-dot-color-red"></div>N/A</span></td><td class="cell-@z;}"><span class="status-value select-value-color-yellow"><div class="status-dot status-dot-color-yellow"></div>Scalable</span></td><td class="cell-tGcX">95</td><td class="cell-QPRV">10</td><td class="cell-BocC">2022</td></tr></tbody></table><br/><br/></div><h1 id="ee931a5a-a041-42b6-9f92-b27a857e529e" class="">Overview</h1><p id="67c57841-f142-41b8-bd7e-2e21ef2e8c61" class="">When evaluating these above mentioned tools, there are several important criteria to consider such as user-friendliness, code availability, reproducibility, documentation, and real-time support.</p><p id="ae84242e-b783-47f5-b8a7-a31eebbd3ebf" class="">For user-friendliness, the technologies should be easy to use and understand, with minimal setup and configuration requirements. The code should be readily available and open source, allowing for maximum flexibility and allowing users to modify the code to their needs. Additionally, the code should be well-documented and easy to understand, with clear instructions on how to use the tool.</p><p id="fbaca82e-c3d3-41b8-9c9d-a2602a1b0571" class="">Reproducibility is also an important criterion, as it allows for results to be easily replicated and verified. Good documentation and code availability can help ensure reproducibility, while also making it easier for users to review and understand the results.</p><p id="220cb663-863a-48af-9a7f-2361fec8a406" class="">Additionally, real-time support is an important factor in evaluating animal pose estimation tools, as it can have a major impact on the usability and accuracy of the tool in applications that require real-time feedback.</p><p id="5ccea4d4-e16c-4467-b274-9b7cbc1dcb8a" class="">Overall, each of these criteria should be taken into account when evaluating animal pose estimation tools. By considering these criteria, users can make an informed decision about which tool is best for their needs.</p><h2 id="6e725e81-913b-4010-a5c2-c797b1684282" class="">DeepLabCut</h2><p id="8119b6d9-dbba-4ab0-a47d-fe4a5658561e" class="">DeepLabCut is based on the concept of Transfer Learning and uses ResNets, similar to those employed in DeeperCut, for feature extraction. As compared to DeeperCut, DeepLabCut has omitted certain elements such as Pairwise Refinement and Integer Linear Programming, resulting in a faster inference speed.  DeepLabCut has been tested to work across a range of animals with distinct morphology from humans. Additionally, it is an integral component of various above mentioned pose estimation pipelines that will be described later on. The overall architecture of the DeepLabCut is shown below:</p><figure id="db9350a3-7d1d-4c8d-8ab5-4baec913bcf3" class="image" style="text-align:center"><a href="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/Untitled.png"><img style="width:384px" src="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/Untitled.png"/></a><figcaption>DeepLabCut uses general feature extractors like ResNet50 and MobileNet as a feature extractor combined with a decoder that takes these deep low resolution features from the encoder and upsamples them to output a heatmap of the bodypart/keypoint </figcaption></figure><figure class="block-color-teal_background callout" style="white-space:pre-wrap;display:flex" id="901b3eb3-4d42-4b93-a273-59fbe80a7ca4"><div style="font-size:1.5em"><span class="icon">💪🏽</span></div><div style="width:100%">pros:<ul id="89376efc-34c6-4e04-be59-0bba5e2677fe" class="bulleted-list"><li style="list-style-type:disc">The DeepLabCut (DLC) is a highly popular and widely utilized tool in the scientific community for markerless tracking. As such, it has a comprehensive amount of documentation, as well as a large support community for seeking assistance and resolving any issues that may arise.</li></ul><ul id="dca358dc-0c71-4494-971a-676b2eb36c53" class="bulleted-list"><li style="list-style-type:disc">The predictions are quite accurate.</li></ul><ul id="7159d294-b1d1-4e75-91ba-fbc682f07703" class="bulleted-list"><li style="list-style-type:disc">The GUI interface is user-friendly and also includes a very intuitive labelling functionality to label keypoints and body parts.</li></ul><ul id="0d9d72c5-dd4a-44d3-a418-e60126ebfada" class="bulleted-list"><li style="list-style-type:disc">The bult-in functionality supports transcoding videos that are very helpful to visualize the prediction results that are </li></ul></div></figure><figure class="block-color-yellow_background callout" style="white-space:pre-wrap;display:flex" id="160eadc3-e7bc-4f0f-9951-0278a962a3a3"><div style="font-size:1.5em"><span class="icon">⚠️</span></div><div style="width:100%">Cons:<ul id="3627c09c-835e-4023-8cb5-e441bdf8faf1" class="bulleted-list"><li style="list-style-type:disc">DLC utilizes large models such as ResNe50 and MobileNet, resulting in a slow inference time that is not appropriate for real-time applications. While DLC Live is available for real-time inference, it is not commonly utilized by many methods that are already developed with offline DLC as their baseline model prior to DLC Live release.</li></ul><ul id="30981f70-d494-4675-9af3-07eef334b494" class="bulleted-list"><li style="list-style-type:disc">The 3D reconstruction currently only supports two cameras, and there are some drawbacks to the 3D pose estimation. For example, it can be difficult to accurately determine the exact position and orientation of an object in 3D space. Additionally, the accuracy of the reconstruction can be affected by the quality of the camera images. Finally, the 3D reconstruction process can be computationally intensive.</li></ul><ul id="648d7143-3d88-48c1-b4f5-9294cfb69244" class="bulleted-list"><li style="list-style-type:disc">Training time can be lengthy. Although there are many pre-trained models available on the ModelZoo, it can be difficult to find one that meets one’s specific needs.</li></ul></div></figure><figure id="b31b740c-e20f-43f1-bcbf-48804fa035f1"><div class="source"><a href="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/DLC_3D_-_Made_with_Clipchamp.mp4">https://s3-us-west-2.amazonaws.com/secure.notion-static.com/46f03737-64e4-4bf2-a026-601aa37d9693/DLC_3D_-_Made_with_Clipchamp.mp4</a></div></figure><h2 id="fb4b7336-d05d-4ded-9e34-11a5649cd141" class="">SLEAP</h2><p id="2a573def-0849-472e-88fb-5c67f8bbf685" class="">LEAP and SLEAP, developed by another group, are two of the alternatives to DeepLabCut that have been proven to achieve great performance for animal tracking. LEAP is based on an iterative, human-in-loop training scheme. A subset of frames from a video is extracted and manually labeled using an interactive GUI. The initial pose estimates on a portion of the frames can be predicted using this labeled data (~10 frames). Then, the original labels, along with manually refined predictions, can be iteratively trained to predict new frames. This process is repeated until the desired pose estimation performance is achieved.</p><p id="756cc31d-e7e0-40b1-9d28-8d6d4c96b1e4" class="">SLEAP is an extended version of LEAP for multi-animal tracking. The major thing that distinguishes SLEAP from DLC is its choice of architecture. Instead of relying on larger models like ReseNet and MobileNet, SLEAP uses a set of small specialized UNet-based data-specific architectures that specialize in key points detection in smaller and larger FOV subjects with fine and coarse features like rats/Mice and Flies respectively.</p><div id="de26b4e6-97dd-4e9c-8166-1c30d61df58b" class="column-list"><div id="6006ab4f-e339-43e0-9d74-a2abff8f63c6" style="width:50%" class="column"><figure id="fa96a157-1cee-4197-86ad-59689cd3fae8" class="image"><a href="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/Untitled%201.png"><img style="width:954px" src="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/Untitled%201.png"/></a></figure></div><div id="6b638926-bcee-4a6d-a06e-dfe9cd262825" style="width:50%" class="column"><figure id="4d60f356-1d9a-4a05-8aca-ac6172775cf8" class="image"><a href="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/Untitled%202.png"><img style="width:432px" src="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/Untitled%202.png"/></a></figure></div></div><p id="579616d8-0bd3-4fc2-8e7d-2ad71a0457c8" class="">The architecture used in SLEAP and the overall schema is shown below:</p><figure id="e3e124cf-6916-43ab-975d-24247a7926af" class="image"><a href="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/Untitled%203.png"><img style="width:624px" src="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/Untitled%203.png"/></a></figure><figure class="block-color-teal_background callout" style="white-space:pre-wrap;display:flex" id="8e26e461-13f1-4827-97d2-c41a28d7d161"><div style="font-size:1.5em"><span class="icon">💪🏽</span></div><div style="width:100%">pros:<ul id="e3791781-c848-4e80-8f7f-a1a35a4b8f28" class="bulleted-list"><li style="list-style-type:disc">Suitable for Real-time application.</li></ul><ul id="852fee9b-9bde-4520-9342-9c020b7b42b6" class="bulleted-list"><li style="list-style-type:disc">Smaller Architecture footprint that is faster to train.</li></ul><ul id="55b40fc0-c8fd-43eb-854a-2a7d4617ca1a" class="bulleted-list"><li style="list-style-type:disc">Accurate Predictions</li></ul><ul id="6323ba51-851b-420d-a5d6-e2c4bc6b3a8d" class="bulleted-list"><li style="list-style-type:disc">The GUI interface is user-friendly and also includes a very intuitive labeling functionality to label keypoints and body parts.</li></ul><ul id="8da5b882-18f5-41db-933d-ceca6256c1e0" class="bulleted-list"><li style="list-style-type:disc">Predicted keypoints visualization in a transcoded video</li></ul><ul id="1af347e1-0828-4f41-bd97-e637acd0e0b6" class="bulleted-list"><li style="list-style-type:disc">Good for multi-animal pose estimation with a bottom-up approach while preserving the animal&#x27;s identities.</li></ul></div></figure><figure class="block-color-yellow_background callout" style="white-space:pre-wrap;display:flex" id="879e3805-e3f6-4137-a818-92a43f1e8ff9"><div style="font-size:1.5em"><span class="icon">⚠️</span></div><div style="width:100%">Cons:<ul id="2832edfc-e627-49d5-af6f-9a884104d457" class="bulleted-list"><li style="list-style-type:disc">Doesn’t utilize transfer learning, therefore larger training data is required for better predictions and acceptable real-time application accuracy</li></ul><ul id="0628cd62-a49c-4dda-b601-d65b0c0b8a39" class="bulleted-list"><li style="list-style-type:disc">Doesn’t support 3D reconstruction.</li></ul><ul id="9e01d341-bbe2-4617-9c3a-4bd1f82dc57e" class="bulleted-list"><li style="list-style-type:disc">Not as well documented and supported as DLC</li></ul></div></figure><p id="bc2139e6-1840-41ae-b643-2413bd50e1c5" class="">
    </p><p id="4e9f71f6-1d52-4bdf-9f27-77d126236cc8" class="">having reviewed two of the most fundamental components in current pose estimation methods, we can describe and analyze the rest of the solutions listed above. Most of these use DLC and/or SLEAP as their baseline. </p><h2 id="082f29a4-328b-47bd-9ac7-a3d395099f3f" class="">Anipose:</h2><p id="014859f5-6382-49ff-93c9-1af143d32354" class="">Anipose is a modular solution for animal pose estimation, consisting of a 3D calibration module, a set of filters to resolve 2D detection errors, a triangulation module to obtain accurate 3D trajectories, and a pipeline for efficient video processing. </p><p id="a3ef2df5-582e-4167-ae80-3551681a4ed3" class="">Anipose extends the DLC to include support for more than 2 cameras. It is recommended to use 3-6 cameras for recording the subject from different angles for an accurate 3D reconstruction of the 2D pose estimation. However, including more cameras means more frames to label and even longer training time. </p><figure class="block-color-pink_background callout" style="white-space:pre-wrap;display:flex" id="83a1abc1-c0ea-4ce4-9a70-c7da1908ea23"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">Since it uses original offline DLC for 2D pose estimation, it comes with all the cons and a tedious camera calibration process that has to be repeated for each of the cameras installed. This makes adipose practical for only a few applications. </div></figure><p id="7cebd16f-cbe9-486a-958b-5a4d41e26de3" class="">
    </p><figure id="28146748-c499-41c4-9c4e-f03d8f58818f" class="image"><a href="https://anipose.readthedocs.io/en/latest/_images/tracking_3cams_full_slower5.gif"><img src="https://anipose.readthedocs.io/en/latest/_images/tracking_3cams_full_slower5.gif"/></a></figure><h2 id="9195be81-1d3a-4cb4-9101-9aa52a417d05" class="">DANNCE</h2><p id="3aeff634-15ef-4942-b280-a7469b3a2b22" class="">DANNCE is a 3D approach designed to track anatomical landmarks in various species and behaviors, using projective geometry and a convolutional neural network to construct inputs. It has been trained and benchmarked using a dataset of nearly seven million frames and has been extended to datasets from rat pups, marmosets, and chickadees, allowing for the quantitative profiling of behavioral lineage during development.</p><p id="a51a1ecb-15bd-408c-889c-aa55216c83a5" class="">Unlike the rest of the methods that use 2D key points input data taken from multiple views for triangulation to construct 3D poses, DANNCE uses projective geometry to construct a 3D input from 2D keypoints to train a 3D CNN for directly predicting 3D poses. This has been shown to greatly increase robustness.</p><p id="6754d250-7654-4feb-9da0-f81aeffc7e55" class="">It is important to notice that DANNCE uses its own developed labeling tool that can label frames taken from 6 camera views at the same time. </p><figure class="block-color-teal_background callout" style="white-space:pre-wrap;display:flex" id="6e17aa02-4a19-41d8-9b61-f70e8f0381a1"><div style="font-size:1.5em"><span class="icon">📌</span></div><div style="width:100%"><strong>The main advantage</strong> of using this labeling too is that it can take any keypoints that are labeled in any 2 of the views, triangulate them and project them onto the rest of the views. there are several advantages to this approach namely:<ul id="d6d8531d-4164-4bb2-869a-a161ca88ae5c" class="bulleted-list"><li style="list-style-type:disc">It reduced the amount of labeling time significantly.</li></ul><ul id="c2cb60da-fdec-4134-a585-85e78ab5fbef" class="bulleted-list"><li style="list-style-type:disc">It creates a spatial consistency between the one keypoint across all views. mean if you move key point in one view it also moves in the other view. This means that key points in all views accurately represent a single point in 3D space. and the 3D reconstruction error is greatly reduced.</li></ul><ul id="c5e45c30-bd68-4cf7-8ab2-1bca0911c0ab" class="bulleted-list"><li style="list-style-type:disc">The second most important advantage is that it enables users to label the obstructed keypoints/body parts accurately which is not possible to label in the rest of the pose estimation tools where we have to label the hidden parts based on guess or leave them altogether. </li></ul></div></figure><figure id="0d591441-3ab1-4974-9a91-f3de7c14b8cd"><div class="source"><a href="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/Media1.mp4">https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d447ffed-9212-490f-a64c-6ed0992d5069/Media1.mp4</a></div></figure><figure class="block-color-purple_background callout" style="white-space:pre-wrap;display:flex" id="851493c9-62d6-4513-a2fb-7af01a9d52bd"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">DANNCE does not have a user-friendly GUI and is implemented in Matlab. and it has not been tested in a real-time scenario.</div></figure><p id="ed667fc3-0058-453d-9c4a-a3b2b9cf2cf0" class="">
    </p><figure id="c2cb80a2-625d-493d-9be6-1372ef945720" class="image"><a href="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/Untitled%204.png"><img style="width:1495px" src="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/Untitled%204.png"/></a></figure><h2 id="c6eb0330-2983-43e1-bf7b-1e86479cfc28" class="">AnyMaze</h2><p id="06206d3c-6917-49fa-af3c-e6c51d502244" class="">AnyMaze is a commercial software for tracking rats in different lab environments and mazes. It comes equipped with a very familiar interface resembling Microsoft Office. As a commercial solution, It is designed to control, record, and analyze animal behavior in laboratory experiments. It can be used to conduct a variety of behavioral assays, including operant conditioning, classical conditioning, and passive avoidance tasks. </p><p id="ba2df3a0-ace8-4a05-94fa-b946d0a23a17" class="">The software provides a user-friendly interface for controlling stimuli presentation and recording animal behavior, and it also includes analysis tools for quantifying behavioral data. It is widely used in neuroscience and psychology research to study a range of topics, including learning and memory, attention, and decision making. </p><p id="9bea261d-129c-4ace-b616-113af4393c2f" class="">AnyMaze is developed to be used for a  variety of laboratory animals, including rodents (such as mice and rats) and larger animals like non-human primates. The software is highly flexible and can be adapted to suit the needs of different experimental paradigms and species. The system can be used to control and record behavior in both simple and complex tasks,</p><p id="4e2b2c5f-f75d-43b3-9a25-436f4419951e" class="">Despite not having the capability to perform pose estimation. It can be used for almost all the standard set of tasks and tests performed on laboratory animals. </p><figure id="8236d93d-e4dc-461e-b1d7-b7543a9b78f0" class="image"><a href="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/Untitled%205.png"><img style="width:1436px" src="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/Untitled%205.png"/></a></figure><figure class="block-color-teal_background callout" style="white-space:pre-wrap;display:flex" id="aae69cd2-b7ef-4b68-abdb-16e90def2b79"><div style="font-size:1.5em"><span class="icon">📌</span></div><div style="width:100%">AnyMaze is a complete solution for animal test tracking and analysis. The company also provide apparatus and other accessories that seamlessly integrate with the tracking software.  In the software, user can define different regions of the test arena or maze, referred to as zones, and observe the animal interactions with them,  and  view reports on the test results in form of intuitive plots, graphs and insights.<p id="818089e8-8162-4df3-98ef-e63118dce356" class=""> AnyMaze can be used to automate tests and run them in batches on up to 40 different apparatus simultaneously therefore, greatly increasing the throughput.</p><p id="a78430e0-5e0b-4e92-90f5-fac6bad87c06" class="">AnyMaze is a No-code platform thus no programming experience required</p></div></figure><p id="a9a50fbe-b8bf-4f83-a63f-dc39e2ff1c91" class="">
    </p><h2 id="ecf79de4-4d43-49dc-9454-2ef6c5dc92d4" class="">DeepPoseKit</h2><p id="1f5c0c81-9dbc-4fba-9228-dfd6e7f74010" class="">DeepPoseKit software is designed to be fast, flexible, and robust, with a high-level programming interface and graphical user-interface for annotations. The software is built using TensorFlow as a backend and is written in Python. The authors have also developed two new models for animal pose estimation, <strong>Stacked DenseNet</strong> and a modified version of Stacked Hourglass. These models are designed to be more efficient and accurate than previous models. It comes equipped with a newly  developed method to process model outputs called<strong> Subpixel Maxima </strong>to allow for fast and accurate keypoint predictions with subpixel precision. The proposed models incorporate a <strong>hierarchical posture graph </strong>to learn the multi-scale geometry between keypoints. The software package, models, and method have been compared to existing models and shown to perform well in terms of speed, accuracy, training time, and generalization ability. The code, documentation, and examples are available on GitHub.</p><div id="744e3278-e158-4baf-9128-161765a85fd2" class="column-list"><div id="1a88aa2e-2ea0-4ff9-9b61-3b9e42756c03" style="width:50%" class="column"><figure id="7673f022-c625-471f-bfb0-46f2a0b3f06f" class="image" style="text-align:right"><a href="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/zebra.gif"><img style="width:240px" src="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/zebra.gif"/></a></figure></div><div id="f67ec234-5246-4b41-80b0-c4c321997a66" style="width:50%" class="column"><figure id="5c471c3c-9817-4cf4-ac8e-67f362322f9f" class="image" style="text-align:left"><a href="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/locust.gif"><img style="width:240px" src="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/locust.gif"/></a></figure></div></div><p id="04a0d97f-23c7-4cdc-8568-cd3c55605b18" class="">
    </p><figure class="block-color-teal_background callout" style="white-space:pre-wrap;display:flex" id="ddbef954-8db3-43e6-a1bd-5c27843a043f"><div style="font-size:1.5em"><span class="icon">💪🏽</span></div><div style="width:100%">DeepPoseKit is a well rounded program for animal pose estimation that is built on top of the current latest trends in deep learning and computer vison. The models and architecures it offers are accurate and faster than the compteteros.</div></figure><figure class="block-color-yellow_background callout" style="white-space:pre-wrap;display:flex" id="18a2b9dc-9dd1-4a55-8829-6e567210ce1f"><div style="font-size:1.5em"><span class="icon">⚠️</span></div><div style="width:100%">It lack a completely polished UI with an end-to-end pose estimation solution.<ul id="b8626158-6360-4af5-b137-c48afce904a1" class="bulleted-list"><li style="list-style-type:disc"> It offers a labeling <em>UI</em>, but for the remaining operations such as training, selecting parameters and models, extracting frames, and visualizing results, one must use the code base provided in the Google Colab notebook..</li></ul><ul id="ba38fb4b-62aa-4a81-8225-39b01c35e40d" class="bulleted-list"><li style="list-style-type:disc">Installing a complex development environment can be a barrier to getting started with this tool. However, it&#x27;s worth the effort, as it will enable you to take full advantage of the tool&#x27;s features.</li></ul></div></figure><p id="dd52cf1e-5664-44f9-94f6-b175774fe85f" class="">
    </p><h2 id="5c6d6547-068f-4a43-8b41-36756487fa9b" class="">DeepEthogram</h2><p id="2f1801a9-f4ba-44ef-9dbb-83355e4a1d0b" class="">DeepEthogram is a tool that uses videos to predict the probability of certain behaviors. To use it, you must first train the flow generator on a set of videos without user input. Then, you must label each frame in a set of training videos for the presence of each behavior of interest. After that, the spatial and flow feature extractors will use the labels to create separate estimates of the probability of each behavior. The extracted feature vectors are then used to train the sequence models to make the final predictions. The models are trained in series by design, instead of all at once, to avoid overfitting and other issues.</p><p id="81eb94d1-e6a1-4de7-825c-35eb05bbdad9" class="">DeepEthogram employs a variety of deep neural networks to generate optic flow, compress the frames into features, and estimate the probability of each behavior. Its system consists of three versions that use different models, depending on the accuracy and speed required. For example, the first version of DeepEthogram utilizes convolutional neural networks to identify motion and structure in the frames and generate a prediction. The second version combines convolutional neural networks and long short-term memory networks, which are recurrent neural networks capable of remembering information from previous frames. The third version uses a combination of convolutional neural networks and support vector machines to process the frames and generate robust predictions.</p><figure id="becd1767-cf7c-4791-b54c-1738a5c60010" class="image"><a href="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/Untitled%206.png"><img style="width:617px" src="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/Untitled%206.png"/></a></figure><p id="0f46ea1b-b6f7-4a2b-ae00-3c8c05ffe446" class="">. </p><p id="8088f2ee-2714-48e7-bd3c-a6eb7d44922a" class="">
    </p><h2 id="5c9dc06f-1944-4205-9fc4-d0c88a0f1222" class="">MARS</h2><p id="6bbbc5ac-0995-496d-a16d-78c7ebe01fc9" class="">MARS is a system that uses deep learning to automatically detect and classify the behavior of mice in videos. It includes three tools: MARS itself, MARS_Developer, and BENTO. </p><ol type="1" id="f218dcbc-8e9c-4709-9dd5-6ddb6e814a94" class="numbered-list" start="1"><li>MARS itself is a pipeline for detecting and classifying the behavior of mice in videos, and comes with a pose estimator and behavior classifiers. </li></ol><ol type="1" id="6ea1f27d-b9ee-4a00-b882-6da3b50ddb39" class="numbered-list" start="2"><li>MARS_Developer is a tool for re-training MARS on new data.</li></ol><ol type="1" id="3498d607-ba94-4698-a39f-79eeb3bed332" class="numbered-list" start="3"><li> BENTO is a tool for managing, visualizing, and analyzing data, including neural recordings, videos, behavior annotations, and audio.</li></ol><p id="3e9bf48a-8b89-4d9a-8bd2-39614e615e77" class="">Thus, It’s an end-to-end computational pipeline for tracking, pose estimation, and behavior classification in interacting laboratory mice. MARS can detect attack, mounting, and close investigation behaviors in a standard resident-intruder assay. </p><p id="e1078d88-59c4-4bfc-a9cf-e62d7f39e1ed" class=""> MARS also includes a MATLAB-based GUI  (BENTO)for synchronous display of neural recording data, multiple videos, human/automated behavior annotations, spectrograms of recorded audio, pose estimations, and other relevant information</p><p id="5d8ffb8b-3831-4517-b5bd-c057314d37c7" class="">MARS can be used to monitor animal behavior over long periods of time. It has the ability to track and measure parameters such as speed, acceleration, and direction. Additionally, the software can detect subtle changes in behavior that may not be easily visible to the naked eye. The data collected can be used in a variety of ways, such as to explore the impact of environmental factors on animal behavior or to identify potential changes in behavior that may be indicative of disease. </p><p id="ef4ca26c-018d-431e-affb-b073c26105ba" class="">MARS comes pre-packaged with a pose estimator trained on manual keypoint annotations of 15,000 video frames of interacting mice, and a set of behavior classifiers trained to detect aggression, mounting, and close investigation behaviors.</p><figure id="6b160f5f-a9c2-4384-bc0a-ba792608a806" class="image"><a href="https://github.com/annkennedy/bento/blob/master/docs/tracking_demo.gif?raw=true"><img src="https://github.com/annkennedy/bento/blob/master/docs/tracking_demo.gif?raw=true"/></a></figure><h2 id="482d6ca1-7bb5-48af-92b1-67cae7c31cc7" class="">VAME</h2><p id="b9f4dfd3-faa9-4d39-9aed-56b8282d07e1" class="">VAME is a PyTorch-based deep learning framework for clustering behavioral signals obtained from pose-estimation tools like DLC. It leverages the power of recurrent neural networks (RNNs) to model sequential data. To learn the complex data distribution, we use the RNN in a variational autoencoder (VAE) setting to extract the latent state of the animal at each step of the input time series.</p><figure id="5c8bb9a4-7f5e-4117-9bb5-75782322160b" class="image"><a href="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/Untitled_video_-_Made_with_Clipchamp.gif"><img style="width:426px" src="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/Untitled_video_-_Made_with_Clipchamp.gif"/></a></figure><p id="9d4b803b-b0d0-459e-b7a8-f36e47f72b0a" class="">It consists of three bidirectional recurrent neural networks, to identify behavioral motifs. and uses a Hidden-Markov-Model to segment the continuous latent space into discrete behavioral motifs. </p><p id="2a914339-81ce-4195-bb16-85e181d0f086" class="">
    </p><h2 id="088b1512-4037-4857-bf6f-5292b5992b51" class="">B-SOiD</h2><p id="b58835ca-8e56-4554-b3ec-529c5f5cfa9a" class="">Similar to VAME, B-SOiD is an open-source, unsupervised algorithm that is designed to identify behavior patterns without user bias. It take pose estimation from DLC but uses a set of unsupervised algorithms different from VAME. It extracts features from these poses through their define algorithms and utilized UMAP for non-linear dimensionality reduction. </p><p id="f36eef80-9373-474b-ae79-1f53794c38ed" class="">Contrast to VAME B-SOiD comes nicely packages as a python streamlit app that is intuitive to run.</p><figure id="93f19e82-c081-42dd-94ff-235f15afa444" class="image"><a href="https://github.com/YttriLab/B-SOID/raw/master/demo/appv2_files/bsoid_mouse_openfield1.gif"><img src="https://github.com/YttriLab/B-SOID/raw/master/demo/appv2_files/bsoid_mouse_openfield1.gif"/></a></figure><p id="8d28e6d3-f1c1-44d9-9c2a-22f909a8ab97" class="">
    </p><h2 id="42144c5e-8f76-45a5-98d3-f57d6ab5f304" class="">BehaviorAtlas</h2><p id="703acd54-8c8e-47ba-aaea-37569575672e" class="">Behavior Atlas is a spatio-temporal decomposition framework for detecting behavioral phenotypes from 3D/2D continuous multidimensional motion features data input. It unsupervisedly decomposes movements (e.g. walking, running, rearing) and emphasizes temporal dynamics. The self-similarity matrix of movement segments describes structure, and enables dimensionality reduction and visualization to construct feature space. This helps to study evolution of movement sequences, higher-order behavior and behavioral state transitions.</p><p id="46088e26-fef3-4487-bb1a-1d45956e5fd2" class="">It comes equipped with a 3D pose estimation code that utilized DLC tracked 2D poses from multiple cameras to construct 3D pose using a MATLAB toolbox. </p><p id="567a1265-ca84-4d1c-8c3b-42f6d2c6d727" class="">BehaviorAtlas is specifically designed to analyze rodent behavior, which has a lot of variation and is hard to measure. It uses a two-step process to break down the continuous animal skeleton postural data into two parts: locomotion and non-locomotor movement. Then, applies a unsupervised clustering to figure out the structure of the behavior. The results are visualized using UMAP </p><figure id="5a225570-4a8e-4f1f-8ccc-12e41244045a"><div class="source"><a href="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/BehaviorAtlas-_Made_with_Clipchamp.mp4">https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ab8fc6f1-5236-4e3f-86b0-8aedd83f9d60/BehaviorAtlas-_Made_with_Clipchamp.mp4</a></div></figure><p id="17df33c2-4dfd-497c-8619-8e8e9377b3f7" class="">
    </p><h2 id="d5e755af-5e0d-4ccd-a7c9-c36ea725caf7" class="">Skeletal Estimation</h2><p id="a93edf11-8130-4119-accd-5a548944bb91" class="">In order to discuss the disparity between the limb kinematics detected by surface tracking using DLC and the underlying skeletal motion, This method utilizes anatomical constraint model to restrict the poses estimation markers into a skeletal and utilizes (Inertial Measurement Unit) IMU readings and (Magnetic Resonance Imaging) MRI scans to verify results.</p><p id="d464aed1-1fe2-4d50-b7c5-a9086d648fff" class="">It relies on DLC for 2D pose estimation which is then lifted to 3D via triangulation and then combined with anatomical constraint model  to do skeletal estimation.</p><p id="edf0d4dc-be04-4ac3-b168-5dfaa8a46115" class="">By inferring to the  skeleton based model constrained using anatomical principles such as joint motion limits, bone rotations, and temporal constraints; it can help better estimate the underlying skeletal of rat with different sizes.</p><figure id="cb53e60a-c912-4bf2-bc4a-c36b2a2f1894"><div class="source"><a href="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/41592_2022_1634_MOESM8_ESM.mp4">https://s3-us-west-2.amazonaws.com/secure.notion-static.com/52d6a614-8ee5-4714-afc5-5944ce61b716/41592_2022_1634_MOESM8_ESM.mp4</a></div></figure><p id="b0a8e62a-bc93-42ea-b238-74d38032971d" class="">
    </p><h2 id="9dfb7a9d-57ad-418d-9d3e-98bbc1b2ae65" class="">AVATAR</h2><p id="20efb947-6045-457b-8584-cb8737505655" class="">AVATAR, based on YOLOv4, is used to detect body parts on rats. These detected body parts are then used for 3D reconstruction with off-the-shelf computer vision algorithms. The results of the 3D reconstruction are visualized through an &quot;action skeletal&quot; of polygons, created by joining the detected 3D parts. </p><p id="d425bb37-a68d-43ce-afef-b2b7a0120c97" class="">For behavior&#x27;s classification, it uses LSTM models. </p><p id="3d95c80e-d4db-4c8f-abc8-7352d2606128" class="">Although being straightforward in nature, the setup was tested to derive phot stimulation rat’s ventral tegmental area (VTA) within 90ms latency.</p><p id="be6a90bf-5d36-4841-a5d1-8f88429955fa" class=""> </p><figure id="dbce1dae-86c0-4d61-a68e-b9c40540b1fe" class="image"><a href="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/Untitled%207.png"><img style="width:592px" src="/Current%20Opinion%20on%20Animal%20Pose%20Estimation%20and%20Beha%20a4821611be9f48c19cdef8bafc7cbbba/Untitled%207.png"/></a></figure><p id="3dbeb686-b8f3-4947-8fa1-71001685349d" class="">
    </p><h1 id="60a26119-1689-426a-ba60-65e57f975607" class="">Final Thoughts</h1><p id="68a33cca-12a1-4f56-8f83-492a6a68bc80" class="">There are a variety of methods available for animal pose estimation and behavior analysis. Each of these methods has its own unique advantages and disadvantages. DeepEthogram is a convolutional neural network-based system that is geared towards accuracy and speed. MARS combines both convolutional neural networks and long short-term memory networks for robust predictions. VAME and B-SOiD are unsupervised algorithms for clustering behavioral signals from pose-estimation tools like DLC. BehaviorAtlas separates skeletal kinematics into locomotor and non-locomotor movements for clustering rodent behavior. Skeletal Estimation utilizes an anatomical constraint model to restrict the poses estimation markers into a skeletal. Finally, AVATAR is a YOLOv4-based system that is used to detect body parts on rats and classify behavior using LSTM models.</p><p id="0dbbcb66-9096-4898-9f44-c052f47b9df2" class="">We have compiled an exhaustive list of high-impact papers on animal neuroethology for a comprehensive literature review.</p><p id="8b811b5e-c461-4bed-8f40-1a634a8bfdf6" class="">No matter which method you choose to use, it is important to consider the pros and cons of each system carefully. Additionally, it is important to consider the resources that are required to use each system and the time it takes to train and run the model. Ultimately, the best system for your application will depend on the data that you have, the accuracy and speed that is required, and the resources that are available.</p><p id="a1b406db-af13-4641-8f18-ba8dd40638f6" class="">In the near future, animal pose estimation is likely to become more accurate and efficient due to continued research and development. This could revolutionize the way we study animals in their natural habitat and in the laboratory.</p><figure class="block-color-pink_background callout" style="white-space:pre-wrap;display:flex" id="6a93f69f-038c-4cfc-9603-3a1a54f9b2d0"><div style="font-size:1.5em"><span class="icon">📌</span></div><div style="width:100%">Needless to say Animal pose estimation has come a long way in the last five years however, <strong>it still falls short of being a comprehensive, well-rounded solution for practical applications that requires minimal setup.</strong> This is because more focus has been placed on making pose models larger and more accurate, rather than doing the engineering work to make them fast and deployable everywhere. In Future , our mission is to design and optimize a model that leverages the best aspects of state-of-the-art architectures, while keeping inference times as low as possible. The result will be a model that can deliver accurate keypoints across a wide variety of poses, environments, and hardware setups. It will be the ideal choice for any practical application.</div></figure><figure class="block-color-teal_background callout" style="white-space:pre-wrap;display:flex" id="55aa79f8-f8a1-4a45-b8c4-8edf96c98ec1"><div style="font-size:1.5em"><span class="icon">✅</span></div><div style="width:100%">I have curated a list of papers related to the topic of <strong>Animal Pose estimation and Computational Neuro Ethology </strong>that can be accessed <a href="https://github.com/snawarhussain/ComputationalNeuroEthologyPapers/">here</a> 🔗</div></figure><p id="88f52fd1-9182-498e-b7d2-5b9fc2c30e74" class="">
    </p><p id="a8b281ed-fc32-4c4d-850e-8819e7f458ad" class="">
    </p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span>
    <!-- Citation Note -->
    <p class="citation"><h2> Cite this article as: </h2></p>
    <div class="citation-container">
        <code class="hljs">
@article{hussain2023animalPos,
  title   = "Current Opinion on Animal Pose Estimation and Behavior Analysis Tools",
  author  = "Snawar Hussain",
  journal = "snawarhussain.github.io",
  year    = "2023",
  url     = "snawarhussain.com//Current-Opinion-on-Animal-Pose-Estimation-Tools-A-Review/"
}</code>
        <button class="copy-code">copy</button>
</div>


    <script>
    function copyCitation() {
        var citationText = document.querySelector(".citation-container").innerText;
        var textarea = document.createElement("textarea");
        textarea.value = citationText;
        document.body.appendChild(textarea);
        textarea.select();
        document.execCommand("copy");
        document.body.removeChild(textarea);
        
        var copyButton = document.querySelector(".copy-code");
        copyButton.textContent = "copied";
        copyButton.classList.add("copied");

        setTimeout(() => {
            copyButton.textContent = "copy";
            copyButton.classList.remove("copied");
        }, 2000);
    }

    document.querySelector(".copy-code").addEventListener("click", copyCitation);
    </script>


</body></html>]]></content><author><name>Snawar Hussain</name></author><summary type="html"><![CDATA[Current Opinion on Animal Pose Estimation and Behavior Analysis Tools /* cspell:disable-file */ /* webkit printing magic: print all background colors */ html { -webkit-print-color-adjust: exact; } * { box-sizing: border-box; -webkit-print-color-adjust: exact; } html, body { margin: 0; padding: 0; } @media only screen { body { margin: 2em auto; max-width: 900px; color: rgb(55, 53, 47); } } body { line-height: 1.5; white-space: pre-wrap; } a, a.visited { color: inherit; text-decoration: underline; } .pdf-relative-link-path { font-size: 80%; color: #444; } h1, h2, h3 { letter-spacing: -0.01em; line-height: 1.2; font-weight: 600; margin-bottom: 0; } .page-title { font-size: 2.5rem; font-weight: 700; margin-top: 0; margin-bottom: 0.75em; } h1 { font-size: 1.875rem; margin-top: 1.875rem; } h2 { font-size: 1.5rem; margin-top: 1.5rem; } h3 { font-size: 1.25rem; margin-top: 1.25rem; } .source { border: 1px solid #ddd; border-radius: 3px; padding: 1.5em; word-break: break-all; } .callout { border-radius: 3px; padding: 1rem; } figure { margin: 1.25em 0; page-break-inside: avoid; } figcaption { opacity: 0.5; font-size: 85%; margin-top: 0.5em; } mark { background-color: transparent; } .indented { padding-left: 1.5em; } hr { background: transparent; display: block; width: 100%; height: 1px; visibility: visible; border: none; border-bottom: 1px solid rgba(55, 53, 47, 0.09); } img { max-width: 100%; } @media only print { img { max-height: 100vh; object-fit: contain; } } @page { margin: 1in; } .collection-content { font-size: 0.875rem; } .column-list { display: flex; justify-content: space-between; } .column { padding: 0 1em; } .column:first-child { padding-left: 0; } .column:last-child { padding-right: 0; } .table_of_contents-item { display: block; font-size: 0.875rem; line-height: 1.3; padding: 0.125rem; } .table_of_contents-indent-1 { margin-left: 1.5rem; } .table_of_contents-indent-2 { margin-left: 3rem; } .table_of_contents-indent-3 { margin-left: 4.5rem; } .table_of_contents-link { text-decoration: none; opacity: 0.7; border-bottom: 1px solid rgba(55, 53, 47, 0.18); } table, th, td { border: 1px solid rgba(55, 53, 47, 0.09); border-collapse: collapse; } table { border-left: none; border-right: none; } th, td { font-weight: normal; padding: 0.25em 0.5em; line-height: 1.5; min-height: 1.5em; text-align: left; } th { color: rgba(55, 53, 47, 0.6); } ol, ul { margin: 0; margin-block-start: 0.6em; margin-block-end: 0.6em; } li > ol:first-child, li > ul:first-child { margin-block-start: 0.6em; } ul > li { list-style: disc; } ul.to-do-list { padding-inline-start: 0; } ul.to-do-list > li { list-style: none; } .to-do-children-checked { text-decoration: line-through; opacity: 0.375; } ul.toggle > li { list-style: none; } ul { padding-inline-start: 1.7em; } ul > li { padding-left: 0.1em; } ol { padding-inline-start: 1.6em; } ol > li { padding-left: 0.2em; } .mono ol { padding-inline-start: 2em; } .mono ol > li { text-indent: -0.4em; } .toggle { padding-inline-start: 0em; list-style-type: none; } /* Indent toggle children */ .toggle > li > details { padding-left: 1.7em; } .toggle > li > details > summary { margin-left: -1.1em; } .selected-value { display: inline-block; padding: 0 0.5em; background: rgba(206, 205, 202, 0.5); border-radius: 3px; margin-right: 0.5em; margin-top: 0.3em; margin-bottom: 0.3em; white-space: nowrap; } .collection-title { display: inline-block; margin-right: 1em; } .page-description { margin-bottom: 2em; } .simple-table { margin-top: 1em; font-size: 0.875rem; empty-cells: show; } .simple-table td { height: 29px; min-width: 120px; } .simple-table th { height: 29px; min-width: 120px; } .simple-table-header-color { background: rgb(247, 246, 243); color: black; } .simple-table-header { font-weight: 500; } time { opacity: 0.5; } .icon { display: inline-block; max-width: 1.2em; max-height: 1.2em; text-decoration: none; vertical-align: text-bottom; margin-right: 0.5em; } img.icon { border-radius: 3px; } .user-icon { width: 1.5em; height: 1.5em; border-radius: 100%; margin-right: 0.5rem; } .user-icon-inner { font-size: 0.8em; } .text-icon { border: 1px solid #000; text-align: center; } .page-cover-image { display: block; object-fit: cover; width: 100%; max-height: 30vh; } .page-header-icon { font-size: 3rem; margin-bottom: 1rem; } .page-header-icon-with-cover { margin-top: -0.72em; margin-left: 0.07em; } .page-header-icon img { border-radius: 3px; } .link-to-page { margin: 1em 0; padding: 0; border: none; font-weight: 500; } p > .user { opacity: 0.5; } td > .user, td > time { white-space: nowrap; } input[type="checkbox"] { transform: scale(1.5); margin-right: 0.6em; vertical-align: middle; } p { margin-top: 0.5em; margin-bottom: 0.5em; } .image { border: none; margin: 1.5em 0; padding: 0; border-radius: 0; text-align: center; } .code, code { background: rgba(135, 131, 120, 0.15); border-radius: 3px; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; tab-size: 2; } code { color: #eb5757; } .code { padding: 1.5em 1em; } .code-wrap { white-space: pre-wrap; word-break: break-all; } .code > code { background: none; padding: 0; font-size: 100%; color: inherit; } blockquote { font-size: 1.25em; margin: 1em 0; padding-left: 1em; border-left: 3px solid rgb(55, 53, 47); } .bookmark { text-decoration: none; max-height: 8em; padding: 0; display: flex; width: 100%; align-items: stretch; } .bookmark-title { font-size: 0.85em; overflow: hidden; text-overflow: ellipsis; height: 1.75em; white-space: nowrap; } .bookmark-text { display: flex; flex-direction: column; } .bookmark-info { flex: 4 1 180px; padding: 12px 14px 14px; display: flex; flex-direction: column; justify-content: space-between; } .bookmark-image { width: 33%; flex: 1 1 180px; display: block; position: relative; object-fit: cover; border-radius: 1px; } .bookmark-description { color: rgba(55, 53, 47, 0.6); font-size: 0.75em; overflow: hidden; max-height: 4.5em; word-break: break-word; } .bookmark-href { font-size: 0.75em; margin-top: 0.25em; } .sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; } .code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; } .serif { font-family: Lyon-Text, Georgia, ui-serif, serif; } .mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; } .pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; } .pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; } .pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; } .pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; } .pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; } .pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; } .pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; } .pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; } .pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .highlight-default { color: rgba(55, 53, 47, 1); } .highlight-gray { color: rgba(120, 119, 116, 1); fill: rgba(120, 119, 116, 1); } .highlight-brown { color: rgba(159, 107, 83, 1); fill: rgba(159, 107, 83, 1); } .highlight-orange { color: rgba(217, 115, 13, 1); fill: rgba(217, 115, 13, 1); } .highlight-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 145, 47, 1); } .highlight-teal { color: rgba(68, 131, 97, 1); fill: rgba(68, 131, 97, 1); } .highlight-blue { color: rgba(51, 126, 169, 1); fill: rgba(51, 126, 169, 1); } .highlight-purple { color: rgba(144, 101, 176, 1); fill: rgba(144, 101, 176, 1); } .highlight-pink { color: rgba(193, 76, 138, 1); fill: rgba(193, 76, 138, 1); } .highlight-red { color: rgba(212, 76, 71, 1); fill: rgba(212, 76, 71, 1); } .highlight-gray_background { background: rgba(241, 241, 239, 1); } .highlight-brown_background { background: rgba(244, 238, 238, 1); } .highlight-orange_background { background: rgba(251, 236, 221, 1); } .highlight-yellow_background { background: rgba(251, 243, 219, 1); } .highlight-teal_background { background: rgba(237, 243, 236, 1); } .highlight-blue_background { background: rgba(231, 243, 248, 1); } .highlight-purple_background { background: rgba(244, 240, 247, 0.8); } .highlight-pink_background { background: rgba(249, 238, 243, 0.8); } .highlight-red_background { background: rgba(253, 235, 236, 1); } .block-color-default { color: inherit; fill: inherit; } .block-color-gray { color: rgba(120, 119, 116, 1); fill: rgba(120, 119, 116, 1); } .block-color-brown { color: rgba(159, 107, 83, 1); fill: rgba(159, 107, 83, 1); } .block-color-orange { color: rgba(217, 115, 13, 1); fill: rgba(217, 115, 13, 1); } .block-color-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 145, 47, 1); } .block-color-teal { color: rgba(68, 131, 97, 1); fill: rgba(68, 131, 97, 1); } .block-color-blue { color: rgba(51, 126, 169, 1); fill: rgba(51, 126, 169, 1); } .block-color-purple { color: rgba(144, 101, 176, 1); fill: rgba(144, 101, 176, 1); } .block-color-pink { color: rgba(193, 76, 138, 1); fill: rgba(193, 76, 138, 1); } .block-color-red { color: rgba(212, 76, 71, 1); fill: rgba(212, 76, 71, 1); } .block-color-gray_background { background: rgba(241, 241, 239, 1); } .block-color-brown_background { background: rgba(244, 238, 238, 1); } .block-color-orange_background { background: rgba(251, 236, 221, 1); } .block-color-yellow_background { background: rgba(251, 243, 219, 1); } .block-color-teal_background { background: rgba(237, 243, 236, 1); } .block-color-blue_background { background: rgba(231, 243, 248, 1); } .block-color-purple_background { background: rgba(244, 240, 247, 0.8); } .block-color-pink_background { background: rgba(249, 238, 243, 0.8); } .block-color-red_background { background: rgba(253, 235, 236, 1); } .select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); } .select-value-color-pink { background-color: rgba(245, 224, 233, 1); } .select-value-color-purple { background-color: rgba(232, 222, 238, 1); } .select-value-color-green { background-color: rgba(219, 237, 219, 1); } .select-value-color-gray { background-color: rgba(227, 226, 224, 1); } .select-value-color-transparentGray { background-color: rgba(227, 226, 224, 0); } .select-value-color-translucentGray { background-color: rgba(255, 255, 255, 0.0375); } .select-value-color-orange { background-color: rgba(250, 222, 201, 1); } .select-value-color-brown { background-color: rgba(238, 224, 218, 1); } .select-value-color-red { background-color: rgba(255, 226, 221, 1); } .select-value-color-yellow { background-color: rgba(253, 236, 200, 1); } .select-value-color-blue { background-color: rgba(211, 229, 239, 1); } .select-value-color-pageGlass { background-color: undefined; } .select-value-color-washGlass { background-color: undefined; } .checkbox { display: inline-flex; vertical-align: text-bottom; width: 16; height: 16; background-size: 16px; margin-left: 2px; margin-right: 5px; } .checkbox-on { background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E"); } .checkbox-off { background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E"); }]]></summary></entry><entry><title type="html">2D Fourier Transform and Complex Numbers in MR Physics</title><link href="https://snawarhussain.com/educational/mri%20technology/data%20analysis/2D-Fourier-Transform-K-space-and-MRI/" rel="alternate" type="text/html" title="2D Fourier Transform and Complex Numbers in MR Physics" /><published>2023-11-24T00:00:00+00:00</published><updated>2023-11-24T00:00:00+00:00</updated><id>https://snawarhussain.com/educational/mri%20technology/data%20analysis/2D-Fourier-Transform-K-space-and-MRI</id><content type="html" xml:base="https://snawarhussain.com/educational/mri%20technology/data%20analysis/2D-Fourier-Transform-K-space-and-MRI/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Magnetic Resonance Imaging (MRI) relies heavily on understanding the shift from 1D Fourier transforms, which deal with time-based signals, to 2D Fourier transforms, crucial for spatial imaging. This post aims to clarify the connection between 1D and 2D Fourier Transform  and later bulding upon these concepts we will explore concepts like K-spaceand MRI. Blending mathematical theory with practical code examples the goal is to enhance basic understanding of MR Physics and Simulation.</p>

<h2 id="the-core-principle-of-fourier-transform">The Core Principle of Fourier Transform</h2>

<p>The Fourier Transform operates on a simple yet profound idea: any signal, whether temporal or spatial, can be broken down into an infinite series of sinusoids. In essence, we can construct any signal using different frequencies of sinusoids. This concept is more than theoretical—it’s fundamental to applying Fourier Transforms.</p>

<p>As part of this exploration, we’ll reference a PIRL video that clearly demonstrates the relationship between K-space, Fourier Transform, and MRI. This post seeks to bring the video’s concepts to life through coding.</p>

<p>To better grasp Fourier Transform’s complexities, it’s beneficial to start with the basics of <a href="/blog/computational%20modeling/mr%20physics/Complex-Numbers-and-Rotations/">complex numbers and their role in rotations</a>, as well as the <a href="/mri%20analysis/signal%20processing/mathematical%20modeling/1D-Fourier-Transform-visual-guide/">simpler 1D Fourier Transform</a>. These foundational topics, explored in the provided links, are key to understanding Fourier Transform’s role in MRI.</p>

<h2 id="2d-fourier-transform--and-mri">2D Fourier Transform  and MRI</h2>
<p>In the realm of image analysis and MRI, the 2D Fourier Transform is a pivotal tool. Unlike 1D signals, which are functions of time, images are functions of space. This transition to spatial domain brings forth the concept of 2D Fourier Transform, which is crucial for MRI.</p>

<h2 id="the-2d-fourier-transform-in-imaging">The 2D Fourier Transform in Imaging</h2>

<p>The 2D Fourier Transform extends the principles of the 1D transform to two dimensions:</p>

\[F(k_x, k_y) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(x, y) e^{-2\pi i(k_xx + k_yy)} \, dx \, dy\]

<p>Here, \(f(x, y)\) is the spatial signal (image), while \(k_x\) and \(k_y\) represent spatial frequencies in the x and y directions, respectively. This transform decomposes an image into its frequency components, revealing how different spatial frequencies and orientations contribute to the overall image.</p>

<p>now \(F(k_x, k_y)\), unlike \(F(\omega)\) in 1D fourier transform,  is a 2D plane where these spatial frequencies are organized thus, each point in this plane maps to a specific spatial frequency within the object. This concept parallels the 1D Fourier transform, but extends it to a 2D framework. Here, we deal with 2D spatial coordinates (x, y) and corresponding 2D spatial frequency coordinates \((k_x, k_y )\) and each point in this plane, just like in \(F(\omega)\), has its own complex plane containing withinin a phasor that scales and shifts this 2D sinosoid. This spatial frequency information is critical in reconstructing the final MRI image.</p>

<p>upon adding all these scale and shifted 2D sinosoids we get the final 2D image just like we did in 1D fourier transform.</p>

<h2 id="ok-but-how-exactly-does-a-2d-sinosoid-look">OK but how exactly does a 2D sinosoid look?</h2>

<p>In \(F(k_x, k_y)\) plane stepping through the \(k_x\) and \(k_y\) coordinates creates 2D sinosoids  with varying ‘wiggles’ in x and y directions.</p>

<p>Understanding the appearance and behavior of 2D sinusoids in the context of the Fourier Transform requires a closer look at the \(F(k_x, k_y)\) plane. In this plane, navigating through various \(k_x\) and \(k_y\) coordinates generates a series of 2D sinusoids, each characterized by distinct patterns or ‘wiggles’ in both the x and y directions.</p>

<h3 id="the-influence-of-k_x-and-k_y-on-sinusoidal-patterns">The Influence of \(k_x\) and \(k_y\) on Sinusoidal Patterns</h3>

<p>Imagine a coded animation, similar to the one in the 1D Fourier transform blog, to better visualize this concept. As you increment the value of \(k_x\), you’ll notice an increase in the frequency of wiggles along the x-direction. Similarly, increasing \(k_y\) boosts the frequency of wiggles in the y-direction. This relationship is key to understanding how 2D sinusoids are formed and manipulated in Fourier space.</p>

<p>Each 2D sinusoid is  shaped by a corresponding phasor. This phasor adjusts both the amplitude and phase of the sinusoid. The resultant effect is a versatile range of sinusoidal waves, each uniquely contributing to the overall image reconstruction process in MRI.</p>

<p>The number of wiggles or oscillations in each direction correlates directly to a specific point in the \(F(k_x, k_y)\) plane. For instance, a sinusoid with 3 complete cycles  in the x-direction and 3 in the y-direction would correspond to a point in \(F(k_x, k_y)\) with \(k_x =3\) value and \(k_y =3\) value. This mapping is fundamental to how spatial frequencies are represented and manipulated in the Fourier Transform, particularly in applications like MRI, where precise spatial information is crucial.</p>

<p align="center">
<img src="/assets/images/2D_FT/2D_sin.gif" width="300" />
 <figcaption>Fig 1: 2D Sinosoid with changing phase  </figcaption>
</p>

<p>Here’s a part of animation that demonstrates the relation between point in \(F(k_x, k_y)\) plane and the 2D sinosoid it generates and the effect of phasor assosiated with each of the 2D sinosoid (spatical frequency) and how it scales and shifts the it.</p>

<p align="center">
<img src="/assets/images/2D_FT/FT_2D.gif" height="600" />
 <figcaption>Fig 2: 2D Sinosoid with changing phase  </figcaption>
</p>

<p>You can play around with the code for this animation on my github repo <a href="https://github.com/snawarhussain/2D_FT_Anim">here</a>.</p>

<p>As a proof of concept, let’s consider a Mario sprite image. We start by taking its Fourier transform, represented as:</p>

\[F(k_x, k_y) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(x, y) e^{-2\pi i(k_x x + k_y y)} \, dx \, dy\]

<p>Here, \(f(x, y)\) is our original Mario sprite image, and \(F(k_x, k_y)\) is its Fourier transform, with \(k_x\) and \(k_y\) representing spatial frequencies in the x and y directions, respectively.</p>

<p>To reconstruct the original image from its Fourier transform, we take each point in the \(F(k_x, k_y)\) plane, which is a complex number (phasor). The reconstruction process involves the following steps:</p>

<ol>
  <li>
    <p><strong>Constructing 2D Sinusoids:</strong> For each point \((k_x, k_y)\) in the Fourier transform, we create a corresponding 2D sinusoid in its complex exponential form:</p>

\[S(x, y; k_x, k_y) = e^{2\pi i(k_x x + k_y y)}\]
  </li>
  <li>
    <p><strong>Scaling with the Phasor:</strong> The phasor at each point in the \(F(k_x, k_y)\) plane, characterized by an amplitude \(A\) and phase \(\phi\), scales the corresponding 2D sinusoid. This scaling is represented as:</p>

\[S_{scaled}(x, y; k_x, k_y) = A \cdot e^{2\pi i(k_x x + k_y y) + \phi}\]
  </li>
  <li>
    <p><strong>Summing Spatial Frequencies:</strong> The final step involves summing these scaled and shifted sinusoids across all spatial frequencies to reconstruct the original image:</p>

\[f_{reconstructed}(x, y) = \sum_{k_x, k_y} S_{scaled}(x, y; k_x, k_y)\]
  </li>
</ol>

<p>This method demonstrates the practical application of Fourier Transform in image processing, showcasing how an image can be decomposed and then reconstructed using the principles of spatial frequencies and phase shifts.</p>

<p align="center">
<img src="/assets/images/2D_FT/mario_recons.gif" height="600" />
 <figcaption>Fig 3: 1: oringal image, 2: 2D FT of the image, 3: Indexing phasors, 4: 2D sine wave associated with each phasor. Addtion of the 2D Sinosoids  </figcaption>
</p>

<p>In Fig.3 as we keep adding the 2D sinosoids we get the final image. This is the essence of the Fourier Transform: decomposing a signal into its 2D sine patterns and then reconstructing it from these components.</p>]]></content><author><name>Snawar Hussain</name></author><category term="Educational" /><category term="MRI Technology" /><category term="Data Analysis" /><category term="MRI" /><category term="Fourier Transform" /><category term="Complex Numbers" /><category term="Image Processing" /><category term="Spatial Frequencies" /><category term="Coding in MRI" /><summary type="html"><![CDATA[Dive deep into the role of complex numbers and Fourier Transform in Magnetic Resonance Imaging (MRI), featuring practical coding examples and a detailed analysis of MR physics and simulation.]]></summary></entry><entry><title type="html">1D Fourier Transform: A Visual Guide for Decoding Signals with Complex Numbers</title><link href="https://snawarhussain.com/mri%20analysis/signal%20processing/mathematical%20modeling/1D-Fourier-Transform-visual-guide/" rel="alternate" type="text/html" title="1D Fourier Transform: A Visual Guide for Decoding Signals with Complex Numbers" /><published>2023-11-23T00:00:00+00:00</published><updated>2023-11-23T00:00:00+00:00</updated><id>https://snawarhussain.com/mri%20analysis/signal%20processing/mathematical%20modeling/1D-Fourier-Transform-visual-guide</id><content type="html" xml:base="https://snawarhussain.com/mri%20analysis/signal%20processing/mathematical%20modeling/1D-Fourier-Transform-visual-guide/"><![CDATA[<p>If you haven’t read my previous blog on the relation between complex numbers and rotations, I would highly recommend you to check it out <a href="/blog/computational%20modeling/mr%20physics/Complex-Numbers-and-Rotations/">here</a>. It will help you understand the concepts in this blog better.</p>

<h2 id="introduction">Introduction</h2>

<p>The Fourier Transform is a mathematical tool that transforms a signal from the time domain to the frequency domain, unveiling the different frequencies that constitute the signal. In its most general form, the Fourier Transform is given by:</p>

\[F(\omega) = \int_{-\infty}^{\infty} f(t) e^{-i \omega t} \, dt\]

<p>Here, \(F(\omega)\) is the Fourier Transform of \(f(t)\), with \(\omega\) representing frequency and \(t\) denoting time. The exponential term \(e^{-2\pi i \omega t}\) is a rotating complex sinusoid, which is crucial for extracting the frequency components from \(f(t)\).</p>

<h3 id="the-discrete-fourier-transform-dft">The Discrete Fourier Transform (DFT)</h3>

<p>In digital applications, we use the Discrete Fourier Transform (DFT) to handle sampled data:</p>

\[F(k) = \sum_{n=0}^{N-1} f(n) e^{ i \frac{kn}{N}}\]

<p>Here, \(f(n)\) is the sampled signal, \(N\) is the number of samples, and \(k\) is the frequency index in the DFT.</p>

<p>The Discrete Fourier Transform (DFT) is a transformative tool in signal processing, translating signals from the time domain into the frequency domain. While the equations of the Fourier Transform and DFT are familiar territory for many, the real magic lies in understanding what happens inside the DFT and how it reveals the intricate composition of any signal.</p>

<p>Fundamental principle behind fourier transform is that any signal that is changing (temporally or spatially) can be represented as an infinite sum of sinosoids. or in other words we can create any signal by adding different frequencies of sinosoids. This is the basis of fourier transform.</p>

<p>The easiest examples would be temporally varrying 1D signals</p>

<h2 id="inside-dft">Inside DFT</h2>
<p>The output of the DFT is a collection of complex numbers, each representing a unique sinusoid in the original signal. These complex numbers, or phasors, hold the key to both the amplitude and phase of these complex sinusoids, essentially encoding how each sinusoid is scaled and shifted.</p>

<h2 id="complex-phasors">Complex Phasors</h2>
<p>A phasor \(F(\omega = \omega_0)\) in the DFT output is more than just a number; it’s a vector in the complex plane.And in fourier transform plot we mostly show the magnitude of this phasor. For example: Figure 1 illustrates the Fourier Transform and Inverse Fourier Transform of a simple signal. In \(F(\omega)\) plot, the x-axis is the frequency index \(\omega =\omega_0\), while the y-axis is the magnitude of the phasor \(|F(\omega_0)|\).</p>

<p>The magnitude indicates the contribution of each sinusoid to the overall signal.</p>
<p align="center">
<img src="/assets/images/1D_FT/ft_ift.png" width="600" />
 <figcaption>Fig.1: Fourier and Inverse Fourier fransform</figcaption>
</p>

<p>Thus a phasor \(F(\omega_0)\):
\(F(\omega = \omega_0) = x + iy\)
with amplitude \(A\):
\(A =  \sqrt{x^2 + y^2}\)
and phase  \(\phi\):
\(\phi = \tan^{-1}(\frac{y}{x})\)</p>

<p>scales the base complex sinosoid \(e^{- i \omega_0 t}\) by a factor of \(A\) and shifts it by a phase of \(\phi\)
\(A e^{- i \omega_0 t + \phi 
}\)</p>

<p>Each of these sinusoids is a fundamental building block of the orignal signal, characterized by a specific frequency. The DFT decomposes the signal into these basic elements, revealing how each frequency contributes to the overall structure of the signal</p>

<p>Since each point alone \((\omega)\) in the Figure 1 is a complex number, thus every frequeny \(\omega\) in the \(F (\omega)\) plot has it’s own Argand plane with real and imaginary axis.
So we can extend our orignal  \(|F(\omega_0)|\)plot from Figure 1 to show that every frequency \(\omega_0\) contains within it a phasor \(F (\omega_0)\) that scales and shifts the sinosoid at \(\omega_0\) according to it’s contribution to the signal.</p>

<p align="center">
<img src="/assets/images/1D_FT/phasor.gif" width="600" />
 <figcaption>Fig 2: phasor associated with a particular frequency sinosoid and how it scales and shifts it. </figcaption>
</p>

<h2 id="sum-of-sinusoids">Sum of Sinusoids</h2>

<p>Ultimately, any  signal \(f(t)\) can be represented by a specific set of scaled and shifted sinusoids. And Summing over these sinosoids, governed by the principles encoded in the phasors, gives us back the original signal in its time-domain form. This is the essence of the DFT: decomposing a signal into its sinusoidal components and then reconstructing it from these components.</p>

<p align="center">
<img src="/assets/images/1D_FT/signal_ft.png" width="600" />
 <figcaption>Fig 3: Singal f(t) represented as phasors of distinct frequencies (Fourier Transform). </figcaption>
</p>

<h2 id="exploring-the-dft-with-coded-animation">Exploring the DFT with coded Animation</h2>

<p>Through animation, we can dynamically illustrate the transformative process of the DFT. The goal is to visually demonstrate how:</p>

<p>Each point in the \(F(\omega)\) plot corresponds to a certain frequency and an associated phasor.
These phasors define the amplitude and phase of sinusoids at these specific frequencies.
By summing these scaled and shifted sinusoids, we can reconstruct the original signal.</p>

<!-- put mp4 video that loops here  -->
<p align="center">
<img src="/assets/images/1D_FT/FT_1D_1.gif" width="600" />
 <figcaption>Fig 4: Python simulation for the phenomenon in Fig. 2  </figcaption>
</p>

<p>For further exploration and hands on experience I have provided the code for this animation on my github repo <a href="https://github.com/snawarhussain/1D_FT_Anim">here</a>.</p>

<p>This blogpost is a prereq to MRI simulation. These fundamental concepts will help us venture into the realm of 2D Fourier Transforms, K-Spaces and their pivotal role in applications like MRI, opening doors to a world where these concepts find profound applications.</p>

<h2 id="reference">Reference</h2>

<ol>
  <li><a href="https://youtu.be/R_4GuyJTzMo?t=350">The Fourier Theory in MRI </a></li>
</ol>]]></content><author><name>Snawar Hussain</name></author><category term="MRI Analysis" /><category term="Signal Processing" /><category term="Mathematical Modeling" /><category term="Fourier Transform" /><category term="MRI" /><category term="Complex Numbers" /><category term="Signal Decomposition" /><category term="Discrete Fourier Transform" /><category term="Phasors" /><category term="Coding in MRI" /><summary type="html"><![CDATA[An in-depth exploration of Fourier Transform and complex numbers in MRI. Understand the critical role these concepts play in signal processing and MR imaging through detailed examples and code.]]></summary></entry><entry><title type="html">Complex Numbers and Rotations: A Primer to Fourier Transform and MR Physics and Simulation</title><link href="https://snawarhussain.com/blog/computational%20modeling/mr%20physics/Complex-Numbers-and-Rotations/" rel="alternate" type="text/html" title="Complex Numbers and Rotations: A Primer to Fourier Transform and MR Physics and Simulation" /><published>2023-10-13T00:00:00+00:00</published><updated>2023-10-13T00:00:00+00:00</updated><id>https://snawarhussain.com/blog/computational%20modeling/mr%20physics/Complex-Numbers-and-Rotations</id><content type="html" xml:base="https://snawarhussain.com/blog/computational%20modeling/mr%20physics/Complex-Numbers-and-Rotations/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Complex numbers often appear mysterious to those who encounter them for the first time. Why would anyone need numbers that are “imaginary”? As it turns out, complex numbers are not only elegant but incredibly useful. They lie at the heart of many scientific and engineering disciplines, including signal processing, control theory, electromagnetism, and, of course, Fourier Transform and Magnetic Resonance (MR) physics.</p>

<h2 id="complex-numbers-and-rotations">Complex Numbers and Rotations</h2>

<p>Complex numbers and rotations are fundamental to the understanding of  the encoding of spatial information. In this Blogpost, we’ll go beyond mere equations and actively code to visualize these concepts. Through this hands-on approach, we’ll deepen our understanding of the core principles involved in not just MR Physics but many other physical concepts that invovle waves, frequencies, phase and rotations.
For a more intuivie understanding of complex numbers, I highly recommend <a href="https://www.youtube.com/watch?v=5PcpBw5Hbwo">3Blue1Brown’s video on the topic</a>. Here we will write code to visualize the concepts discussed in the video.</p>

<h2 id="what-are-complex-numbers">What are Complex Numbers?</h2>

<p>A complex number \(z\) is defined as \(z = a + bi\), where \(a\) and \(b\) are real numbers, and \(i\) is the imaginary unit with the property \(i^2 = -1\).</p>

\[z = a + bi\]

<h2 id="plotting-complex-numbers">Plotting Complex Numbers</h2>

<h3 id="the-complex-plane">The Complex Plane</h3>

<p>The real part is plotted along the x-axis, and the imaginary part along the y-axis. This 2D plane is known as the complex plane.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">markers</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.animation</span> <span class="kn">import</span> <span class="n">FuncAnimation</span>

<span class="k">def</span> <span class="nf">plot_arrow</span><span class="p">(</span><span class="n">complex_num</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)):</span>
        
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">xlim</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ylim</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s">'equal'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s">'both'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s">'both'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">xlim</span><span class="p">),</span><span class="mi">0</span><span class="p">,</span><span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">xlim</span><span class="p">)])</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">yticks</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">ylim</span><span class="p">),</span><span class="mi">0</span><span class="p">,</span><span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">ylim</span><span class="p">)])</span>
    <span class="c1"># Define a colormap (cmap) and normalize it
</span>    <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s">'viridis'</span><span class="p">)</span>
    <span class="n">norm</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">Normalize</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">complex_num</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">complex_num</span><span class="p">)):</span>
        <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">complex_num</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">real</span><span class="p">,</span> <span class="n">complex_num</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">imag</span>
        <span class="n">color</span> <span class="o">=</span> <span class="n">cmap</span><span class="p">(</span><span class="n">norm</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>

        <span class="n">ax</span><span class="p">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="c1"># Add a legend
</span>
    <span class="n">legends</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s">'z = </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">'</span> <span class="k">for</span> <span class="n">z</span> <span class="ow">in</span> <span class="n">complex_num</span><span class="p">]</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">legends</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s">'upper left'</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
    
    
    <span class="k">return</span> <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span>
    

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">complex_num</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="o">+</span><span class="mf">2j</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plot_arrow</span><span class="p">(</span><span class="n">complex_num</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">),</span><span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/euler_post/eluer_anim_1_0.png" alt="png" /></p>

<h3 id="scaling-complex-numbers">Scaling Complex Numbers</h3>

<p>We often need to scale complex numbers to unit magnitude. The magnitude (\(\lVert z \rVert\)) of a complex number \(z = a + bi\) is given by:</p>

\[\lVert z \rVert = \sqrt{a^2 + b^2}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Scaled complex numbers
</span><span class="n">mag</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">complex_num</span><span class="p">)</span>
<span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">angle</span><span class="p">(</span><span class="n">complex_num</span><span class="p">)</span>
<span class="n">scaled_complex_num</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">mag</span> <span class="o">*</span> <span class="n">complex_num</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plot_arrow</span><span class="p">(</span><span class="n">scaled_complex_num</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/images/euler_post/eluer_anim_2_0.png" alt="png" /></p>

<h3 id="eulers-formula-and-exponential-form-of-complex-numbers">Euler’s Formula and Exponential Form of Complex Numbers</h3>

<p>One of the most beautiful equations in mathematics is Euler’s formula:</p>

\[e^{ix} = \cos(x) + i \sin(x)\]

<p>This formula allows us to represent complex numbers in exponential form, providing a new way to understand rotation and scaling in the complex plane.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#alternative representation
</span><span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">angle</span><span class="p">(</span><span class="n">scaled_complex_num</span><span class="p">)</span>
<span class="n">exponential_form</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">1j</span> <span class="o">*</span> <span class="n">angle</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">real</span><span class="p">(</span><span class="n">exponential_form</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">imag</span><span class="p">(</span><span class="n">exponential_form</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plot_arrow</span><span class="p">(</span><span class="n">exponential_form</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/images/euler_post/eluer_anim_3_0.png" alt="png" /></p>

<h3 id="what-if-we-want-to-describe-rotations">what if we want to describe rotations?</h3>

<p>Euler’s formula is a powerful tool for describing rotations in the complex plane. Let’s say we want to rotate a complex number \(z\) by \(\theta\) radians. We can do this by multiplying \(z\) by \(e^{i\theta}\).
here we keep the \(z\) to be 1 and vary the theta from 0 to 2pi</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># what if we want to describe rotations? 
</span><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">8</span><span class="p">)</span>
<span class="n">complex_nums</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">1j</span> <span class="o">*</span> <span class="n">theta</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plot_arrow</span><span class="p">(</span><span class="n">complex_nums</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/euler_post/eluer_anim_4_0.png" alt="png" /></p>

<h3 id="ploting-the-real-and-imaginary-part-of-the-complex-number">Ploting the real and imaginary part of the complex number</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># plotting only the real part of the complex number
</span><span class="n">theta</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span>  <span class="c1"># 1 rotation
</span><span class="n">dt</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="o">+</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">complex_num</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">1j</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">*</span> <span class="n">dt</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">real</span><span class="p">(</span><span class="n">complex_num</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">'real'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">imag</span><span class="p">(</span><span class="n">complex_num</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">'imag'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/assets/images/euler_post/eluer_anim_5_0.png" alt="png" /></p>

<p>so basically \(e^{ix}=cos(x)+ isin(x)\)</p>

<h3 id="animation-showing-how-exponential-form-of-complex-number-can-be-used-to-describe-rotations-in-2d-complex-plane">Animation showing how exponential form of complex number can be used to describe rotations in 2D complex plane</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.animation</span> <span class="kn">import</span> <span class="n">FuncAnimation</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="k">class</span> <span class="nc">animate</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">complex_nums</span><span class="p">,</span> <span class="n">dt</span><span class="p">):</span>
        <span class="c1"># Figure setup
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dt</span> <span class="o">=</span> <span class="n">dt</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">complex_num</span> <span class="o">=</span> <span class="n">complex_nums</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fig</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ax</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ax</span><span class="p">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s">'equal'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s">'both'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s">'both'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">yticks</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

        <span class="c1"># Define a colormap (cmap) and normalize it
</span>        <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s">'viridis'</span><span class="p">)</span>
        <span class="n">color</span> <span class="o">=</span> <span class="n">cmap</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">arrow</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ax</span><span class="p">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">legend_text</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">([</span><span class="sa">f</span><span class="s">'z = </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">complex_num</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">'</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s">'upper left'</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
        <span class="c1"># Create the legend in the init function
</span>
    <span class="k">def</span> <span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">arrow</span><span class="p">.</span><span class="n">set_xy</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>  <span class="c1"># Initialize arrow at (0, 0)
</span>        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">arrow</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">legend_text</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">i</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">arrow</span><span class="p">.</span><span class="n">set_xy</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">complex_num</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">real</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">complex_num</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">imag</span><span class="p">]])</span>  <span class="c1"># Set arrow's vertices
</span>        <span class="c1"># Update the legend text
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">legend_text</span><span class="p">.</span><span class="n">get_texts</span><span class="p">()[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_text</span><span class="p">(</span><span class="sa">f</span><span class="s">'z = </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">complex_num</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">arrow</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">legend_text</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">anim</span> <span class="o">=</span> <span class="n">FuncAnimation</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fig</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">update</span><span class="p">,</span> <span class="n">init_func</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">init</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dt</span><span class="p">),</span> <span class="n">interval</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">blit</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">anim</span>

<span class="n">theta</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span>  <span class="c1"># 1 rotation
</span><span class="n">dt</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span> <span class="o">+</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">complex_num</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">1j</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">*</span> <span class="n">dt</span><span class="p">)</span>
<span class="n">anim</span> <span class="o">=</span> <span class="n">animate</span><span class="p">(</span><span class="n">complex_num</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>
<span class="n">an</span> <span class="o">=</span> <span class="n">anim</span><span class="p">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/euler_post/anim1.gif" alt="gif" /></p>

<h3 id="changing-the-direction-of-rotation">Changing the direction of rotation</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># including a minus sign in the exponential form will result in a rotation in the opposite direction
</span><span class="n">theta</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span>  <span class="c1"># 1 rotation
</span><span class="n">dt</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span> <span class="o">+</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">complex_num</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">1j</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">*</span> <span class="n">dt</span><span class="p">)</span>
<span class="n">anim</span> <span class="o">=</span> <span class="n">animate</span><span class="p">(</span><span class="n">complex_num</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>
<span class="n">an</span> <span class="o">=</span> <span class="n">anim</span><span class="p">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/euler_post/anim2.gif" alt="gif" /></p>

<h3 id="inlcude-frequency-component-to-the-complex-number-to-control-the-speed-of-rotation">Inlcude frequency component to the complex number to control the speed of rotation</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">f</span> <span class="o">=</span> <span class="p">.</span><span class="mi">3</span>  <span class="c1"># frequency
</span><span class="n">theta</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span>  <span class="c1"># 1 rotation
</span><span class="n">dt</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span> <span class="o">+</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">complex_num</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">1j</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">*</span><span class="n">f</span><span class="o">*</span> <span class="n">dt</span><span class="p">)</span>   <span class="c1"># exp(1j * 2*np.pi* f * t) or exp(1j * omega * dt)
</span>
<span class="n">anim</span> <span class="o">=</span> <span class="n">animate</span><span class="p">(</span><span class="n">complex_num</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>
<span class="n">an</span> <span class="o">=</span> <span class="n">anim</span><span class="p">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="/assets/images/euler_post/anim3.gif" alt="gif" /></p>

<h3 id="defining-a-sinonoidal-function">defining a sinonoidal function</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># defining a sinonoidal function
</span><span class="n">g_t</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">dt</span><span class="p">)</span><span class="o">+</span><span class="mf">0.5</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">g_t</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="/assets/images/euler_post/eluer_anim_6_0.png" alt="png" /></p>

<h3 id="scaling-the-rotations-with-the-function-gt">Scaling the rotations with the function \(g(t)\)</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># multiplying the complex number with the function g_t scales the rotating complex number
</span><span class="n">theta</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span>  <span class="c1"># 1 rotation
</span><span class="n">f</span> <span class="o">=</span> <span class="p">.</span><span class="mi">3</span>  <span class="c1"># frequency
</span><span class="n">dt</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span> <span class="o">+</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">complex_num</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">1j</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">*</span><span class="n">f</span><span class="o">*</span> <span class="n">dt</span><span class="p">)</span><span class="o">*</span><span class="n">g_t</span>

<span class="n">anim</span> <span class="o">=</span> <span class="n">animate</span><span class="p">(</span><span class="n">complex_num</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>
<span class="n">an</span> <span class="o">=</span> <span class="n">anim</span><span class="p">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="/assets/images/euler_post/anim4.gif" alt="gif" /></p>

<h3 id="why-not-vectors">Why Not Vectors?</h3>

<p>At this point, one might wonder, why not use vectors? Vectors can describe points in space and can be scaled and rotated. The answer lies in the algebraic closure of complex numbers and their ability to simplify many mathematical derivations. Complex numbers allow for a unified and elegant representation that turns complicated mathematical operations into simple algebraic ones, especially when it comes to Fourier Transform and MR physics.</p>

<h3 id="the-role-of-complex-numbers-in-fourier-transform">The Role of Complex Numbers in Fourier Transform</h3>

<p>Fourier Transform breaks down any signal into a sum of sinusoids. In essence, it transforms our signal into a new domain where it’s expressed as a linear combination of exponential functions. This is where complex numbers shine.</p>

\[X(f) = \int_{-\infty}^{\infty} x(t) \cdot e^{-i 2 \pi f t} \, dt\]

<p>The role of complex numbers here is twofold:</p>

<ol>
  <li>The sinusoidal functions are compactly represented using Euler’s formula.</li>
  <li>The rotation and scaling properties of complex numbers help in understanding how each frequency component contributes to the overall signal.</li>
</ol>

<h3 id="key-takeaways">Key Takeaways</h3>

<ol>
  <li>
    <p><strong>Complex Plane Representation</strong>: Complex numbers can be visualized in a 2D Cartesian coordinate system known as the complex plane. In this plane, the real part serves as the x-axis, while the imaginary part serves as the y-axis.</p>
  </li>
  <li>
    <p><strong>Scaling and Rotation</strong>: Similar to vectors, complex numbers can be scaled and rotated within the 2D complex plane. These operations are particularly useful for understanding various mathematical and physical phenomena.</p>
  </li>
  <li>
    <p><strong>Exponential Form</strong>: Euler’s formula provides an elegant exponential representation for complex numbers. This form is not only mathematically compact but also extremely useful for understanding the rotation and scaling of complex numbers over time.</p>
  </li>
  <li>
    <p><strong>Trigonometric Components</strong>: The rotation of a complex number inherently involves sinusoidal (sine) and cosinusoidal (cosine) components. This is best represented through Euler’s formula, \(e^{ix} = \cos(x) + i\sin(x)\).</p>
  </li>
  <li>
    <p><strong>Fourier Transform</strong>: Understanding these properties of complex numbers is crucial when delving into Fourier Transform. The Fourier Transform essentially decomposes a signal into a series of scaled and rotated complex numbers, which are essentially sinusoids in disguise.</p>
  </li>
</ol>]]></content><author><name>Snawar Hussain</name></author><category term="Blog" /><category term="Computational Modeling" /><category term="MR Physics" /><category term="Complex Numbers" /><category term="Fourier Transform" /><category term="MR Physics" /><category term="Rotations" /><category term="MRI" /><summary type="html"><![CDATA[Exploring of complex numbers and their role in Fourier Transform and Magnetic Resonance Physics with code. This guide elucidates the mathematical foundations and practical applications in MR imaging.]]></summary></entry></feed>