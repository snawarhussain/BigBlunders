<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://snawarhussain.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://snawarhussain.com/" rel="alternate" type="text/html" /><updated>2024-06-11T13:51:58+00:00</updated><id>https://snawarhussain.com/feed.xml</id><title type="html">Snawar Hussain</title><subtitle></subtitle><author><name>Snawar Hussain</name></author><entry><title type="html">GPT model Causal Multi-Head Attention Mechanism: Pure and Simple</title><link href="https://snawarhussain.com/educational/llms/Causal-Attention-Mechanism-Pure-and-Simple/" rel="alternate" type="text/html" title="GPT model Causal Multi-Head Attention Mechanism: Pure and Simple" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://snawarhussain.com/educational/llms/Causal-Attention-Mechanism-Pure-and%20Simple</id><content type="html" xml:base="https://snawarhussain.com/educational/llms/Causal-Attention-Mechanism-Pure-and-Simple/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>I have always found a disconnect between the theoretical understanding of attention mechanisms, the illustrations and their practical code implementation in models like GPT and wasn’t able to merge all the knowledge while I was trying to understand multi-head attention mechanism in-depth. Since transformers work on batched input, they things change up a bit and it is always confusion to relate the illustration and theory to the code implementation. In this post, I aim to bridge this gap by providing a simple and intuitive explanation of the vanilla as well as causal attention mechanism in GPT models.</p>

<p>Let’s break it down step-by-step, starting from the input sequence and moving through the entire process.</p>

<h2 id="mathematical-representation-of-attention-mechanism">Mathematical Representation of Attention Mechanism</h2>

<p>We all know the attention mechanism in transformers can be mathematically represented as follows:</p>

\[Y = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V\]

<p>here \(Q\), \(K\), and \(V\) are the query, key, and value matrices, respectively. The softmax function is applied to the scaled dot-product of \(Q\) and \(K\), divided by the square root of the dimension of the key matrix. The output is then multiplied by the value matrix to obtain the final output \(Y\). 
Here i am interested in the dimensions of the input data as well as that of \(Q, K\) and \(V\) and how they are used in the attention mechanism. and what dimension the operations in the attention mechanism are performed on.</p>

<h2 id="input-data-representation">Input Data Representation</h2>
<p>as we know the input data to the transformer model is represented as a sequence of tokens. Each token is embedded into a vector of dimension <code class="language-plaintext highlighter-rouge">n_embd</code>. The input is represented as a matrix X of shape (<code class="language-plaintext highlighter-rouge">batch_size, seq_length,  n_embd</code>). This matrix is then passed through linear transformations to obtain the query, key, and value matrices \(Q\), \(K\), and \(V\), respectively.</p>

<p>Given that we have a trained tokenizer, tokenized the input sequence and passed it through an input embedding layer (<code class="language-plaintext highlighter-rouge">nn.embedding()</code>), we can represent the input data as a matrix \(X\) of shape (<code class="language-plaintext highlighter-rouge">batch_size, seq_length, n_embd</code>). This matrix contains the embeddings of the input tokens, where each row corresponds to a token and each column represents the embedding dimension.</p>

<p>suppose we have an input <strong>X</strong> with batch size is <strong>4</strong>, with sequence tokens (<code class="language-plaintext highlighter-rouge">eq_length</code>) <strong>12</strong>, after passing it through our tokenizer and embedding layer, we end up with final input of <code class="language-plaintext highlighter-rouge">[4,12,6]</code> (<code class="language-plaintext highlighter-rouge">batch_size, seq_length, n_embd</code>) The input data matrix \(X\) is represented as shown in the figure 01 below.</p>

<!-- html for putting svg figure -->
<figure class="styled-figure">
  <img src="/assets/images/atten_mech/attn_01.svg" alt="Input Data Matrix X" />
  <figcaption>Figure 01: Input Data Matrix X: where each row corresponds to a token and each column represents the embedding dimension</figcaption>
</figure>

<p>In the figure 01 above, the \(12 \times 6\) matrix represents a sinlge batch of the input data matrix \(X\), where each row corresponds to a token and each column represents the embedding dimension. we have a total of 4 batches of the input data matrix \(X\).</p>

<p><!--  highlight this part of the blog --></p>

<h3 id="multi-head-self-attention">Multi-Head Self-Attention</h3>

<blockquote>
  <p>Many blogs and articles mention that the input X is projected into queries, keys, and values of equal dimension, which are then used to compute attention based on a given equation. They often state that this process is repeated in parallel through multi-head attention.</p>
</blockquote>

<div style="border-left: 5px solid #007bff; padding: 10px; margin: 20px 0;">


However, in practice, multi-head attention is implemented more cleverly. The primary goal is to allow the model to attend to information from different representation subspaces simultaneously at different positions. This approach enables the model to learn richer and more diverse representations of the input data.

</div>

<p>This concept will become clearer once we delve into the implementation details of multi-head attention.</p>

<p>Since each token as a an embedding of dimension <code class="language-plaintext highlighter-rouge">n_embd</code> which is 6 in our case, what is done is we further divide the each batch of \(12 \times 6\) (<code class="language-plaintext highlighter-rouge">seq_length, n_embd</code>) matrix into a ([<code class="language-plaintext highlighter-rouge">seq_length, num_h ,h_size</code>])
so embedding dimension is divided into <code class="language-plaintext highlighter-rouge">num_h</code> heads, where each head has a dimension of <code class="language-plaintext highlighter-rouge">h_size</code>. This is done by reshaping the input data matrix \(X\) into a tensor of shape (<code class="language-plaintext highlighter-rouge">batch_size, seq_length, num_h, h_size</code>). and for each batch we reshape it into [<code class="language-plaintext highlighter-rouge">num_h, seq_length, h_size</code>]</p>

<p>so in order for multi-head attention to work, the embedding dimension <code class="language-plaintext highlighter-rouge">n_embd</code> should be divisible by the number of heads <code class="language-plaintext highlighter-rouge">num_h</code>.  in our case the embedding dimension is 6 and we have decide number of heads to be 3, so we end up with a tensor of shape <code class="language-plaintext highlighter-rouge">[3,12,2]</code> (<code class="language-plaintext highlighter-rouge">num_h, seq_length,  h_size</code>) for each batch. as shown in figure 02 below.</p>

<figure class="styled-figure">
  <img src="/assets/images/atten_mech/attn_02.svg" height="600" />
  <figcaption>Figure 02:the embedding dimension for each batch is divided into multiple heads, allowing the model to attend to different aspects of the tokens simultaneously</figcaption>
</figure>

<p>For each batch of the input data matrix X of shape <code class="language-plaintext highlighter-rouge">[12,6]</code> passed through linear transformations to obtain the query, key, and value matrices Q, K, and V, each of shape <code class="language-plaintext highlighter-rouge">[12,6]</code> respectively. \(QKV\) are then reshaped it into a tensor of shape <code class="language-plaintext highlighter-rouge">[3,12,2]</code> (<code class="language-plaintext highlighter-rouge">num_h, seq_length,  h_size</code>).</p>

<p>Did you notice something ? Instead of projecting the input data X into multiple \(QKV\) matrices for each head , we do the projection once and divide the embedding dimension into multiple heads.</p>

<p class="notice--info"><strong>Info:</strong> Did you notice something ? Instead of projecting the input data \(X\) into multiple \(QKV\) matrices for each head , we do the projection once and divide the embedding dimension into multiple heads. The reason for this is that it allows the model to attend to information from different representation subspaces simultaneously at different positions. This approach enables the model to learn richer and more diverse representations of the input data.</p>

<blockquote>
  <p>Since embeddings of each token represent a different aspect of the token, dividing the embedding dimension into multiple heads allows the model to attend to different aspects (sub embedding space) of the token simultaneously.</p>
</blockquote>

<blockquote>
  <p>This way, One aspect of a token might be more attentive to a certain aspect of an other token while a different aspect of the same token might learn to pay attention to certain aspect of a different token in a different head.  This is the essence of multi-head attention.</p>
</blockquote>

<p>We project each batch into \(QKV\) matrices, each of shape <code class="language-plaintext highlighter-rouge">[12,6]</code> (<code class="language-plaintext highlighter-rouge">seq_length,  n_embd</code>). The \(QKV\) matrices are then reshaped into a tensor of shape <code class="language-plaintext highlighter-rouge">[3,12,2]</code> (<code class="language-plaintext highlighter-rouge">num_h, seq_length,  h_size</code>) for each batch.</p>

<p>and inside each head, we first compute the scaled dot-product attention between \(Q\) <code class="language-plaintext highlighter-rouge">[12,2]</code> and \(K\) <code class="language-plaintext highlighter-rouge">[12,2]</code> to get the attention scores of shape <code class="language-plaintext highlighter-rouge">[12,12]</code> as shown in figure 03 below.</p>

<figure class="styled-figure">
  <img src="/assets/images/atten_mech/attn_03.svg" alt="Input Data Matrix X" />
  <figcaption>Figure 03: Scaled Dot-Product Attention Q@K for Each Head</figcaption>
</figure>

<p>basically – because we chopped up the original token embeddings of each token into multiple heads of size <code class="language-plaintext highlighter-rouge">h_size</code>– for each head we get different attention scores since we are attending to different aspects of each token in each head. as highlighted in the figure 04 below.</p>

<figure class="styled-figure">
  <img src="/assets/images/atten_mech/attn_05.svg" alt="Input Data Matrix X" />
  <figcaption>Figure 04: Different aspect of the tokens give rise to unique attention score matrices</figcaption>
</figure>

<p>The attention scores are then passed through the softmax function to obtain the attention weights. that are then multiplied by the value matrix \(V\) <code class="language-plaintext highlighter-rouge">[12, 2]</code> to obtain the output of the head \(y\) of shape <code class="language-plaintext highlighter-rouge">[12,2]</code> (figure 04).</p>

<figure class="styled-figure">
  <img src="/assets/images/atten_mech/attn_04.svg" alt="Input Data Matrix X" />
  <figcaption>Figure 05: Attention score is multiplied with V to aggregate context between concepts (sub-embedding of each token in that head)  </figcaption>
</figure>

<p>This process is done for each of 3 heads, and the outputs are concatenated and reshaped to ge the final output of the multi-head attention mechanism as shown in figure 06 below.</p>

<figure class="styled-figure">
  <img src="/assets/images/atten_mech/attn_06.svg" alt="Input Data Matrix X" />
  <figcaption>Figure 06: partial Context aggregation at sub-embedding level of each head are combined to get the final context aware embeddings of tokens</figcaption>
</figure>

<h3 id="causal-attention-mechanism">Causal Attention Mechanism</h3>

<p>The causal attention mechanism is a variant of the multi-head attention mechanism that restricts the model from attending to tokens that come after the current token. This is achieved by masking the attention scores of the tokens that come after the current token (Figure 07). The masking is done by setting the attention scores to negative infinity <code class="language-plaintext highlighter-rouge">-inf</code> before passing them through the softmax function and multiplying it with the \(V\) matrix.</p>

<figure class="styled-figure">
  <img src="/assets/images/atten_mech/attn_07.svg" alt="Input Data Matrix X" />
  <figcaption>Figure 07: Each token (in rows) only have the attention score for tokens before it to prevent looking into the future </figcaption>
</figure>

<h3 id="code-implementation">Code Implementation</h3>

<p>The code implementation of the multi-head attention mechanism in PyTorch is shown below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CausalSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span><span class="n">GPTConfig</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span> <span class="o">%</span> <span class="n">config</span><span class="p">.</span><span class="n">n_head</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">c_attn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="o">*</span><span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">c_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">n_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_embd</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s">'bias'</span><span class="p">,</span>
                             <span class="n">tril</span><span class="p">(</span><span class="n">ones</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">)).</span><span class="n">view</span><span class="p">(</span>
                                    <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> 
                                    <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">,</span>
                                    <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">))</span> <span class="c1"># create a lower triangular matrix of ones
</span>        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>   <span class="c1"># B: batch size, T: sequence length, C: n_embd [4, 12,6]
</span>        
        <span class="n">qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">c_attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># [4, 12, 6] -&gt; [4, 12, 6*3] -&gt; [4, 12, 18]
</span>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># [4, 12, 6], [4, 12, 6], [4, 12, 6]
</span>        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">C</span><span class="o">//</span><span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># [4, 12, 3, 2] transpose-&gt; [4, 3, 12, 2] 
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">C</span><span class="o">//</span><span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># [4, 12, 3, 2] transpose-&gt; [4, 3, 12, 2] 
</span>        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">C</span><span class="o">//</span><span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="c1"># [4, 12, 3, 2] transpose-&gt; [4, 3, 12, 2] 
</span>        <span class="n">att</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">k</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span> <span class="c1"># [4, 3, 12, 12]
</span>        <span class="n">att</span> <span class="o">=</span> <span class="n">att</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">[:,:,:</span><span class="n">T</span><span class="p">,:</span><span class="n">T</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span> <span class="c1"># [4, 3, 12, 12]replace zero with -inf
</span>        <span class="n">att</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">att</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># [4, 3, 12, 12] softmax
</span>        <span class="n">y</span> <span class="o">=</span> <span class="n">att</span> <span class="o">@</span> <span class="n">v</span> <span class="c1"># [4, 3, 12, 12] @ [4, 3, 12, 2] -&gt; [4, 3, 12, 2]
</span>        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="n">contiguous</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span> <span class="c1"># [4, 3, 12, 2] transpose-&gt; [4, 12, 3, 2] view-&gt; [4, 12, 6]
</span>        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">c_proj</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># [4, 12, 6] -&gt; [4, 12, 6] learnable linear layer
</span>        <span class="k">return</span> <span class="n">y</span>   <span class="c1"># [4, 12, 6]
</span></code></pre></div></div>

<h4 id="initialization">initialization</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CausalSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">GPTConfig</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span> <span class="o">%</span> <span class="n">config</span><span class="p">.</span><span class="n">n_head</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">c_attn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">c_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">n_head</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_embd</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s">'bias'</span><span class="p">,</span>
                             <span class="n">tril</span><span class="p">(</span><span class="n">ones</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">)).</span><span class="n">view</span><span class="p">(</span>
                                    <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">))</span>
</code></pre></div></div>

<p><span style="color:#d36135; font-weight: bold">Asserting Divisibility:</span></p>

<p>Ensure that the embedding dimension (n_embd) is divisible by the number of heads (n_head). This is important for splitting the embeddings into multiple heads.
Linear Layers for Projections:</p>

<p><span style="color:#d36135; font-weight: bold">self.c_attn:</span></p>

<p>A linear layer to project the input into queries, keys, and values. The output dimension is three times the embedding dimension to accommodate Q, K, and V.</p>

<p><span style="color:#d36135; font-weight: bold">self.c_proj:</span></p>

<p>A linear layer to project the concatenated outputs of the multi-head attention back to the original embedding dimension.</p>

<p><span style="color:#d36135; font-weight: bold">Registering the Causal Mask:</span></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="p">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s">'bias'</span><span class="p">,</span> <span class="n">tril</span><span class="p">(</span><span class="n">ones</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">)).</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">))</span>
</code></pre></div></div>
<p>Creates a lower triangular matrix of ones using <code class="language-plaintext highlighter-rouge">tril(ones(config.block_size, config.block_size))</code> to serve as a causal mask. 
The name is said to ‘bias’ to match the naming scheme of GPT to lead pre-trained weight ^_~</p>

<p>The mask is reshaped and registered as a buffer, which means it won’t be updated during training but is persistent in the model’s state. ( some mumbo jumbo for leading GPT weights more on this in the next post or )</p>

<h4 id="forward-pass">Forward Pass</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>
    
    <span class="n">qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">c_attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">att</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">k</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">att</span> <span class="o">=</span> <span class="n">att</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">T</span><span class="p">,</span> <span class="p">:</span><span class="n">T</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span>
    <span class="n">att</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">att</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">att</span> <span class="o">@</span> <span class="n">v</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="n">contiguous</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">c_proj</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>
</code></pre></div></div>

<p><span style="color:#d36135; font-weight: bold">Input Dimensions:</span></p>

<p>B, T, C = x.size(): Extract the batch size (B), sequence length (T), and embedding dimension (C) from the input tensor x.</p>

<p><span style="color:#d36135; font-weight: bold">Linear Projection to Q, K, V:</span></p>

<p><em>qkv = self.c_attn(x):</em> Apply the linear layer to project the input into queries (q), keys (k), and values (v).
<em>q, k, v = qkv.split(C, dim=2):</em> Split the concatenated qkv tensor into separate q, k, and v tensors.
Reshaping and Transposing for Multi-Head Attention:</p>

<p><em>k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2):</em> Reshape k to [B, T, nh, hs] and then transpose to [B, nh, T, hs].</p>

<p>Similar operations are performed for q and v.</p>

<p><span style="color:#d36135; font-weight: bold">Scaled Dot-Product Attention:</span></p>

<p><em>att = (q @ k.transpose(-2, -1)) * (k.size(-1) ** -0.5):</em> Compute the attention scores using the dot product of q and k, scaled by the square root of the head size.
<em>att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(‘-inf’)):</em> Apply the causal mask to ensure that each position can only attend to previous positions.
<em>att = nn.functional.softmax(att, dim=-1):</em> Apply softmax to obtain the attention weights.</p>

<p><span style="color:#d36135; font-weight: bold">Apply Attention Weights to Values:</span></p>

<p><em>y = att @ v:</em> Compute the weighted sum of the values using the attention weights.</p>

<p><span style="color:#d36135; font-weight: bold">Combining Heads:</span></p>

<p><em>y = y.transpose(1, 2).contiguous().view(B, T, C):</em> Transpose and reshape the output to combine the heads back into the original embedding dimension.</p>

<p><span style="color:#d36135; font-weight: bold">Final Linear Projection:</span></p>

<p><em>y = self.c_proj(y):</em> Apply the final linear projection to produce the output of the attention mechanism.</p>

<link rel="stylesheet" href="/assets/css/atten_mech/style.css" />]]></content><author><name>Snawar Hussain</name></author><category term="Educational" /><category term="LLMs" /><category term="GPT" /><category term="Generative Models" /><category term="Attention Mechanism" /><category term="Multi-Head Attention" /><category term="Transformers" /><category term="Causal Attention" /><category term="Self-Attention" /><category term="Machine Learning" /><category term="Deep Learning" /><category term="AI" /><summary type="html"><![CDATA[Exploring the crucial role of gradient fields in MRI for stepping through K-space.]]></summary></entry><entry><title type="html">Getting TensorFlow to Work with GPU in Conda Environment on Linux or WSL</title><link href="https://snawarhussain.com/blog/linux/linux-tensorflow-with-cuda-in-conda-environment/" rel="alternate" type="text/html" title="Getting TensorFlow to Work with GPU in Conda Environment on Linux or WSL" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://snawarhussain.com/blog/linux/linux-tensorflow-with-cuda-in-conda-environment</id><content type="html" xml:base="https://snawarhussain.com/blog/linux/linux-tensorflow-with-cuda-in-conda-environment/"><![CDATA[<p>Let’s admit it, installing TensorFlow with CUDA support is a pain in the neck and doesn’t work right away on the first attempt 99% of the time. Many of us have faced the frustration of seeing TensorFlow fail to utilize the GPU even though <code class="language-plaintext highlighter-rouge">nvidia-smi</code> confirms it’s there. If you’re running Linux or WSL and have installed TensorFlow in a Conda environment but are struggling to get it to use your GPU, this guide is for you. Follow these steps to ensure TensorFlow can utilize the CUDA and cuDNN libraries installed within your Conda environment, rather than relying on a global installation that might be outdated or incompatible with your version of tensorflow.</p>

<h2 id="prerequisites">Prerequisites</h2>

<p>Before starting, ensure you have the following:</p>
<ol>
  <li>A working installation of Conda.</li>
  <li>TensorFlow installed in a Conda environment.</li>
  <li>NVIDIA drivers installed and verified with <code class="language-plaintext highlighter-rouge">nvidia-smi</code> command.</li>
  <li>CUDA and cuDNN installed within your Conda environment.</li>
</ol>

<h2 id="step-1-verify-your-environment">Step 1: Verify Your Environment</h2>

<p>First, verify that you can run <code class="language-plaintext highlighter-rouge">nvidia-smi</code> and that it correctly shows your GPU:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvidia-smi
</code></pre></div></div>
<p>This command should display information about your NVIDIA GPU. If it doesn’t, you may need to install the NVIDIA drivers or check your hardware configuration.</p>

<p>Next, activate your Conda environment and check if TensorFlow is installed:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda activate &lt;your_environment&gt;
python <span class="nt">-c</span> <span class="s2">"import tensorflow as tf; print(tf.__version__)"</span>
</code></pre></div></div>
<p>if TensorFlow is installed, you should see the version number printed. If not, install TensorFlow with cuda in your Conda environment:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>tensorflow[and-cuda]
</code></pre></div></div>
<p>on papers this should be enough to have your tensorflow up and running with CUDA support, but in reality it often doesn’t work as expected. especially if you don’t have a system wide installation of CUDA and cuDNN or if it’s not compatible with the version of TensorFlow you’re using.</p>

<p>For the sake of sanity check let’s run a simple TensorFlow code to see if it’s already using the GPU:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-c</span> <span class="s2">"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"</span>
</code></pre></div></div>
<p>if you see something like <code class="language-plaintext highlighter-rouge">[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]</code> then you’re good to go, otherwise follow the next steps.</p>

<h2 id="step-2-identify-the-cuda-and-cudnn-paths">Step 2: Identify the CUDA and cuDNN Paths</h2>

<p>We want tensorflow to use the CUDA and cuDNN libraries installed within the Conda environment that we installed with the above <code class="language-plaintext highlighter-rouge">pip install tensorflow[and-cuda]</code>, rather than relying on a global installation that might be outdated.
 Make sure to activate your conda environment where tensorflow with cuda is installed In linux terminal run</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-c</span> <span class="s2">"import nvidia.cudnn; print(nvidia.cudnn.__file__)"</span>
</code></pre></div></div>
<p>if it prints out something like:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/home/username/miniconda3/envs/tf/lib/python3.11/site-packages/nvidia/cudnn/__init__.py
</code></pre></div></div>
<p>then that means we do have a cuDNN library installed within the conda environment but now we just need to set the path so that tensorflow can see it.</p>

<p>save the cudnn paths in a variable:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">CUDNN_PATH</span><span class="o">=</span><span class="si">$(</span><span class="nb">dirname</span> <span class="si">$(</span>python <span class="nt">-c</span> <span class="s2">"import nvidia.cudnn; print(nvidia.cudnn.__file__)"</span><span class="si">))</span>
<span class="nb">echo</span> <span class="nv">$CUDNN_PATH</span>
</code></pre></div></div>
<p>this will save the path to the cuDNN library in the <code class="language-plaintext highlighter-rouge">CUDNN_PATH</code> variable and print it out to the terminal.</p>

<h2 id="step-3-set-ld_library_path-environment-variable">Step 3: Set <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> Environment Variable:</h2>

<p>Now we need to set the <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> environment variable to include the paths to the CUDA and cuDNN libraries within the Conda environment. This will allow TensorFlow to find and use these libraries when running on the GPU.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="k">${</span><span class="nv">CUDNN_PATH</span><span class="k">}</span>/lib:<span class="nv">$LD_LIBRARY_PATH</span>
<span class="nb">echo</span> <span class="nv">$LD_LIBRARY_PATH</span>
</code></pre></div></div>
<p>This adds the cuDNN library path to the LD_LIBRARY_PATH.</p>

<h2 id="step-4-test-tensorflow-with-cuda">Step 4: Test TensorFlow with CUDA</h2>

<p>Now test again if TensorFlow can see the GPU and use CUDA and cuDNN libraries:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-c</span> <span class="s2">"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"</span>
</code></pre></div></div>
<p>if you see <code class="language-plaintext highlighter-rouge">[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]</code> then you’re good to go, otherwise you might need to check the paths again and make sure they’re correct.</p>

<h2 id="step-5-presist-the-changes">Step 5: Presist the changes</h2>

<p>To make sure the changes persist across terminal sessions, we want to update the <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> each time our conda environment is activated. To do this, we need to add some lines to the <code class="language-plaintext highlighter-rouge">activate</code> script of the conda environment:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nano <span class="nv">$CONDA_PREFIX</span>/etc/conda/activate.d/env_vars.sh
</code></pre></div></div>
<p>Add the following lines to the file:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/sh</span>
<span class="nb">export </span><span class="nv">CUDNN_PATH</span><span class="o">=</span><span class="si">$(</span><span class="nb">dirname</span> <span class="si">$(</span>python <span class="nt">-c</span> <span class="s2">"import nvidia.cudnn; print(nvidia.cudnn.__file__)"</span><span class="si">))</span>
<span class="nb">export </span><span class="nv">OLD_LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>
<span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="k">${</span><span class="nv">CUDNN_PATH</span><span class="k">}</span>/lib:<span class="nv">$LD_LIBRARY_PATH</span>
</code></pre></div></div>

<p>Save the file and exit the editor. Now, whenever you activate your conda environment, the <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> will be updated to include the paths to the CUDA and cuDNN libraries within the conda environment.</p>

<h2 id="create-deactivation-script">Create Deactivation Script</h2>

<p>To ensure that the <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> is reset when you deactivate the conda environment, create a <code class="language-plaintext highlighter-rouge">deactivate</code> script that unsets the <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> variable. To do this, run the following command:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nano <span class="nv">$CONDA_PREFIX</span>/etc/conda/deactivate.d/env_vars.sh
</code></pre></div></div>

<p>Add the following lines to the file:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/sh</span>
<span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$OLD_LD_LIBRARY_PATH</span>
<span class="nb">unset </span>OLD_LD_LIBRARY_PATH
<span class="nb">unset </span>CUDNN_PATH
</code></pre></div></div>
<p>That’s it! Now, whenever you deactivate your conda environment, the <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> will be reset to its original value.</p>

<p class="notice--info"><strong>Note:</strong> Tensorflow by default looks for cudnn in the environment variable <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code>.Although it seems like we are just running bunch of commands in a shell without understanding what they do. The main reason we are doing this is first to find cudnn installed within our conda environment that is accessible to python and then set the default path <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> to where the cudnn is installed. So that tensorflow can use it.And we can update the <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> each time our conda environment is activated by adding the commands to the <code class="language-plaintext highlighter-rouge">activate</code> script of the conda environment. This way we don’t have to run the commands each time we activate the environment.</p>]]></content><author><name>Snawar Hussain</name></author><category term="Blog" /><category term="Linux" /><category term="TensorFlow" /><category term="GPU" /><category term="CUDA" /><category term="cuDNN" /><category term="Conda" /><category term="Linux" /><category term="WSL2" /><category term="WSL" /><category term="Deep Learning" /><category term="AI" /><summary type="html"><![CDATA[Guide to set-up TensorFlow to use GPU in a Conda environment.Follow these steps to ensure TensorFlow leverages CUDA and cuDNN installed in your Conda environment.]]></summary></entry><entry><title type="html">Navigating K-space in MRI: The Role of Gradient Fields</title><link href="https://snawarhussain.com/educational/mri%20technology/Kspace-walk-using-encoding-gradients-in-MRI/" rel="alternate" type="text/html" title="Navigating K-space in MRI: The Role of Gradient Fields" /><published>2024-01-25T00:00:00+00:00</published><updated>2024-01-25T00:00:00+00:00</updated><id>https://snawarhussain.com/educational/mri%20technology/Kspace-walk-using-encoding-gradients-in-MRI</id><content type="html" xml:base="https://snawarhussain.com/educational/mri%20technology/Kspace-walk-using-encoding-gradients-in-MRI/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>In Magnetic Resonance Imaging (MRI), gradient fields, among many other things, are used to encode spatial information. This post aims to clarify the connection between gradient fields and spatial frequencies in the K-space, and how gradients walks us through K-space. Blending mathematical theory with practical code examples, the goal is to enhance basic understanding of MR Physics and Simulation.</p>

<h2 id="mathematical-relationship-between-k-space-and-gradient-fields">Mathematical relationship between K-space and Gradient fields</h2>

<p>Recall from MRI theory that the gradients are used to change the mangetic field strenght of the B0 field in a particular direction. 
For example a gradient in the x direction will change the magnetic field strength in the x direction.And similar is the case with gradients in the y or z direction.This change in magnetic field strength is represented by the following equation:</p>

\[B(x,y,z) = (B_0 + G_x.x + G_z.y+ G_z.z)_z\]

<p>It is important to note that the overall magnetic field  is still pointing in the z direction. The gradients only change the strenght of magnetic field in space and not the direction. That is why the right side of the above equation is subscripted with z.
visually we can represent this change in magnetic field strength as shown in the following interactive plot:
by changing slider values we can change the magnetic field strength in the x, y and z directions. the overall magnetic field is still pointing in the z direction. The gradients only change the strenght of magnetic field shown in colorcoded arrows all pointing in the z direction.</p>

<div class="container">
<!-- add heigh and width to the div cube -->
    <div id="grid"></div>
    <input type="range" id="gx-slider" min="-100" max="100" value="0" />
    <input type="range" id="gy-slider" min="-100" max="100" value="0" />
    <input type="range" id="gz-slider" min="-100" max="100" value="0" />
</div>

<h2 id="rotating-vs-labratory-frame-of-reference">Rotating vs Labratory Frame of reference</h2>
<p>Consider a simple scenario where follwing an RF field and a slice selection gradient, we have a 2D slice of the spin magnetic moments knocked into the transverse plane all rotating at larmor frequency in phase.</p>

<!-- <div class="frames-container">
    <div class="frame" id="lab-frame">
        <h2>Labratory Frame</h2>
        <div class="arrow_grid" id="lab-cube"></div>
    </div>
    <div class="frame" id="rotational-frame">
        <h2>Rotational Frame</h2>
        <div class="arrow_grid" id="rotational-cube"></div>
    </div>
</div> -->

<p align="center">
<img src="/assets/images/ks_grad/lab_rotat.gif" height="300" />
 <figcaption>Fig 1: Labratory vs Rotational Frame of reference</figcaption>
</p>

<p>In the laboratory frame of reference, the spin magnetic moments are precessing at the Larmor frequency and are initially in phase. However, when we shift our perspective to the rotating frame of reference, these spin magnetic moments appear stationary. This stationary appearance results from the rotating frame moving synchronously with the spins’ Larmor precession. This concept is at the core of what we call the rotating frame of reference.</p>

<p>Opting for the rotating frame of reference in our subsequent discussions offers a significant advantage. It simplifies the visualization of gradient effects on spin magnetic moments, allowing us to observe these effects without the complexity introduced by their rapid precession in the laboratory frame. Essentially, it provides a clearer and more stable viewpoint, making it easier to comprehend the changes induced by the gradients without the ‘dizzying’ effects of spin motion seen in the lab frame.</p>

<h2 id="gradient-fields-and-spin-magnetic-moments">Gradient Fields and Spin Magnetic Moments</h2>

<p>When a gradient field is introduced in any direction, it alters the magnetic field strength along that direction. This variation in magnetic field strength causes the spin magnetic moments to precess at different frequencies in that specific direction. The reason for this is that the precession frequency of spin magnetic moments is directly proportional to the magnetic field strength, as described by the equation:</p>

\[\omega = \gamma B\]

<p>Here, \(\omega\) represents the precession frequency, \(\gamma\) is the gyromagnetic ratio, and \(B\) signifies the magnetic field strength.</p>

<p>In a rotating frame of reference, this differential in precession frequencies translates to spin dephasing along the spatial gradient. This dephasing occurs because spins are not precessing synchronously with the rotating frame.</p>

<p>For instance, a Gx gradient introduces spin magnetic moment dephasing along the x-axis. Spins located on one side of the x-axis origin dephase in the opposite direction to those on the other side, while spins precisely at the x-axis origin do not dephase. Analogous effects occur with Gy and Gz gradients, affecting spin dephasing along the y-axis and z-axis, respectively.</p>

<h2 id="2d-fourier-transform-and-mri">2D Fourier Transform and MRI</h2>

<p>In our previous discussion on <a href="/educational/mri%20technology/data%20analysis/2D-Fourier-Transform-K-space-and-MRI/">2D Fourier Transform</a> we simplified the concept of image decomposition using the principles of the 2D Fourier Transform. To avoid the complexities of mathematical rigor, let’s recall the key idea: any two-dimensional (2D) spatial signal, such as an image, can be decomposed into components resembling 2D sinusoids. These sinusoidal patterns represent variations in both spatial frequency and phase orientation. By gathering enough amount of these 2D sinusoids, varying in their spatial frequencies and orientations, we can reconstruct the original image. This process forms the crux of the inverse 2D Fourier Transform.</p>

<p>Each of these 2D sinusoids, characterized by specific oscillations along the x and y axes, correlates to a unique point in the 2D Fourier Transform plane, denoted as \(F(k_x, k_y)\). This plane is, in fact, what is known in MRI terminology as the K-space. K-space is a conceptual framework used for understanding and processing the data acquired by MRI scanners to produce images. It represents the spatial frequencies of the object being imaged, with each point in K-space contributing to the overall image’s formation.</p>

<h2 id="stepping-through-k-space-using-gradients">Stepping Through K-space using Gradients</h2>

<p>The role of gradients in MRI is pivotal for navigating through K-space, and to demonstrate this, I have created an interactive graph. Imagine a 2D slice with spin magnetic moments initially aligned in the transverse plane and rotating at the Larmor frequency, in phase within the rotating frame of reference.</p>

<p>In our interactive plot, you’ll find horizontal and vertical sliders representing the application of gradient fields. As you adjust these sliders, you are effectively changing the frequency and phase of the spin magnetic moments. This adjustment simulates the effects of frequency and phase encoding in MRI.</p>

<p>The horizontal slider corresponds to the frequency encoding gradient. Moving this slider alters the precession frequency of the spins along one axis, thus varying their position along the frequency-encoded direction in K-space.</p>

<p>The vertical slider, on the other hand, represents the phase encoding gradient. Adjusting this slider changes the phase of the spins, thereby moving them along the phase-encoded direction in K-space.</p>

<p>As you interact with these sliders, observe the changes in a 2D grid of arrows on the plot. These arrows symbolize the spin magnetic moments and their orientation in response to the applied gradients. Were you able to see it ??</p>

<div class="threejs">
<!-- add heigh and width to the div cube -->
    <div id="cube"></div>
    <input type="range" id="horizontal-slider" min="-360" max="360" value="0" />
    <input type="range" id="vertical-slider" min="-360" max="360" value="0" />

</div>

<p>By turing on the gradients in the x and y direction we are varying the frequency and phase of the spins in the x and y direction based on
their location in space and in-turn creating our own 2D sinosoids corresponding to a certain point in the 2D K-space.</p>

<p>This is exactly what we do in MRI. We collect 2D sinosoids (of spin magnetic moments of different tissues) varying in spatial frequency and phase and once we have collected enough of them we can reconstruct the original image back using the inverse Fourier Transform.</p>

<p>Mathematically, the relationship between the gradient fields and their representation in K-space can be expressed as follows:</p>

\[k(x,y,z) = \gamma \int G(x,y,z) dt\]

<p>In this equation, \(k(x,y,z)\) represents the position in K-space , \(\gamma\) is the gyromagnetic ratio, \(G(x,y,z)\) is the gradient applied, and \(dt\) denotes the integration over time. This formulation illustrates that the position in K-space is directly proportional to the time integral of the gradient field applied in a arbitrary direction.</p>

<h3 id="the-influence-of-k_x-and-k_y-on-sinusoidal-patterns">The Influence of \(k_x\) and \(k_y\) on Sinusoidal Patterns</h3>

<p>Recall that the number of wiggles or oscillations in 2D sinosoid in x and y direction correlates directly to a specific point in the \(F(k_x, k_y)\) plane aka K-space. For instance, a sinusoid with 3 complete cycles  in the x-direction and 3 in the y-direction would correspond to a point in \(F(k_x, k_y)\) with \(k_x =3\) value and \(k_y =3\) value. This mapping is fundamental to how spatial frequencies are represented and manipulated in the Fourier Transform, particularly in applications like MRI, where precise spatial information is crucial.</p>

<p>Here’s a part of animation that demonstrates the relation between point in K-Space and the 2D sinosoid it generates and the effect of phasor assosiated with each of the 2D sinosoid (spatical frequency) and how it scales and shifts it.</p>

<p align="center">
<img src="/assets/images/2D_FT/FT_2D.gif" height="600" />
 <figcaption>Fig 2: 2D Sinosoid with changing phase  </figcaption>
</p>

<h2 id="conclusion">Conclusion</h2>

<p>In summary, the intricacies of MRI technology, particularly the role of gradient fields in navigating through K-space, underscore the sophistication of this imaging modality. Through the careful manipulation of gradients, MRI is capable of encoding spatial information into the spin magnetic moments, which are then mapped onto K-space. The fundamental equation \(k = \gamma \int G dt\) elegantly captures this relationship, demonstrating how the position in K-space is determined by the integrated effect of the applied gradients over time.</p>

<p>This deep interplay between physical principles and technological implementation not only facilitates the creation of detailed anatomical images but also opens avenues for advanced imaging techniques. The interactive tools and visualizations we discussed serve as potent means for understanding these complex concepts, making the abstract principles of MRI more tangible and comprehensible.</p>

<p>As we continue to refine and advance MRI technology, the potential for improved diagnostics and research expands. The journey through K-space, facilitated by gradient fields, is not just a cornerstone of MRI but a testament to the ingenuity and continual evolution of medical imaging technology.</p>

<!-- Include necessary scritps -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r121/three.min.js"></script>

<!-- Include OrbitControls.js from jsDelivr -->
<script src="https://cdn.jsdelivr.net/npm/three@0.121.0/examples/js/controls/OrbitControls.js"></script>

<script src="/assets/js/arrowgrid.js"></script>

<link rel="stylesheet" href="/assets/css/arrowgrid.css" />

<script src="https://cdn.jsdelivr.net/npm/chroma-js@2.1.0/chroma.min.js"></script>

<script src="/assets/js/threejs.js"></script>

<link rel="stylesheet" href="/assets/css/threejs.css" />]]></content><author><name>Snawar Hussain</name></author><category term="Educational" /><category term="MRI Technology" /><category term="MRI" /><category term="K-space" /><category term="Gradient Fields" /><category term="Fourier Transform" /><category term="Spin Magnetic Moments" /><category term="Image Reconstruction" /><category term="Medical Imaging" /><summary type="html"><![CDATA[Exploring the crucial role of gradient fields in MRI for stepping through K-space.]]></summary></entry><entry><title type="html">2D Fourier Transform and Complex Numbers in MR Physics</title><link href="https://snawarhussain.com/educational/mri%20technology/data%20analysis/2D-Fourier-Transform-K-space-and-MRI/" rel="alternate" type="text/html" title="2D Fourier Transform and Complex Numbers in MR Physics" /><published>2023-11-24T00:00:00+00:00</published><updated>2023-11-24T00:00:00+00:00</updated><id>https://snawarhussain.com/educational/mri%20technology/data%20analysis/2D-Fourier-Transform-K-space-and-MRI</id><content type="html" xml:base="https://snawarhussain.com/educational/mri%20technology/data%20analysis/2D-Fourier-Transform-K-space-and-MRI/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Magnetic Resonance Imaging (MRI) relies heavily on understanding the shift from 1D Fourier transforms, which deal with time-based signals, to 2D Fourier transforms, crucial for spatial imaging. This post aims to clarify the connection between 1D and 2D Fourier Transform  and later bulding upon these concepts we will explore concepts like K-spaceand MRI. Blending mathematical theory with practical code examples the goal is to enhance basic understanding of MR Physics and Simulation.</p>

<h2 id="the-core-principle-of-fourier-transform">The Core Principle of Fourier Transform</h2>

<p>The Fourier Transform operates on a simple yet profound idea: any signal, whether temporal or spatial, can be broken down into an infinite series of sinusoids. In essence, we can construct any signal using different frequencies of sinusoids. This concept is more than theoretical—it’s fundamental to applying Fourier Transforms.</p>

<p>As part of this exploration, we’ll reference a PIRL video that clearly demonstrates the relationship between K-space, Fourier Transform, and MRI. This post seeks to bring the video’s concepts to life through coding.</p>

<p>To better grasp Fourier Transform’s complexities, it’s beneficial to start with the basics of <a href="/blog/computational%20modeling/mr%20physics/Complex-Numbers-and-Rotations/">complex numbers and their role in rotations</a>, as well as the <a href="/mri%20analysis/signal%20processing/mathematical%20modeling/1D-Fourier-Transform-visual-guide/">simpler 1D Fourier Transform</a>. These foundational topics, explored in the provided links, are key to understanding Fourier Transform’s role in MRI.</p>

<h2 id="2d-fourier-transform--and-mri">2D Fourier Transform  and MRI</h2>
<p>In the realm of image analysis and MRI, the 2D Fourier Transform is a pivotal tool. Unlike 1D signals, which are functions of time, images are functions of space. This transition to spatial domain brings forth the concept of 2D Fourier Transform, which is crucial for MRI.</p>

<h2 id="the-2d-fourier-transform-in-imaging">The 2D Fourier Transform in Imaging</h2>

<p>The 2D Fourier Transform extends the principles of the 1D transform to two dimensions:</p>

\[F(k_x, k_y) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(x, y) e^{-2\pi i(k_xx + k_yy)} \, dx \, dy\]

<p>Here, \(f(x, y)\) is the spatial signal (image), while \(k_x\) and \(k_y\) represent spatial frequencies in the x and y directions, respectively. This transform decomposes an image into its frequency components, revealing how different spatial frequencies and orientations contribute to the overall image.</p>

<p>now \(F(k_x, k_y)\), unlike \(F(\omega)\) in 1D fourier transform,  is a 2D plane where these spatial frequencies are organized thus, each point in this plane maps to a specific spatial frequency within the object. This concept parallels the 1D Fourier transform, but extends it to a 2D framework. Here, we deal with 2D spatial coordinates (x, y) and corresponding 2D spatial frequency coordinates \((k_x, k_y )\) and each point in this plane, just like in \(F(\omega)\), has its own complex plane containing withinin a phasor that scales and shifts this 2D sinosoid. This spatial frequency information is critical in reconstructing the final MRI image.</p>

<p>upon adding all these scale and shifted 2D sinosoids we get the final 2D image just like we did in 1D fourier transform.</p>

<h2 id="ok-but-how-exactly-does-a-2d-sinosoid-look">OK but how exactly does a 2D sinosoid look?</h2>

<p>In \(F(k_x, k_y)\) plane stepping through the \(k_x\) and \(k_y\) coordinates creates 2D sinosoids  with varying ‘wiggles’ in x and y directions.</p>

<p>Understanding the appearance and behavior of 2D sinusoids in the context of the Fourier Transform requires a closer look at the \(F(k_x, k_y)\) plane. In this plane, navigating through various \(k_x\) and \(k_y\) coordinates generates a series of 2D sinusoids, each characterized by distinct patterns or ‘wiggles’ in both the x and y directions.</p>

<h3 id="the-influence-of-k_x-and-k_y-on-sinusoidal-patterns">The Influence of \(k_x\) and \(k_y\) on Sinusoidal Patterns</h3>

<p>Imagine a coded animation, similar to the one in the 1D Fourier transform blog, to better visualize this concept. As you increment the value of \(k_x\), you’ll notice an increase in the frequency of wiggles along the x-direction. Similarly, increasing \(k_y\) boosts the frequency of wiggles in the y-direction. This relationship is key to understanding how 2D sinusoids are formed and manipulated in Fourier space.</p>

<p>Each 2D sinusoid is  shaped by a corresponding phasor. This phasor adjusts both the amplitude and phase of the sinusoid. The resultant effect is a versatile range of sinusoidal waves, each uniquely contributing to the overall image reconstruction process in MRI.</p>

<p>The number of wiggles or oscillations in each direction correlates directly to a specific point in the \(F(k_x, k_y)\) plane. For instance, a sinusoid with 3 complete cycles  in the x-direction and 3 in the y-direction would correspond to a point in \(F(k_x, k_y)\) with \(k_x =3\) value and \(k_y =3\) value. This mapping is fundamental to how spatial frequencies are represented and manipulated in the Fourier Transform, particularly in applications like MRI, where precise spatial information is crucial.</p>

<p align="center">
<img src="/assets/images/2D_FT/2D_sin.gif" width="300" />
 <figcaption>Fig 1: 2D Sinosoid with changing phase  </figcaption>
</p>

<p>Here’s a part of animation that demonstrates the relation between point in \(F(k_x, k_y)\) plane and the 2D sinosoid it generates and the effect of phasor assosiated with each of the 2D sinosoid (spatical frequency) and how it scales and shifts the it.</p>

<p align="center">
<img src="/assets/images/2D_FT/FT_2D.gif" height="600" />
 <figcaption>Fig 2: 2D Sinosoid with changing phase  </figcaption>
</p>

<p>You can play around with the code for this animation on my github repo <a href="https://github.com/snawarhussain/2D_FT_Anim">here</a>.</p>

<p>As a proof of concept, let’s consider a Mario sprite image. We start by taking its Fourier transform, represented as:</p>

\[F(k_x, k_y) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(x, y) e^{-2\pi i(k_x x + k_y y)} \, dx \, dy\]

<p>Here, \(f(x, y)\) is our original Mario sprite image, and \(F(k_x, k_y)\) is its Fourier transform, with \(k_x\) and \(k_y\) representing spatial frequencies in the x and y directions, respectively.</p>

<p>To reconstruct the original image from its Fourier transform, we take each point in the \(F(k_x, k_y)\) plane, which is a complex number (phasor). The reconstruction process involves the following steps:</p>

<ol>
  <li>
    <p><strong>Constructing 2D Sinusoids:</strong> For each point \((k_x, k_y)\) in the Fourier transform, we create a corresponding 2D sinusoid in its complex exponential form:</p>

\[S(x, y; k_x, k_y) = e^{2\pi i(k_x x + k_y y)}\]
  </li>
  <li>
    <p><strong>Scaling with the Phasor:</strong> The phasor at each point in the \(F(k_x, k_y)\) plane, characterized by an amplitude \(A\) and phase \(\phi\), scales the corresponding 2D sinusoid. This scaling is represented as:</p>

\[S_{scaled}(x, y; k_x, k_y) = A \cdot e^{2\pi i(k_x x + k_y y) + \phi}\]
  </li>
  <li>
    <p><strong>Summing Spatial Frequencies:</strong> The final step involves summing these scaled and shifted sinusoids across all spatial frequencies to reconstruct the original image:</p>

\[f_{reconstructed}(x, y) = \sum_{k_x, k_y} S_{scaled}(x, y; k_x, k_y)\]
  </li>
</ol>

<p>This method demonstrates the practical application of Fourier Transform in image processing, showcasing how an image can be decomposed and then reconstructed using the principles of spatial frequencies and phase shifts.</p>

<p align="center">
<img src="/assets/images/2D_FT/mario_recons.gif" height="600" />
 <figcaption>Fig 3: 1: oringal image, 2: 2D FT of the image, 3: Indexing phasors, 4: 2D sine wave associated with each phasor. Addtion of the 2D Sinosoids  </figcaption>
</p>

<p>In Fig.3 as we keep adding the 2D sinosoids we get the final image. This is the essence of the Fourier Transform: decomposing a signal into its 2D sine patterns and then reconstructing it from these components.</p>]]></content><author><name>Snawar Hussain</name></author><category term="Educational" /><category term="MRI Technology" /><category term="Data Analysis" /><category term="MRI" /><category term="Fourier Transform" /><category term="Complex Numbers" /><category term="Image Processing" /><category term="Spatial Frequencies" /><category term="Coding in MRI" /><summary type="html"><![CDATA[Dive deep into the role of complex numbers and Fourier Transform in Magnetic Resonance Imaging (MRI), featuring practical coding examples and a detailed analysis of MR physics and simulation.]]></summary></entry><entry><title type="html">1D Fourier Transform: A Visual Guide for Decoding Signals with Complex Numbers</title><link href="https://snawarhussain.com/mri%20analysis/signal%20processing/mathematical%20modeling/1D-Fourier-Transform-visual-guide/" rel="alternate" type="text/html" title="1D Fourier Transform: A Visual Guide for Decoding Signals with Complex Numbers" /><published>2023-11-23T00:00:00+00:00</published><updated>2023-11-23T00:00:00+00:00</updated><id>https://snawarhussain.com/mri%20analysis/signal%20processing/mathematical%20modeling/1D-Fourier-Transform-visual-guide</id><content type="html" xml:base="https://snawarhussain.com/mri%20analysis/signal%20processing/mathematical%20modeling/1D-Fourier-Transform-visual-guide/"><![CDATA[<p>If you haven’t read my previous blog on the relation between complex numbers and rotations, I would highly recommend you to check it out <a href="/blog/computational%20modeling/mr%20physics/Complex-Numbers-and-Rotations/">here</a>. It will help you understand the concepts in this blog better.</p>

<h2 id="introduction">Introduction</h2>

<p>The Fourier Transform is a mathematical tool that transforms a signal from the time domain to the frequency domain, unveiling the different frequencies that constitute the signal. In its most general form, the Fourier Transform is given by:</p>

\[F(\omega) = \int_{-\infty}^{\infty} f(t) e^{-i \omega t} \, dt\]

<p>Here, \(F(\omega)\) is the Fourier Transform of \(f(t)\), with \(\omega\) representing frequency and \(t\) denoting time. The exponential term \(e^{-2\pi i \omega t}\) is a rotating complex sinusoid, which is crucial for extracting the frequency components from \(f(t)\).</p>

<h3 id="the-discrete-fourier-transform-dft">The Discrete Fourier Transform (DFT)</h3>

<p>In digital applications, we use the Discrete Fourier Transform (DFT) to handle sampled data:</p>

\[F(k) = \sum_{n=0}^{N-1} f(n) e^{ i \frac{kn}{N}}\]

<p>Here, \(f(n)\) is the sampled signal, \(N\) is the number of samples, and \(k\) is the frequency index in the DFT.</p>

<p>The Discrete Fourier Transform (DFT) is a transformative tool in signal processing, translating signals from the time domain into the frequency domain. While the equations of the Fourier Transform and DFT are familiar territory for many, the real magic lies in understanding what happens inside the DFT and how it reveals the intricate composition of any signal.</p>

<p>Fundamental principle behind fourier transform is that any signal that is changing (temporally or spatially) can be represented as an infinite sum of sinosoids. or in other words we can create any signal by adding different frequencies of sinosoids. This is the basis of fourier transform.</p>

<p>The easiest examples would be temporally varrying 1D signals</p>

<h2 id="inside-dft">Inside DFT</h2>
<p>The output of the DFT is a collection of complex numbers, each representing a unique sinusoid in the original signal. These complex numbers, or phasors, hold the key to both the amplitude and phase of these complex sinusoids, essentially encoding how each sinusoid is scaled and shifted.</p>

<h2 id="complex-phasors">Complex Phasors</h2>
<p>A phasor \(F(\omega = \omega_0)\) in the DFT output is more than just a number; it’s a vector in the complex plane.And in fourier transform plot we mostly show the magnitude of this phasor. For example: Figure 1 illustrates the Fourier Transform and Inverse Fourier Transform of a simple signal. In \(F(\omega)\) plot, the x-axis is the frequency index \(\omega =\omega_0\), while the y-axis is the magnitude of the phasor \(|F(\omega_0)|\).</p>

<p>The magnitude indicates the contribution of each sinusoid to the overall signal.</p>
<p align="center">
<img src="/assets/images/1D_FT/ft_ift.png" width="600" />
 <figcaption>Fig.1: Fourier and Inverse Fourier fransform</figcaption>
</p>

<p>Thus a phasor \(F(\omega_0)\):
\(F(\omega = \omega_0) = x + iy\)
with amplitude \(A\):
\(A =  \sqrt{x^2 + y^2}\)
and phase  \(\phi\):
\(\phi = \tan^{-1}(\frac{y}{x})\)</p>

<p>scales the base complex sinosoid \(e^{- i \omega_0 t}\) by a factor of \(A\) and shifts it by a phase of \(\phi\)
\(A e^{- i \omega_0 t + \phi 
}\)</p>

<p>Each of these sinusoids is a fundamental building block of the orignal signal, characterized by a specific frequency. The DFT decomposes the signal into these basic elements, revealing how each frequency contributes to the overall structure of the signal</p>

<p>Since each point alone \((\omega)\) in the Figure 1 is a complex number, thus every frequeny \(\omega\) in the \(F (\omega)\) plot has it’s own Argand plane with real and imaginary axis.
So we can extend our orignal  \(|F(\omega_0)|\)plot from Figure 1 to show that every frequency \(\omega_0\) contains within it a phasor \(F (\omega_0)\) that scales and shifts the sinosoid at \(\omega_0\) according to it’s contribution to the signal.</p>

<p align="center">
<img src="/assets/images/1D_FT/phasor.gif" width="600" />
 <figcaption>Fig 2: phasor associated with a particular frequency sinosoid and how it scales and shifts it. </figcaption>
</p>

<h2 id="sum-of-sinusoids">Sum of Sinusoids</h2>

<p>Ultimately, any  signal \(f(t)\) can be represented by a specific set of scaled and shifted sinusoids. And Summing over these sinosoids, governed by the principles encoded in the phasors, gives us back the original signal in its time-domain form. This is the essence of the DFT: decomposing a signal into its sinusoidal components and then reconstructing it from these components.</p>

<p align="center">
<img src="/assets/images/1D_FT/signal_ft.png" width="600" />
 <figcaption>Fig 3: Singal f(t) represented as phasors of distinct frequencies (Fourier Transform). </figcaption>
</p>

<h2 id="exploring-the-dft-with-coded-animation">Exploring the DFT with coded Animation</h2>

<p>Through animation, we can dynamically illustrate the transformative process of the DFT. The goal is to visually demonstrate how:</p>

<p>Each point in the \(F(\omega)\) plot corresponds to a certain frequency and an associated phasor.
These phasors define the amplitude and phase of sinusoids at these specific frequencies.
By summing these scaled and shifted sinusoids, we can reconstruct the original signal.</p>

<!-- put mp4 video that loops here  -->
<p align="center">
<img src="/assets/images/1D_FT/FT_1D_1.gif" width="600" />
 <figcaption>Fig 4: Python simulation for the phenomenon in Fig. 2  </figcaption>
</p>

<p>For further exploration and hands on experience I have provided the code for this animation on my github repo <a href="https://github.com/snawarhussain/1D_FT_Anim">here</a>.</p>

<p>This blogpost is a prereq to MRI simulation. These fundamental concepts will help us venture into the realm of 2D Fourier Transforms, K-Spaces and their pivotal role in applications like MRI, opening doors to a world where these concepts find profound applications.</p>

<h2 id="reference">Reference</h2>

<ol>
  <li><a href="https://youtu.be/R_4GuyJTzMo?t=350">The Fourier Theory in MRI </a></li>
</ol>]]></content><author><name>Snawar Hussain</name></author><category term="MRI Analysis" /><category term="Signal Processing" /><category term="Mathematical Modeling" /><category term="Fourier Transform" /><category term="MRI" /><category term="Complex Numbers" /><category term="Signal Decomposition" /><category term="Discrete Fourier Transform" /><category term="Phasors" /><category term="Coding in MRI" /><summary type="html"><![CDATA[An in-depth exploration of Fourier Transform and complex numbers in MRI. Understand the critical role these concepts play in signal processing and MR imaging through detailed examples and code.]]></summary></entry><entry><title type="html">Complex Numbers and Rotations: A Primer to Fourier Transform and MR Physics and Simulation</title><link href="https://snawarhussain.com/blog/computational%20modeling/mr%20physics/Complex-Numbers-and-Rotations/" rel="alternate" type="text/html" title="Complex Numbers and Rotations: A Primer to Fourier Transform and MR Physics and Simulation" /><published>2023-10-13T00:00:00+00:00</published><updated>2023-10-13T00:00:00+00:00</updated><id>https://snawarhussain.com/blog/computational%20modeling/mr%20physics/Complex-Numbers-and-Rotations</id><content type="html" xml:base="https://snawarhussain.com/blog/computational%20modeling/mr%20physics/Complex-Numbers-and-Rotations/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Complex numbers often appear mysterious to those who encounter them for the first time. Why would anyone need numbers that are “imaginary”? As it turns out, complex numbers are not only elegant but incredibly useful. They lie at the heart of many scientific and engineering disciplines, including signal processing, control theory, electromagnetism, and, of course, Fourier Transform and Magnetic Resonance (MR) physics.</p>

<h2 id="complex-numbers-and-rotations">Complex Numbers and Rotations</h2>

<p>Complex numbers and rotations are fundamental to the understanding of  the encoding of spatial information. In this Blogpost, we’ll go beyond mere equations and actively code to visualize these concepts. Through this hands-on approach, we’ll deepen our understanding of the core principles involved in not just MR Physics but many other physical concepts that invovle waves, frequencies, phase and rotations.
For a more intuivie understanding of complex numbers, I highly recommend <a href="https://www.youtube.com/watch?v=5PcpBw5Hbwo">3Blue1Brown’s video on the topic</a>. Here we will write code to visualize the concepts discussed in the video.</p>

<h2 id="what-are-complex-numbers">What are Complex Numbers?</h2>

<p>A complex number \(z\) is defined as \(z = a + bi\), where \(a\) and \(b\) are real numbers, and \(i\) is the imaginary unit with the property \(i^2 = -1\).</p>

\[z = a + bi\]

<h2 id="plotting-complex-numbers">Plotting Complex Numbers</h2>

<h3 id="the-complex-plane">The Complex Plane</h3>

<p>The real part is plotted along the x-axis, and the imaginary part along the y-axis. This 2D plane is known as the complex plane.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">markers</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.animation</span> <span class="kn">import</span> <span class="n">FuncAnimation</span>

<span class="k">def</span> <span class="nf">plot_arrow</span><span class="p">(</span><span class="n">complex_num</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)):</span>
        
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">xlim</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ylim</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s">'equal'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s">'both'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s">'both'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">xlim</span><span class="p">),</span><span class="mi">0</span><span class="p">,</span><span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">xlim</span><span class="p">)])</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">yticks</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">ylim</span><span class="p">),</span><span class="mi">0</span><span class="p">,</span><span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">ylim</span><span class="p">)])</span>
    <span class="c1"># Define a colormap (cmap) and normalize it
</span>    <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s">'viridis'</span><span class="p">)</span>
    <span class="n">norm</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">Normalize</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">complex_num</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">complex_num</span><span class="p">)):</span>
        <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">complex_num</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">real</span><span class="p">,</span> <span class="n">complex_num</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">imag</span>
        <span class="n">color</span> <span class="o">=</span> <span class="n">cmap</span><span class="p">(</span><span class="n">norm</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>

        <span class="n">ax</span><span class="p">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="c1"># Add a legend
</span>
    <span class="n">legends</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s">'z = </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">'</span> <span class="k">for</span> <span class="n">z</span> <span class="ow">in</span> <span class="n">complex_num</span><span class="p">]</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">legends</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s">'upper left'</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
    
    
    <span class="k">return</span> <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span>
    

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">complex_num</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="o">+</span><span class="mf">2j</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plot_arrow</span><span class="p">(</span><span class="n">complex_num</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">),</span><span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/euler_post/eluer_anim_1_0.png" alt="png" /></p>

<h3 id="scaling-complex-numbers">Scaling Complex Numbers</h3>

<p>We often need to scale complex numbers to unit magnitude. The magnitude (\(\lVert z \rVert\)) of a complex number \(z = a + bi\) is given by:</p>

\[\lVert z \rVert = \sqrt{a^2 + b^2}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Scaled complex numbers
</span><span class="n">mag</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">complex_num</span><span class="p">)</span>
<span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">angle</span><span class="p">(</span><span class="n">complex_num</span><span class="p">)</span>
<span class="n">scaled_complex_num</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">mag</span> <span class="o">*</span> <span class="n">complex_num</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plot_arrow</span><span class="p">(</span><span class="n">scaled_complex_num</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/images/euler_post/eluer_anim_2_0.png" alt="png" /></p>

<h3 id="eulers-formula-and-exponential-form-of-complex-numbers">Euler’s Formula and Exponential Form of Complex Numbers</h3>

<p>One of the most beautiful equations in mathematics is Euler’s formula:</p>

\[e^{ix} = \cos(x) + i \sin(x)\]

<p>This formula allows us to represent complex numbers in exponential form, providing a new way to understand rotation and scaling in the complex plane.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#alternative representation
</span><span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">angle</span><span class="p">(</span><span class="n">scaled_complex_num</span><span class="p">)</span>
<span class="n">exponential_form</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">1j</span> <span class="o">*</span> <span class="n">angle</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">real</span><span class="p">(</span><span class="n">exponential_form</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">imag</span><span class="p">(</span><span class="n">exponential_form</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plot_arrow</span><span class="p">(</span><span class="n">exponential_form</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/images/euler_post/eluer_anim_3_0.png" alt="png" /></p>

<h3 id="what-if-we-want-to-describe-rotations">what if we want to describe rotations?</h3>

<p>Euler’s formula is a powerful tool for describing rotations in the complex plane. Let’s say we want to rotate a complex number \(z\) by \(\theta\) radians. We can do this by multiplying \(z\) by \(e^{i\theta}\).
here we keep the \(z\) to be 1 and vary the theta from 0 to 2pi</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># what if we want to describe rotations? 
</span><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">8</span><span class="p">)</span>
<span class="n">complex_nums</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">1j</span> <span class="o">*</span> <span class="n">theta</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plot_arrow</span><span class="p">(</span><span class="n">complex_nums</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/euler_post/eluer_anim_4_0.png" alt="png" /></p>

<h3 id="ploting-the-real-and-imaginary-part-of-the-complex-number">Ploting the real and imaginary part of the complex number</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># plotting only the real part of the complex number
</span><span class="n">theta</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span>  <span class="c1"># 1 rotation
</span><span class="n">dt</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="o">+</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">complex_num</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">1j</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">*</span> <span class="n">dt</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">real</span><span class="p">(</span><span class="n">complex_num</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">'real'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">imag</span><span class="p">(</span><span class="n">complex_num</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">'imag'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/assets/images/euler_post/eluer_anim_5_0.png" alt="png" /></p>

<p>so basically \(e^{ix}=cos(x)+ isin(x)\)</p>

<h3 id="animation-showing-how-exponential-form-of-complex-number-can-be-used-to-describe-rotations-in-2d-complex-plane">Animation showing how exponential form of complex number can be used to describe rotations in 2D complex plane</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.animation</span> <span class="kn">import</span> <span class="n">FuncAnimation</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="k">class</span> <span class="nc">animate</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">complex_nums</span><span class="p">,</span> <span class="n">dt</span><span class="p">):</span>
        <span class="c1"># Figure setup
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dt</span> <span class="o">=</span> <span class="n">dt</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">complex_num</span> <span class="o">=</span> <span class="n">complex_nums</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fig</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ax</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ax</span><span class="p">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s">'equal'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s">'both'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s">'both'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">yticks</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

        <span class="c1"># Define a colormap (cmap) and normalize it
</span>        <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s">'viridis'</span><span class="p">)</span>
        <span class="n">color</span> <span class="o">=</span> <span class="n">cmap</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">arrow</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ax</span><span class="p">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">legend_text</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">([</span><span class="sa">f</span><span class="s">'z = </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">complex_num</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">'</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s">'upper left'</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
        <span class="c1"># Create the legend in the init function
</span>
    <span class="k">def</span> <span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">arrow</span><span class="p">.</span><span class="n">set_xy</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>  <span class="c1"># Initialize arrow at (0, 0)
</span>        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">arrow</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">legend_text</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">i</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">arrow</span><span class="p">.</span><span class="n">set_xy</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">complex_num</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">real</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">complex_num</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">imag</span><span class="p">]])</span>  <span class="c1"># Set arrow's vertices
</span>        <span class="c1"># Update the legend text
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">legend_text</span><span class="p">.</span><span class="n">get_texts</span><span class="p">()[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_text</span><span class="p">(</span><span class="sa">f</span><span class="s">'z = </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">complex_num</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">arrow</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">legend_text</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">anim</span> <span class="o">=</span> <span class="n">FuncAnimation</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fig</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">update</span><span class="p">,</span> <span class="n">init_func</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">init</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dt</span><span class="p">),</span> <span class="n">interval</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">blit</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">anim</span>

<span class="n">theta</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span>  <span class="c1"># 1 rotation
</span><span class="n">dt</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span> <span class="o">+</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">complex_num</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">1j</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">*</span> <span class="n">dt</span><span class="p">)</span>
<span class="n">anim</span> <span class="o">=</span> <span class="n">animate</span><span class="p">(</span><span class="n">complex_num</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>
<span class="n">an</span> <span class="o">=</span> <span class="n">anim</span><span class="p">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/euler_post/anim1.gif" alt="gif" /></p>

<h3 id="changing-the-direction-of-rotation">Changing the direction of rotation</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># including a minus sign in the exponential form will result in a rotation in the opposite direction
</span><span class="n">theta</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span>  <span class="c1"># 1 rotation
</span><span class="n">dt</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span> <span class="o">+</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">complex_num</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">1j</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">*</span> <span class="n">dt</span><span class="p">)</span>
<span class="n">anim</span> <span class="o">=</span> <span class="n">animate</span><span class="p">(</span><span class="n">complex_num</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>
<span class="n">an</span> <span class="o">=</span> <span class="n">anim</span><span class="p">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/euler_post/anim2.gif" alt="gif" /></p>

<h3 id="inlcude-frequency-component-to-the-complex-number-to-control-the-speed-of-rotation">Inlcude frequency component to the complex number to control the speed of rotation</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">f</span> <span class="o">=</span> <span class="p">.</span><span class="mi">3</span>  <span class="c1"># frequency
</span><span class="n">theta</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span>  <span class="c1"># 1 rotation
</span><span class="n">dt</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span> <span class="o">+</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">complex_num</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">1j</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">*</span><span class="n">f</span><span class="o">*</span> <span class="n">dt</span><span class="p">)</span>   <span class="c1"># exp(1j * 2*np.pi* f * t) or exp(1j * omega * dt)
</span>
<span class="n">anim</span> <span class="o">=</span> <span class="n">animate</span><span class="p">(</span><span class="n">complex_num</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>
<span class="n">an</span> <span class="o">=</span> <span class="n">anim</span><span class="p">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="/assets/images/euler_post/anim3.gif" alt="gif" /></p>

<h3 id="defining-a-sinonoidal-function">defining a sinonoidal function</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># defining a sinonoidal function
</span><span class="n">g_t</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">dt</span><span class="p">)</span><span class="o">+</span><span class="mf">0.5</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">g_t</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="/assets/images/euler_post/eluer_anim_6_0.png" alt="png" /></p>

<h3 id="scaling-the-rotations-with-the-function-gt">Scaling the rotations with the function \(g(t)\)</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># multiplying the complex number with the function g_t scales the rotating complex number
</span><span class="n">theta</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span>  <span class="c1"># 1 rotation
</span><span class="n">f</span> <span class="o">=</span> <span class="p">.</span><span class="mi">3</span>  <span class="c1"># frequency
</span><span class="n">dt</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span> <span class="o">+</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">complex_num</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">1j</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">*</span><span class="n">f</span><span class="o">*</span> <span class="n">dt</span><span class="p">)</span><span class="o">*</span><span class="n">g_t</span>

<span class="n">anim</span> <span class="o">=</span> <span class="n">animate</span><span class="p">(</span><span class="n">complex_num</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>
<span class="n">an</span> <span class="o">=</span> <span class="n">anim</span><span class="p">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="/assets/images/euler_post/anim4.gif" alt="gif" /></p>

<h3 id="why-not-vectors">Why Not Vectors?</h3>

<p>At this point, one might wonder, why not use vectors? Vectors can describe points in space and can be scaled and rotated. The answer lies in the algebraic closure of complex numbers and their ability to simplify many mathematical derivations. Complex numbers allow for a unified and elegant representation that turns complicated mathematical operations into simple algebraic ones, especially when it comes to Fourier Transform and MR physics.</p>

<h3 id="the-role-of-complex-numbers-in-fourier-transform">The Role of Complex Numbers in Fourier Transform</h3>

<p>Fourier Transform breaks down any signal into a sum of sinusoids. In essence, it transforms our signal into a new domain where it’s expressed as a linear combination of exponential functions. This is where complex numbers shine.</p>

\[X(f) = \int_{-\infty}^{\infty} x(t) \cdot e^{-i 2 \pi f t} \, dt\]

<p>The role of complex numbers here is twofold:</p>

<ol>
  <li>The sinusoidal functions are compactly represented using Euler’s formula.</li>
  <li>The rotation and scaling properties of complex numbers help in understanding how each frequency component contributes to the overall signal.</li>
</ol>

<h3 id="key-takeaways">Key Takeaways</h3>

<ol>
  <li>
    <p><strong>Complex Plane Representation</strong>: Complex numbers can be visualized in a 2D Cartesian coordinate system known as the complex plane. In this plane, the real part serves as the x-axis, while the imaginary part serves as the y-axis.</p>
  </li>
  <li>
    <p><strong>Scaling and Rotation</strong>: Similar to vectors, complex numbers can be scaled and rotated within the 2D complex plane. These operations are particularly useful for understanding various mathematical and physical phenomena.</p>
  </li>
  <li>
    <p><strong>Exponential Form</strong>: Euler’s formula provides an elegant exponential representation for complex numbers. This form is not only mathematically compact but also extremely useful for understanding the rotation and scaling of complex numbers over time.</p>
  </li>
  <li>
    <p><strong>Trigonometric Components</strong>: The rotation of a complex number inherently involves sinusoidal (sine) and cosinusoidal (cosine) components. This is best represented through Euler’s formula, \(e^{ix} = \cos(x) + i\sin(x)\).</p>
  </li>
  <li>
    <p><strong>Fourier Transform</strong>: Understanding these properties of complex numbers is crucial when delving into Fourier Transform. The Fourier Transform essentially decomposes a signal into a series of scaled and rotated complex numbers, which are essentially sinusoids in disguise.</p>
  </li>
</ol>]]></content><author><name>Snawar Hussain</name></author><category term="Blog" /><category term="Computational Modeling" /><category term="MR Physics" /><category term="Complex Numbers" /><category term="Fourier Transform" /><category term="MR Physics" /><category term="Rotations" /><category term="MRI" /><summary type="html"><![CDATA[Exploring of complex numbers and their role in Fourier Transform and Magnetic Resonance Physics with code. This guide elucidates the mathematical foundations and practical applications in MR imaging.]]></summary></entry><entry><title type="html">Understanding the Reparameterization Trick in Variational Autoencoders</title><link href="https://snawarhussain.com/blog/genrative%20models/python/vae/tutorial/machine%20learning/Reparameterization-trick-in-VAEs-explained/" rel="alternate" type="text/html" title="Understanding the Reparameterization Trick in Variational Autoencoders" /><published>2023-07-27T00:00:00+00:00</published><updated>2023-07-27T00:00:00+00:00</updated><id>https://snawarhussain.com/blog/genrative%20models/python/vae/tutorial/machine%20learning/Reparameterization-trick-in-VAEs-explained</id><content type="html" xml:base="https://snawarhussain.com/blog/genrative%20models/python/vae/tutorial/machine%20learning/Reparameterization-trick-in-VAEs-explained/"><![CDATA[<p>Variational Autoencoders (VAEs) have become increasingly popular in the machine learning field for their ability to perform effective unsupervised learning. They have found a wide range of applications, from image generation to anomaly detection. A key component of VAEs that differentiates them from standard autoencoders is the use of the so-called “reparameterization trick”. This article aims to demystify this concept and provide a clear understanding of its purpose and operation.</p>

<h2 id="what-is-the-reparameterization-trick">What is the Reparameterization Trick?</h2>

<p>The reparameterization trick is a mathematical operation used in the training process of VAEs. It’s a technique that helps bypass a significant problem in training VAEs: the backpropagation algorithm cannot be applied directly through random nodes.</p>

<p>The VAE architecture includes a sampling operation where we sample latent variables from a distribution parameterized by the outputs of the encoder. The direct application of backpropagation here is problematic because of the inherent randomness of the sampling operation.
In other words, since computing the latent \(z\) involve sampling from a (multivariate normal) distribution. This sampling operation introduce stochasticity and therefore cannot be differentiated.</p>
<p align="center">
<img src="/assets/images/vae/vae.png" />
 <figcaption>VAE illustration. The gray part of the model is the reparameterization trick to compute the latent z</figcaption>
</p>

<p class="notice--info"><strong>Note:</strong> The reason this operation is stochastic, or random, is because drawing a sample from a probability distribution is a random process. Even though the parameters of the distribution (the mean and variance) are fixed output of the encoder for a given input, the actual samples that you draw from the distribution will vary each time you draw a sample. This is the nature of sampling from a probability distribution</p>

<h2 id="why-do-we-need-the-reparameterization-trick">Why Do We Need the Reparameterization Trick?</h2>

<p>Let’s consider once again the structure of a VAE: it comprises an encoder, a decoder, and a latent space in between. The encoder’s role is to map inputs to a distribution in the latent space, defined by two parameters: a mean \((μ)\) and a standard deviation \((σ)\).</p>

<p>This latent distribution represents the learned representation of the input data. The decoder then generates an output by sampling points from this distribution. However, if we were to sample these points directly, this operation introduces a stochastic element that prevents the direct application of backpropagation.</p>

<p>Backpropagation relies on computing gradients of deterministic (i.e., non-random) operations. Therefore, we need a method that introduces the necessary randomness for sampling while preserving the differentiability of the operations involved. This is where the reparameterization trick comes in.</p>

<p class="notice--info">The backpropagation algorithm, which is used to train neural networks, requires the ability to compute exact gradients. Because the sampling operation is random, it doesn’t have a well-defined gradient (weights always randomly changing). This means that we can’t use the backpropagation algorithm to train the encoder and decoder networks in a VAE.</p>

<h2 id="the-reparameterization-trick-unveiled">The Reparameterization Trick Unveiled</h2>

<p>The reparameterization trick works by separating the deterministic and the stochastic parts of the sampling operation. Instead of directly sampling from the distribution \(N(μ, σ^2)\), we sample ε from a standard Normal distribution \(N(0, 1)\) and compute the desired sample z as:</p>

\[z = μ + σ * ε\]

<p>Here, \(ε\) introduces the necessary randomness. The operation \(μ + σ * ε\) is entirely deterministic and differentiable, meaning we can apply backpropagation through it.</p>

<p>This method allows us to incorporate the random element required for sampling from the latent distribution while preserving the chain of differentiable operations needed for backpropagation.</p>

<h2 id="why-not-μ--σ">Why Not \(μ + σ\)?</h2>

<p>You might be wondering why we can’t simply compute \(z\) as \(μ + σ\). Without the randomness introduced by \(ε\), every time we run the encoder with the same input, we’d get the exact same output \(z\). This does not reflect the probabilistic nature of the latent space that we’re trying to model.</p>

<p>In addition, during training, we want the model to learn to map an input to a region in the latent space, not just to a single point. This is made possible by the randomness introduced by \(ε\), which is key to the model’s ability to generate diverse outputs during the decoding process.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The reparameterization trick is a powerful method that makes the training of VAEs possible and efficient. By cleverly separating the random and deterministic elements of the sampling operation in the VAE, it allows us to leverage the power of backpropagation while maintaining the stochastic nature of the model. Understanding this trick is key to gaining a deeper insight into the workings of VAEs and their various applications in machine learning.</p>]]></content><author><name>Snawar Hussain</name></author><category term="Blog" /><category term="Genrative Models" /><category term="Python" /><category term="VAE" /><category term="Tutorial" /><category term="Machine Learning" /><category term="Python3" /><category term="Math for AI" /><category term="VAE" /><category term="Variational Autoencoder" /><category term="Generative Models" /><category term="3D Visualization" /><category term="Motion Capture" /><category term="Animation" /><category term="High FPS 3D plotting" /><summary type="html"><![CDATA[Explore the intricacies of Variational Autoencoders (VAEs) and the pivotal role of the reparameterization trick in their training process. Learn how this ingenious technique circumvents the challenges posed by stochastic sampling operations, paving the way for the application of backpropagation and gradient-based optimization methods.]]></summary></entry><entry><title type="html">Visualizing and Animating 3D Motion Capture Data with PyVista</title><link href="https://snawarhussain.com/blog/data%20visualization/python/tutorial/3D-motion-capture-data-animation-with-pyvista/" rel="alternate" type="text/html" title="Visualizing and Animating 3D Motion Capture Data with PyVista" /><published>2023-07-13T00:00:00+00:00</published><updated>2023-07-13T00:00:00+00:00</updated><id>https://snawarhussain.com/blog/data%20visualization/python/tutorial/3D-motion-capture-data-animation-with-pyvista</id><content type="html" xml:base="https://snawarhussain.com/blog/data%20visualization/python/tutorial/3D-motion-capture-data-animation-with-pyvista/"><![CDATA[<h1 id="visualizing-and-animating-3d-motion-capture-data-with-pyvista">Visualizing and Animating 3D Motion Capture Data with PyVista</h1>

<p>3D visualization is crucial for understanding complex data structures and models, especially in the realm of physics, engineering, and computer graphics. Motion capture data, in particular, is heavily used in film industry, game development, biomedical research, computational ethology and sports science. Matplotlib has been the go-to library for many Python users due to its simplicity and extensive features. However, for 3D visualization, particularly for rendering and animating motion capture data, we hit a snag. The rendering process in Matplotlib is quite slow and often becomes impractical when dealing with a large number of frames. This is where PyVista comes into play.</p>

<p>PyVista is a 3D visualization and analysis library that provides a streamlined, Pythonic interface to the Visualization Toolkit (VTK). It is capable of creating high quality 3D visualizations and animations with high frame rates, making it an excellent choice for visualizing motion capture data.</p>

<h2 id="1-setting-up-the-scene">1. Setting Up The Scene</h2>

<p>The first step in visualizing 3D motion capture data is to set up the scene. This involves creating the 3D body parts and skeleton connections, and initializing the plotter.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pyvista</span> <span class="k">as</span> <span class="n">pv</span>

<span class="k">def</span> <span class="nf">create_body_part_mesh</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">pv</span><span class="p">.</span><span class="n">Sphere</span><span class="p">(</span><span class="n">radius</span><span class="o">=</span><span class="n">radius</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">create_skeleton_line</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">z1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">z2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">pv</span><span class="p">.</span><span class="n">Line</span><span class="p">((</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">z1</span><span class="p">),</span> <span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">z2</span><span class="p">))</span>

<span class="n">plotter</span> <span class="o">=</span> <span class="n">pv</span><span class="p">.</span><span class="n">Plotter</span><span class="p">(</span><span class="n">notebook</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">off_screen</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="2-loading-motion-capture-data">2. Loading Motion Capture Data</h2>

<p>Next, we load the motion capture data and add each body part and skeleton connection to the plotter. The data usually comes in a structured format where each row represents a frame and each column represents a body part or a skeleton connection.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">frame_rate</span> <span class="o">=</span> <span class="mi">30</span>
<span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_frames</span><span class="p">):</span>
    <span class="n">plotter</span><span class="p">.</span><span class="n">clear</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">body_part</span> <span class="ow">in</span> <span class="n">body_parts</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">num</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">body_part</span><span class="si">}</span><span class="s">_x'</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">num</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">body_part</span><span class="si">}</span><span class="s">_y'</span><span class="p">]</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">num</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">body_part</span><span class="si">}</span><span class="s">_z'</span><span class="p">]</span>
        <span class="n">mesh</span> <span class="o">=</span> <span class="n">create_body_part_mesh</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
        <span class="n">plotter</span><span class="p">.</span><span class="n">add_mesh</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"red"</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">connection</span> <span class="ow">in</span> <span class="n">skeleton</span><span class="p">:</span>
        <span class="n">part1</span> <span class="o">=</span> <span class="n">connection</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">part2</span> <span class="o">=</span> <span class="n">connection</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">z1</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">num</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">part1</span><span class="si">}</span><span class="s">_x'</span><span class="p">],</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">num</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">part1</span><span class="si">}</span><span class="s">_y'</span><span class="p">],</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">num</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">part1</span><span class="si">}</span><span class="s">_z'</span><span class="p">]</span>
        <span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">z2</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">num</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">part2</span><span class="si">}</span><span class="s">_x'</span><span class="p">],</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">num</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">part2</span><span class="si">}</span><span class="s">_y'</span><span class="p">],</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">num</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">part2</span><span class="si">}</span><span class="s">_z'</span><span class="p">]</span>
        <span class="n">skl_mesh</span> <span class="o">=</span> <span class="n">create_skeleton_line</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">z1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">z2</span><span class="p">)</span>
        <span class="n">plotter</span><span class="p">.</span><span class="n">add_mesh</span><span class="p">(</span><span class="n">skl_mesh</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"blue"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="3-animating-the-scene">3. Animating the Scene</h2>

<p>To animate the motion capture data, we loop over each frame and update the points of each mesh. We then render the plotter and write the current frame to a file.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_frames</span><span class="p">):</span>
    <span class="n">mean_position</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">body_part</span><span class="p">,</span> <span class="n">mesh</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">body_parts</span><span class="p">,</span> <span class="n">body_part_meshes</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">num</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">body_part</span><span class="si">}</span><span class="s">_x'</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">num</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">body_part</span><span class="si">}</span><span class="s">_y'</span><span class="p">]</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">num</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">body_part</span><span class="si">}</span><span class="s">_z'</span><span class="p">]</span>
        <span class="n">mesh</span><span class="p">.</span><span class="n">points</span> <span class="o">=</span> <span class="n">pv</span><span class="p">.</span><span class="n">Sphere</span><span class="p">(</span><span class="n">radius</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">)).</span><span class="n">points</span>
        <span class="n">mean_position</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">])</span>
    <span class="n">mean_position</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">body_parts</span><span class="p">)</span>
    <span class="n">plotter</span><span class="p">.</span><span class="n">camera</span><span class="p">.</span><span class="n">focal_point</span> <span class="o">=</span> <span class="n">mean_position</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">connection</span><span class="p">,</span> <span class="n">mesh</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">skeleton</span><span class="p">,</span> <span class="n">skeleton_line_meshes</span><span class="p">):</span>
        <span class="n">part1</span> <span class="o">=</span> <span class="n">connection</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">part2</span> <span class="o">=</span> <span class="n">connection</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">z1</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">num</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">part1</span><span class="si">}</span><span class="s">_x'</span><span class="p">],</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">num</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">part1</span><span class="si">}</span><span class="s">_y'</span><span class="p">],</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">num</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">part1</span><span class="si">}</span><span class="s">_z'</span><span class="p">]</span>
        <span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">z2</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">num</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">part2</span><span class="si">}</span><span class="s">_x'</span><span class="p">],</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">num</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">part2</span><span class="si">}</span><span class="s">_y'</span><span class="p">],</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">num</span><span class="p">,</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">part2</span><span class="si">}</span><span class="s">_z'</span><span class="p">]</span>
        <span class="n">mesh</span><span class="p">.</span><span class="n">points</span> <span class="o">=</span> <span class="n">pv</span><span class="p">.</span><span class="n">Line</span><span class="p">((</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">z1</span><span class="p">),</span> <span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">z2</span><span class="p">)).</span><span class="n">points</span>

    <span class="n">plotter</span><span class="p">.</span><span class="n">write_frame</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="4-putting-it-all-together">4. Putting It All Together</h2>

<p>Finally, we put all the code together and run the script. The script will generate a series of PNG files in the current directory. We can then use FFmpeg to convert the PNG files into a video file.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nb">df</span> <span class="o">=</span> pd.read_csv<span class="o">(</span>r<span class="s1">'C:\Users\pc\PycharmProjects\RWKV_for_rat\notebooks\mocap_data_interpolated_mod.csv'</span><span class="o">)</span>
body_parts <span class="o">=</span> list<span class="o">(</span><span class="nb">set</span><span class="o">(</span>col.split<span class="o">(</span><span class="s1">'_'</span><span class="o">)[</span>0] <span class="k">for </span>col <span class="k">in </span>df.columns <span class="k">if</span> <span class="s1">'_'</span> <span class="k">in </span>col<span class="o">))</span>
num_frames <span class="o">=</span> len<span class="o">(</span><span class="nb">df</span><span class="o">)</span>
skeleton <span class="o">=</span> <span class="o">[</span>                    <span class="c">#defining the skeletal structure for plotting skeletal lines.</span>
    <span class="o">[</span><span class="s1">'HeadF'</span>, <span class="s1">'HeadB'</span><span class="o">]</span>,
    <span class="o">[</span><span class="s1">'HeadB'</span>, <span class="s1">'HeadL'</span><span class="o">]</span>,
    <span class="o">[</span><span class="s1">'HeadF'</span>, <span class="s1">'HeadL'</span><span class="o">]</span>,
    <span class="o">[</span><span class="s1">'HeadF'</span>, <span class="s1">'SpineF'</span><span class="o">]</span>,
    <span class="o">[</span><span class="s1">'HeadB'</span>, <span class="s1">'SpineF'</span><span class="o">]</span>,
    <span class="o">[</span><span class="s1">'HeadL'</span>, <span class="s1">'SpineF'</span><span class="o">]</span>,
    <span class="o">[</span><span class="s1">'SpineF'</span>, <span class="s1">'SpineM'</span><span class="o">]</span>,
    <span class="o">[</span><span class="s1">'SpineM'</span>, <span class="s1">'SpineL'</span><span class="o">]</span>,
    <span class="o">[</span><span class="s1">'SpineF'</span>, <span class="s1">'Offset1'</span><span class="o">]</span>,
    <span class="o">[</span><span class="s1">'Offset1'</span>, <span class="s1">'Offset2'</span><span class="o">]</span>,
    <span class="o">[</span><span class="s1">'Offset1'</span>, <span class="s1">'SpineM'</span><span class="o">]</span>,
    <span class="o">[</span><span class="s1">'Offset2'</span>, <span class="s1">'SpineL'</span><span class="o">]</span>,
    <span class="o">[</span><span class="s1">'Offset2'</span>, <span class="s1">'SpineM'</span><span class="o">]</span>,
    <span class="o">[</span><span class="s1">'SpineF'</span>, <span class="s1">'ShoulderL'</span><span class="o">]</span>,
    <span class="o">[</span><span class="s1">'SpineF'</span>, <span class="s1">'ShoulderR'</span><span class="o">]</span>,
    <span class="o">[</span><span class="s1">'ShoulderL'</span>, <span class="s1">'ElbowL'</span><span class="o">]</span>,
    <span class="o">[</span><span class="s1">'ArmL'</span>, <span class="s1">'ElbowL'</span><span class="o">]</span>,
    <span class="o">[</span><span class="s1">'ShoulderR'</span>, <span class="s1">'ElbowR'</span><span class="o">]</span>,
    <span class="o">[</span><span class="s1">'ArmR'</span>, <span class="s1">'ElbowR'</span><span class="o">]</span>,
    <span class="o">[</span><span class="s1">'SpineL'</span>, <span class="s1">'HipL'</span><span class="o">]</span>,
    <span class="o">[</span><span class="s1">'SpineL'</span>, <span class="s1">'HipR'</span><span class="o">]</span>,
    <span class="o">[</span><span class="s1">'HipL'</span>, <span class="s1">'KneeL'</span><span class="o">]</span>,
    <span class="o">[</span><span class="s1">'KneeL'</span>, <span class="s1">'ShinL'</span><span class="o">]</span>,
    <span class="o">[</span><span class="s1">'HipR'</span>, <span class="s1">'KneeR'</span><span class="o">]</span>,
    <span class="o">[</span><span class="s1">'KneeR'</span>, <span class="s1">'ShinR'</span><span class="o">]</span>
<span class="o">]</span>

<span class="c"># Assuming that our data is in a csv file wiht the following format:</span>
<span class="c"># Each row represents a frame and we have 3 columns for each body part (x, y, z)</span>

import pyvista as pv
import numpy as np
import <span class="nb">time
</span>from matplotlib.cm import get_cmap

def create_body_part_mesh<span class="o">(</span>x, y, z, <span class="nv">radius</span><span class="o">=</span>5<span class="o">)</span>:
    <span class="s2">"""Create a sphere mesh representing a body part."""</span>
    <span class="k">return </span>pv.Sphere<span class="o">(</span><span class="nv">radius</span><span class="o">=</span>radius, <span class="nv">center</span><span class="o">=(</span>x, y, z<span class="o">))</span>

def create_skeleton_line<span class="o">(</span>x1, y1, z1, x2, y2, z2<span class="o">)</span>:
    <span class="s2">"""Create a line mesh representing a skeleton connection."""</span>
    <span class="k">return </span>pv.Line<span class="o">((</span>x1, y1, z1<span class="o">)</span>, <span class="o">(</span>x2, y2, z2<span class="o">))</span>

<span class="c">#Initialize the plotter</span>
plotter <span class="o">=</span> pv.Plotter<span class="o">(</span><span class="nv">notebook</span><span class="o">=</span>False, <span class="nv">off_screen</span><span class="o">=</span>False<span class="o">)</span>
plotter.open_gif<span class="o">(</span><span class="s2">"rat.gif"</span><span class="o">)</span>
<span class="c"># Assuming you have a suitable frame rate for your data</span>
frame_rate <span class="o">=</span> 30

body_part_meshes <span class="o">=</span> <span class="o">[]</span>
skeleton_line_meshes <span class="o">=</span> <span class="o">[]</span>


<span class="c"># Get a color map</span>
colormap <span class="o">=</span> get_cmap<span class="o">(</span><span class="s2">"tab20"</span><span class="o">)</span>

<span class="c"># Normalize the colormap from 0 to 1</span>
normalize <span class="o">=</span> lambda val: <span class="o">(</span>val - 0<span class="o">)</span> / <span class="o">(</span>1 - 0<span class="o">)</span>

<span class="c"># Assuming min_val and max_val are the minimum and maximum values for the data range</span>
<span class="c"># If your data ranges from 0 to N (where N &gt; 0), you can set min_val = 0 and max_val = N</span>

body_part_colors <span class="o">=</span> <span class="o">[</span>colormap<span class="o">(</span>i<span class="o">)</span> <span class="k">for </span>i <span class="k">in </span>range<span class="o">(</span>len<span class="o">(</span>body_parts<span class="o">))]</span>


<span class="c"># Create initial meshes</span>
<span class="k">for </span>body_part, color <span class="k">in </span>zip<span class="o">(</span>body_parts, body_part_colors<span class="o">)</span>:
    x <span class="o">=</span> df.loc[0, f<span class="s1">'{body_part}_x'</span><span class="o">]</span>
    y <span class="o">=</span> df.loc[0, f<span class="s1">'{body_part}_y'</span><span class="o">]</span>
    z <span class="o">=</span> df.loc[0, f<span class="s1">'{body_part}_z'</span><span class="o">]</span>
    mesh <span class="o">=</span> create_body_part_mesh<span class="o">(</span>x, y, z<span class="o">)</span>
    plotter.add_mesh<span class="o">(</span>mesh, <span class="nv">color</span><span class="o">=</span>color<span class="o">)</span>
    body_part_meshes.append<span class="o">(</span>mesh<span class="o">)</span>
    
<span class="k">for </span>connection <span class="k">in </span>skeleton:
    part1 <span class="o">=</span> connection[0]
    part2 <span class="o">=</span> connection[1]
    x1, y1, z1 <span class="o">=</span> df.loc[0, f<span class="s1">'{part1}_x'</span><span class="o">]</span>, df.loc[0, f<span class="s1">'{part1}_y'</span><span class="o">]</span>, df.loc[0, f<span class="s1">'{part1}_z'</span><span class="o">]</span>
    x2, y2, z2 <span class="o">=</span> df.loc[0, f<span class="s1">'{part2}_x'</span><span class="o">]</span>, df.loc[0, f<span class="s1">'{part2}_y'</span><span class="o">]</span>, df.loc[0, f<span class="s1">'{part2}_z'</span><span class="o">]</span>
    skl_mesh <span class="o">=</span> create_skeleton_line<span class="o">(</span>x1, y1, z1, x2, y2, z2<span class="o">)</span>
    plotter.add_mesh<span class="o">(</span>skl_mesh, <span class="nv">color</span><span class="o">=</span><span class="s2">"black"</span>, <span class="nv">line_width</span><span class="o">=</span>2<span class="o">)</span>
    skeleton_line_meshes.append<span class="o">(</span>skl_mesh<span class="o">)</span>

<span class="c"># Loop over each frame</span>
<span class="k">for </span>num <span class="k">in </span>range<span class="o">(</span>1, num_frames<span class="o">)</span>: <span class="c"># start from 1 as we already plotted the first frame</span>
    <span class="c"># Update each body part</span>
    mean_position <span class="o">=</span> np.array<span class="o">([</span>0.0, 0.0, 0.0]<span class="o">)</span>
    <span class="k">for </span>body_part, mesh <span class="k">in </span>zip<span class="o">(</span>body_parts, body_part_meshes<span class="o">)</span>:
        x <span class="o">=</span> df.loc[num, f<span class="s1">'{body_part}_x'</span><span class="o">]</span>
        y <span class="o">=</span> df.loc[num, f<span class="s1">'{body_part}_y'</span><span class="o">]</span>
        z <span class="o">=</span> df.loc[num, f<span class="s1">'{body_part}_z'</span><span class="o">]</span>
        mesh.points <span class="o">=</span> pv.Sphere<span class="o">(</span><span class="nv">radius</span><span class="o">=</span>5, <span class="nv">center</span><span class="o">=(</span>x, y, z<span class="o">))</span>.points  <span class="c"># Update points of existing mesh</span>
        mean_position +<span class="o">=</span> np.array<span class="o">([</span>x, y, z]<span class="o">)</span>

    mean_position /<span class="o">=</span> len<span class="o">(</span>body_parts<span class="o">)</span>  <span class="c"># Calculate mean position of all body parts</span>
    plotter.camera.focal_point <span class="o">=</span> mean_position.tolist<span class="o">()</span>  <span class="c"># Update the camera focal point</span>

    <span class="c"># Update each skeleton connection</span>
    <span class="k">for </span>connection, mesh <span class="k">in </span>zip<span class="o">(</span>skeleton, skeleton_line_meshes<span class="o">)</span>:
        part1 <span class="o">=</span> connection[0]
        part2 <span class="o">=</span> connection[1]
        x1, y1, z1 <span class="o">=</span> df.loc[num, f<span class="s1">'{part1}_x'</span><span class="o">]</span>, df.loc[num, f<span class="s1">'{part1}_y'</span><span class="o">]</span>, df.loc[num, f<span class="s1">'{part1}_z'</span><span class="o">]</span>
        x2, y2, z2 <span class="o">=</span> df.loc[num, f<span class="s1">'{part2}_x'</span><span class="o">]</span>, df.loc[num, f<span class="s1">'{part2}_y'</span><span class="o">]</span>, df.loc[num, f<span class="s1">'{part2}_z'</span><span class="o">]</span>
        mesh.points <span class="o">=</span> pv.Line<span class="o">((</span>x1, y1, z1<span class="o">)</span>, <span class="o">(</span>x2, y2, z2<span class="o">))</span>.points  <span class="c"># Update points of existing mesh</span>

    plotter.write_frame<span class="o">()</span>  <span class="c"># Write the current frame</span>
plotter.close<span class="o">()</span>
</code></pre></div></div>
<!-- add mocap viz .GIF -->
<p><img src="/assets/images/mocap/mocap.gif" alt="mocap viz" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<p>In this post, we’ve explored how to use PyVista to visualize and animate 3D motion capture data with high frame rate. By leveraging the power of PyVista, we can create compelling visualizations that not only bring our data to life but also provide deep insights into the underlying dynamicsand animations with high frame rates, making it an excellent choice for visualizing motion capture data.</p>]]></content><author><name>Snawar Hussain</name></author><category term="Blog" /><category term="Data Visualization" /><category term="Python" /><category term="Tutorial" /><category term="Python3" /><category term="PyVista" /><category term="3D Visualization" /><category term="Motion Capture" /><category term="Animation" /><category term="High FPS 3D plotting" /><summary type="html"><![CDATA[A comprehensive Python tutorial on visualizing and animating 3D motion capture data using PyVista with high frame rate.]]></summary></entry><entry><title type="html">Unraveling the Complexity of Neural Data: Neural Modes, Manifolds, and Dimensionality Reduction</title><link href="https://snawarhussain.com/blog/neuroscience/PCA-Neural-modes-and-Neural-Manifolds/" rel="alternate" type="text/html" title="Unraveling the Complexity of Neural Data: Neural Modes, Manifolds, and Dimensionality Reduction" /><published>2023-06-08T00:00:00+00:00</published><updated>2023-06-08T00:00:00+00:00</updated><id>https://snawarhussain.com/blog/neuroscience/PCA-Neural-modes-and-Neural-Manifolds</id><content type="html" xml:base="https://snawarhussain.com/blog/neuroscience/PCA-Neural-modes-and-Neural-Manifolds/"><![CDATA[<table>
  <thead>
    <tr>
      <th>Symbol</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>\(X\)</td>
      <td>The original data matrix, where each row corresponds to a neuron and each column corresponds to a time point</td>
    </tr>
    <tr>
      <td>\(N\)</td>
      <td>The number of neurons</td>
    </tr>
    <tr>
      <td>\(M\)</td>
      <td>The number of observations (time points)</td>
    </tr>
    <tr>
      <td>\(x_{ij}\)</td>
      <td>The activity of the \(i\)th neuron at the \(j\)th time point</td>
    </tr>
    <tr>
      <td>\(v_1, v_2, v_3\)</td>
      <td>The first three principal components</td>
    </tr>
    <tr>
      <td>\(v_{ij}\)</td>
      <td>The contribution of the \(j\)th time point to the \(i\)th principal component</td>
    </tr>
    <tr>
      <td>\(V\)</td>
      <td>The matrix of the first three principal components</td>
    </tr>
    <tr>
      <td>\(A\)</td>
      <td>The matrix of activation dynamics of the principal components</td>
    </tr>
    <tr>
      <td>\(a_{ij}\)</td>
      <td>The activation of the \(i\)th principal component for the \(j\)th neuron</td>
    </tr>
    <tr>
      <td>\(X_{recons}\)</td>
      <td>The reconstructed data matrix</td>
    </tr>
    <tr>
      <td>\(x_{recons,ij}\)</td>
      <td>The reconstructed activity of the \(i\)th neuron at the \(j\)th time point</td>
    </tr>
  </tbody>
</table>

<h2 id="introduction">Introduction</h2>

<p>The human brain, with its billions of interconnected neurons, is a complex system that generates a vast amount of data. Understanding this data is a significant challenge in neuroscience. One of the key tools neuroscientists use to tackle this challenge is dimensionality reduction, a statistical technique that simplifies high-dimensional data while preserving its essential structure. In this post, we’ll explore how dimensionality reduction, particularly through methods like Principal Component Analysis (PCA), helps us understand neural data by identifying neural modes and neural manifolds.</p>

<h2 id="preprocessing-of-neural-spike-data">Preprocessing of Neural Spike Data</h2>

<p>Neural spikes can be recorded by inserting an electrode into a specific region of interest in the brain. This allows us to capture the neural spiking activity of hundreds of neurons in the vicinity. This neural spike activity is stored in a matrix where each row corresponds to a different neuron and each column contains the time in seconds/milliseconds when that specific neuron fired. This is a common format for neural spiking data, but it’s not immediately suitable for analysis like dimensionality reduction, which requires a fixed-length vector for each observation (in this case, each time point).</p>

<h3 id="binning">Binning</h3>

<p>One common way to handle this kind of data is to convert it into a “binned” format, where you divide time into small bins and count the number of spikes from each neuron in each bin. This gives you a 2D array where each row corresponds to a time bin and each column corresponds to a neuron.</p>

<h3 id="smoothing">Smoothing</h3>

<p>The raw binned data can be quite noisy, as it includes both the signal (the underlying neural activity) and the noise (random fluctuations). Smoothing can help reduce this noise and make the signal more apparent. One common way to smooth the data is to convolve it with a Gaussian kernel. This is equivalent to replacing each spike count with a weighted average of the spike counts in nearby time bins, where the weights are determined by a Gaussian function. This has the effect of smoothing out rapid fluctuations in the spike counts.</p>

<p>After initial binning and smoothing, we get
the data \(X\) in an \(M \times{N}\) matrix format where \(N\) is the number of neurons and the \(M\) is the fixed length time dimension.</p>

<p>Given this format, the neural population activity at any given point in time \(t\) can be plotted in this \(N\)-dimensional space that is spanned by the \(N\) number of neurons. So each sample in this matrix \(X\) is a point in this \(N\)-dimensional space.</p>

<p align="center">
<img src="/assets/images/neural_modes/N-space.png" width="400" />
 <figcaption>Neural population activity spanned across N-dimensional space.</figcaption>
</p>

<p>If we were to plot the recorded neuronal population activity in this \(N\)-dimensional space within a certain time interval, one might hypothesize that the neurons will explore the whole space. But this is not the case.</p>

<p align="center">
<img src="/assets/images/neural_modes/hypo.gif" width="700" />
 <figcaption>Recorded activity of neural population plotted after preprocessing. The neurons do not seem to explore the high dimensional space in its entirety.</figcaption>
</p>

<p>It has been shown through several repeated experiments that a population of neurons does not explore the whole high \(N\)-dimensional space. Instead, due to the fact that these neurons are interconnected to each other and are not entirely independent, their overall activity is confined to a lower dimensional subspace residing in this high dimensional space. These subspaces are often referred to as  <strong>Neural Manifolds</strong>.</p>

<p align="center">
<img src="/assets/images/neural_modes/low_mani.gif" width="700" />
 <figcaption>Population of neurons confining their activity into a lower dimensional manifold within the high N-dimensional space due to interconnectivity.</figcaption>
</p>

<p>These Neural Manifolds can be uncovered with dimensionality reduction techniques like PCA. The idea is to reduce the dimensionality of the data and to identify these main modes of variation in the neural activity.</p>

<p>The Manifolds are spanned by these <strong>Neural Modes</strong>. These modes capture the coordinated activity among neurons that often underlie specific functions or behaviors.</p>

<h2 id="principle-component-analysis">Principle Component Analysis</h2>

<p>Principal Component Analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors are an uncorrelated orthogonal basis set.</p>

<p>In other words, the Principal components are basically the orthogonal neural modes. When the original neural population activity is projected onto these neural modes (Principal components), they reveal the underlying pattern of activity. Since the results of projecting the neural activity onto a neural mode is basically equivalent to finding the weighted linear combination of the original neurons that shows their contribution to that specific mode.</p>

<p>Now, let’s dive a bit deeper into the mathematics of PCA.</p>

<p>Sticking to our previous notation for the neural data. Given a dataset \(X\) of dimension \(M \times N\) (where \(M\) is the number of observations and \(N\) is the number of variables), the goal of PCA is to find a new set of variables, the principal components, that are uncorrelated and that explain the most variance. In other words, we want to find a new basis for the data that retains the most information.
In machine learning terms, the \(M\) observations are the samples and the \(N\) variables are the features. The principal components are the new features that we want to find.</p>

<p class="notice--info"><strong>Note:</strong> In case of our neural data, our observations (\(N\)) are the neurons and the and each value across \(M\) is the activity of a certain neuron at a certain time point. According to the hypothesis we established earlier, the neural activity is confined to a lower dimensional manifold. So, the principal components are the new features that we want to find that span this \(d\)-dimensional manifold such that \(d &lt; N\). Implying that the neural population activity can be explained with a lower number of hidden variables (refer to as neural modes) instead of the original \(N\) neurons.</p>

<p>The first step in PCA is to standardize the data. This involves subtracting the mean and dividing by the standard deviation for each variable. Let’s denote the standardized data as \(Z\).</p>

<p>The next step is to compute the covariance matrix of \(Z\). The covariance matrix \(C\) is given by:</p>

\[C = \frac{1}{N-1} Z^T Z\]

<p>The covariance matrix is a symmetric matrix that contains the variances of the variables on its diagonal and the covariances between each pair of variables in the other entries.</p>

<p>The next step is to compute the eigenvalues and eigenvectors of the covariance matrix. The eigenvectors of the covariance matrix correspond to the principal components and the eigenvalues correspond to the variance explained by the principal components.</p>

<p>Let’s denote the eigenvalues by \(\lambda_i\) and the eigenvectors by \(v_i\). They satisfy the following equation:</p>

\[C v_i = \lambda_i v_i\]

<p>The principal components are then given by projecting the standardized data \(Z\) onto the eigenvectors:</p>

\[PC_i = Z v_i\]

<p>The variance explained by the \(i\) th principal component is given by the corresponding eigenvalue \(\lambda_i\).</p>

<p>Let’s denote our data matrix as \(X\), where each row corresponds to a single time point and each column corresponds to a single neuron. After preprocessing and standardizing our data, we perform PCA on \(X\).</p>

<p>The PCA procedure will yield a set of eigenvectors \(v_i\) (the principal components) and corresponding eigenvalues \(\lambda_i\). Each eigenvector \(v_i\) is a vector of the same length as the number of neurons, and its elements are the coefficients of the linear combination of the original neurons’ activities that forms the principal component.</p>

<p>So, if we have \(N\) neurons, each eigenvector \(v_i\) will be a \(N\)-dimensional vector, where the \(j\)th element of \(v_i\) represents the contribution of the \(j\)th neuron to the \(i\)th principal component.</p>

<p>To be more explicit, if \(v_i = [c_1, c_2, ..., c_N]\) is the \(i\)th principal component, then the \(i\)th principal component is a linear combination of the original neurons given by:</p>

\[PC_i = c_1 * neuron_1 + c_2 * neuron_2 + ... + c_N * neuron_N\]

<p>where \(c_j\) is the contribution (or weight) of the \(j\) th neuron to the \(i\) th principal component, and \(neuron_j\) is the activity of the \(j\) th neuron.</p>

<p>These weights \(c_j\) can be positive or negative, and their magnitude indicates the degree to which each neuron contributes to the principal component. A large positive weight means that the neuron strongly contributes to the principal component, while a large negative weight means that the neuron contributes in the opposite direction. A weight close to zero means that the neuron does not contribute much to the principal component.</p>

<p class="notice--warning"><strong>Note:</strong> The goal of apply PCA is to achieve dimetionality reduction across the neurons and NOT the time points.</p>

<p>Let’s consider our original  neural dataset \(X\)  of dimension \(M \times N\) , where \(N\)  is the number of neurons and \(M\)  is the number of observations (time points). The data matrix \(X\)  can be written as:</p>

\[X = [x_1, x_2, ..., x_N] = \begin{bmatrix}
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1N} \\
x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{M1} &amp; x_{M2} &amp; \cdots &amp; x_{MN}
\end{bmatrix}\]

<p>where \(x_{ij}\)  is the activity at the \(i\) th time pooint of the \(j\) th neuron time point.</p>

<p>After applying PCA, we obtain a set of principal components. Let’s denote the first three principal components as \(v_1\) , \(v_2\) , and \(v_3\) . Each of these is a vector of length \(M\) , so they can be written as:</p>

<p>\(v_1 = [v_{11}, v_{12}, ..., v_{1N}]^T\) <br />
\(v_2 = [v_{21}, v_{22}, ..., v_{2N}]^T\)<br />
\(v_3 = [v_{31}, v_{32}, ..., v_{3N}]^T\)</p>

<p>Here, \(v_{ij}\)  is the contribution of the \(j\) th neuron to the \(i\) th principal component or neural mode.</p>

<p>These principal components can be combined into a \(3 \times N\)  matrix \(V\) , where each row is a principal component:</p>

\[V = [v_1^T; v_2^T; v_3^T] = \begin{bmatrix}
v_{11} &amp; v_{12} &amp; \cdots &amp; v_{1N} \\
v_{21} &amp; v_{22} &amp; \cdots &amp; v_{2N} \\
v_{31} &amp; v_{32} &amp; \cdots &amp; v_{3N}
\end{bmatrix}\]

<p>The activation dynamics of the principal components can be computed by projecting the original data onto the principal components. This can be done via matrix multiplication:</p>

\[A = XV^T\]

<p>Here, \(A\)  is a \(M \times 3\)  matrix, where each column is the activation dynamics of a principal component:</p>

\[A = [a_1, a_2, a_3] = \begin{bmatrix}
a_{11} &amp; a_{21} &amp; a_{31} \\
a_{12} &amp; a_{22} &amp; a_{32} \\
\vdots &amp; \vdots &amp; \vdots \\
a_{M1} &amp; a_{M2} &amp; a_{M3}
\end{bmatrix}\]

<p>Here, \(a_{ij}\)  is the activation of the \(i\) th principal component for the \(j\) th neuron.</p>

<p>So, the matrix \(V\)  represents the principal components in the space of the original time points, and the matrix \(A\)  represents the activation dynamics of the principal components for each neuron.</p>

<p>The original neuron activity can be reconstructed from the principal components by reversing the projection. This can be done via matrix multiplication:</p>

\[X_{recons} = AV\]

<p>Here, \(X_{recons}\) is a \(M \times N\) matrix, where each column is the recons activity of a neuron:</p>

\[X_{recons} =  \\  

[x_{recons,1}, x_{recons,2}, ..., x_{recons,M}] = \\  

\begin{bmatrix}  
x_{recons,11} &amp; x_{recons,12} &amp; \cdots &amp; x_{recons,1N} \\
x_{recons,21} &amp; x_{recons,22} &amp; \cdots &amp; x_{recons,2N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{recons,M1} &amp; x_{recons,M2} &amp; \cdots &amp; x_{recons,MN}
\end{bmatrix}\]

<p>Here, \(x_{recons,ij}\) is the reconstructed activity of the \(i\) th neuron at the \(j\) th time point.</p>

<p>This is how you can interpret the principal components representing the neural modes that are computed as the linear combinations of the original neurons and how the original neural activity can be represented as the linear combination of the activation dynamics \(A\) and the neural modes or eigen matrix \(V\)</p>

<p>Now let’s implement this in python.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#@title Data retrieval
#@markdown This cell downloads the example dataset that we will use in this tutorial.
</span><span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'https://osf.io/sy5xt/download'</span><span class="p">)</span>
<span class="k">if</span> <span class="n">r</span><span class="p">.</span><span class="n">status_code</span> <span class="o">!=</span> <span class="mi">200</span><span class="p">:</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'Failed to download data'</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
  <span class="n">spike_times</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">io</span><span class="p">.</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">r</span><span class="p">.</span><span class="n">content</span><span class="p">),</span> <span class="n">allow_pickle</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="s">'spike_times'</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">spike_times</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(734,)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Define the bin edges (for example, bins of 1 millisecond from 0 to the maximum spike time)
</span><span class="n">bin_edges</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">st</span><span class="p">)</span> <span class="k">for</span> <span class="n">st</span> <span class="ow">in</span> <span class="n">spike_times</span><span class="p">]),</span> <span class="mf">0.001</span><span class="p">)</span>

<span class="c1"># Initialize an empty array to hold the binned data
</span><span class="n">binned_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">bin_edges</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">spike_times</span><span class="p">)))</span>

<span class="c1"># For each neuron, count the number of spikes in each bin
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">neuron_spike_times</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">spike_times</span><span class="p">):</span>
    <span class="n">binned_data</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">neuron_spike_times</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bin_edges</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.ndimage</span> <span class="kn">import</span> <span class="n">gaussian_filter1d</span>

<span class="c1"># Apply a Gaussian filter to each neuron's binned spike counts
</span><span class="n">smoothed_data</span> <span class="o">=</span> <span class="n">gaussian_filter1d</span><span class="p">(</span><span class="n">binned_data</span><span class="p">[:</span><span class="mi">50000</span><span class="p">,</span> <span class="p">:],</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">smoothed_data</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(50000, 734)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Choose a neuron to visualize
</span><span class="n">neuron</span> <span class="o">=</span> <span class="mi">200</span>

<span class="c1"># Plot the raw binned spike counts
</span><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">binned_data</span><span class="p">[:</span><span class="mi">1000</span><span class="p">,</span> <span class="n">neuron</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Raw Binned Spike Counts for Neuron {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">neuron</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Time Bin'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Spike Count'</span><span class="p">)</span>

<span class="c1"># Plot the smoothed spike counts
</span><span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">smoothed_data</span><span class="p">[:</span><span class="mi">1000</span><span class="p">,</span> <span class="n">neuron</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Smoothed Spike Counts for Neuron {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">neuron</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Time Bin'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Spike Count'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/assets/images/neural_modes/nerual_modes_files/nerual_modes_5_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">smoothed_data</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">100</span><span class="p">].</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(50000, 100)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="c1"># Apply PCA
</span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>
<span class="n">pca</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">smoothed_data</span><span class="p">[:,</span> <span class="p">:])</span>

<span class="c1"># Transform the data into the principal component space
</span><span class="n">smoothed_data_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">smoothed_data</span><span class="p">[:,</span> <span class="p">:])</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">pca</span><span class="p">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Explained Variance Ratio of Principal Components'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Principal Component'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Explained Variance Ratio'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/neural_modes/nerual_modes_files/nerual_modes_8_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">smoothed_data_pca</span><span class="p">[:</span><span class="mi">1000</span><span class="p">,</span> <span class="n">i</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">'Principal Component </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/assets/images/neural_modes/nerual_modes_files/nerual_modes_9_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">smoothed_data_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">smoothed_data_pca</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Principal Component 1'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Principal Component 2'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Neural Manifold'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/assets/images/neural_modes/nerual_modes_files/nerual_modes_10_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>


<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s">'3d'</span><span class="p">)</span>

<span class="c1"># Create a sequence of colors based on time
</span><span class="n">colors</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">viridis</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">smoothed_data_pca</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

<span class="c1"># Plot a line in 3D space
</span><span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">smoothed_data_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">smoothed_data_pca</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">smoothed_data_pca</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'darkblue'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># Plot points along the line
</span><span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">smoothed_data_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">smoothed_data_pca</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">smoothed_data_pca</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Principal Component 1'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Principal Component 2'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s">'Principal Component 3'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'3D Visualization of First Three Principal Components'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/assets/images/neural_modes/nerual_modes_files/nerual_modes_11_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Define the number of principal components to plot
</span><span class="n">num_components</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">35</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="c1"># Plot each principal component
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_components</span><span class="p">):</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">num_components</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">smoothed_data_pca</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">'Principal Component </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Time (bins)'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Activity'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/assets/images/neural_modes/nerual_modes_files/nerual_modes_12_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>Applying PCA to neural data, which is essentially time series data, is a common practice in neuroscience. The idea is to reduce the dimensionality of the data and to identify the main modes of variation in the neural activity. Here’s how it can be done</p>

<p><strong>Preprocessing</strong>: Neural data often needs to be preprocessed before PCA can be applied. This can involve steps like filtering the data to remove noise, normalizing the data, or subtracting the mean activity across all neurons.</p>

<p><strong>Formatting the data</strong>: For PCA, the data needs to be in a 2D matrix format, where each row is an observation and each column is a variable. In the context of neural data, an observation could be the activity of all neurons at a single point in time, and a variable would be the activity of a single neuron over time. So, you would reshape your data such that each row of your matrix corresponds to a single time point, and each column corresponds to a single neuron.</p>

<p><strong>Applying PCA</strong>: Once your data is in the correct format, you can apply PCA as usual. This involves computing the covariance matrix of your data, and then finding the eigenvalues and eigenvectors of this matrix. The eigenvectors are the principal components, and they represent the directions in which your data varies the most.</p>

<p><strong>Interpreting the results</strong>: The output of PCA will be a set of principal components, which are orthogonal to each other and explain the maximum variance in the data. Each principal component is a linear combination of the original neurons, and the coefficients in this linear combination tell you how much each neuron contributes to that component. The first few principal components often capture a large portion of the variance in the data, and can be used to visualize and further analyze the data.</p>

<p>It’s important to note that PCA is a linear method, which means it assumes that the main modes of variation in your data are linear. If this is not the case, other methods like Independent Component Analysis (ICA) or Non-negative Matrix Factorization (NMF) might be more appropriate.</p>

<p>Also, PCA does not take into account the temporal structure of the data. If the temporal dynamics are important, methods like time-lagged PCA or dynamical systems analysis might be more appropriate.</p>]]></content><author><name>Snawar Hussain</name></author><category term="Blog" /><category term="Neuroscience" /><category term="Neural Data" /><category term="PCA" /><category term="Dimensionality Reduction" /><category term="Neural Modes" /><category term="Neural Manifolds" /><summary type="html"><![CDATA[A guide to understanding neural data using dimensionality reduction techniques such as Principal Component Analysis (PCA). The concept of neural manifolds and modes in neuroscience]]></summary></entry><entry><title type="html">Understanding Probability Theory: A Primer for Computational Neuroethologists</title><link href="https://snawarhussain.com/blog/probability%20theory/computational%20neuroethology/tutorial/Probabiltiy-Theory_Primer/" rel="alternate" type="text/html" title="Understanding Probability Theory: A Primer for Computational Neuroethologists" /><published>2023-06-08T00:00:00+00:00</published><updated>2023-06-08T00:00:00+00:00</updated><id>https://snawarhussain.com/blog/probability%20theory/computational%20neuroethology/tutorial/Probabiltiy-Theory_Primer</id><content type="html" xml:base="https://snawarhussain.com/blog/probability%20theory/computational%20neuroethology/tutorial/Probabiltiy-Theory_Primer/"><![CDATA[<p>Probability theory is the mathematical framework that underpins statistical analysis. It’s a field that has its roots in the study of gambling and uncertainty, but it’s now an essential tool for a wide range of disciplines. For computational neuroethologists, understanding probability theory is crucial for everything from designing experiments and analyzing data, to building and testing models of neural systems. In this post, we’ll cover the basics of probability theory, using examples from the field of computational neuroethology to illustrate key concepts.</p>

<h2 id="sample-space-and-events">Sample Space and Events</h2>

<p>The <strong>sample space</strong> \(Ω\) is the set of all possible outcomes of a random experiment. Each outcome \(ω ∈ Ω\) can be thought of as a complete description of the state of the world at the end of the experiment. For example, in a study of rat behavior, the sample space might be the set of all possible sequences of a rat’s movements in a maze.</p>

<p>An <strong>event</strong> \(A\) is a subset of the sample space, i.e., \(A ⊆ Ω\). It’s a collection of possible outcomes of an experiment. For example, an event might be that a rat reaches the end of a maze.</p>

<h2 id="probability-measure">Probability Measure</h2>

<p>A <strong>probability measure</strong> \(P\) is a function \(P : F → R\) that assigns a probability to each event. It satisfies the following properties:</p>

<ul>
  <li>\(P(A) ≥ 0\), for all \(A ∈ F\)</li>
  <li>
\[P(Ω) = 1\]
  </li>
  <li>If \(A_1, A_2, ...\) are disjoint events (i.e., \(A_i ∩ A_j = ∅\) whenever \(i ≠ j\)), then \(P(∪_iA_i) = ∑_iP(A_i)\)</li>
</ul>

<p>These are known as the Axioms of Probability.</p>

<h2 id="random-variables">Random Variables</h2>

<p>A <strong>random variable</strong> \(X\) is a function \(X : Ω → R\). Typically, random variables are denoted using upper case letters \(X(ω)\) or more simply \(X\) (where the dependence on the random outcome \(ω\) is implied). The value that a random variable may take on is denoted using lower case letters \(x\).</p>

<p>For example, in a study of rat behavior, the elements of the sample space \(Ω\) are sequences of the rat’s movements. Suppose that \(X(ω)\) is the time it takes for the rat to reach the end of the maze. Given that the maze has a fixed length, \(X(ω)\) can take only a finite number of values, so it is known as a discrete random variable.</p>

<h2 id="cumulative-distribution-functions-cdfs">Cumulative Distribution Functions (CDFs)</h2>

<p>A cumulative distribution function (CDF) is a function \(F_X : R → [0, 1]\) which specifies a probability measure as, \(F_X(x) ≜ P(X ≤ x)\). Here, \(F_X(x)\) is the CDF of the random variable \(X\), \(R\) represents the set of real numbers, and \(P(X ≤ x)\) is the probability that the random variable \(X\) takes a value less than or equal to \(x\).</p>

<h3 id="example">Example</h3>

<p>Consider a rat navigating through a maze. Let \(X\) be the time it takes for the rat to reach the end of the maze. The CDF \(F_X(t)\) gives the probability that the rat will reach the end of the maze in time less than or equal to \(t\).</p>

<p align="center">
<img src="/assets/images/prob_for_neuro/rat_maze.jpg" width="400" />
 <center> <figcaption>Illustration generated by DALL-E</figcaption> </center>
</p>

<h2 id="probability-mass-functions-pmfs">Probability Mass Functions (PMFs)</h2>

<p>When a random variable \(X\) takes on a finite set of possible values (i.e., \(X\) is a discrete random variable), a simpler way to represent the probability measure associated with a random variable is to directly specify the probability of each value that the random variable can assume. In particular, a probability mass function (PMF) is a function \(p_X : Ω → R\) such that</p>

\[p_X(x) ≜ P(X = x)\]

<p>In the case of discrete random variable, we use the notation \(Val(X)\) for the set of possible values that the random variable \(X\) may assume. For example, if \(X(ω)\) is a random variable indicating the number of heads out of ten tosses of coin, then \(Val(X) = {0, 1, 2, . . . , 10}\).</p>

<h3 id="example-1">Example</h3>

<p>Consider a rat navigating through a maze. Let \(X\) be the number of turns it takes for the rat to reach the end of the maze. The PMF \(p_X(k)\) gives the probability that the rat will reach the end of the maze in \(k\) turns.</p>

<h2 id="probability-density-functions-pdfs">Probability Density Functions (PDFs)</h2>

<p>For some continuous random variables, the cumulative distribution function \(F_X(x)\) is differentiable everywhere. In these cases, we define the Probability Density Function or PDF as the derivative of the CDF, i.e.,</p>

\[f_X(x) ≜ \frac{dF_X(x)}{dx}\]

<p>Note here, that the PDF for a continuous random variable may not always exist (i.e., if \(F_X(x)\) is not differentiable everywhere).</p>

<p>According to the properties of differentiation, for very small $∆x$,</p>

\[P(x ≤ X ≤ x + ∆x) ≈ f_X(x)∆x\]

<h3 id="example-2">Example</h3>

<p>Consider a rat navigating through a maze. Let \(X\) be the time it takes for the rat to reach the end of the maze. The PDF \(f_X(t)\) gives the probability density function of the time it takes for the rat to reach the end of the maze.</p>

<h2 id="joint-and-marginal-probability-mass-functions">Joint and Marginal Probability Mass Functions</h2>

<p>If \(X\) and \(Y\) are discrete random variables, then the joint probability mass function \(p_{XY} : R×R → [0, 1]\) is defined by</p>

\[p_{XY}(x, y) = P(X = x, Y = y)\]

<p>Here, \(0 ≤ p_{XY}(x, y) ≤ 1\) for all \(x, y\), and \(\sum_{x∈Val(X)}\sum_{y∈Val(Y)} p_{XY}(x, y) = 1\).</p>

<p>How does the joint PMF over two variables relate to the probability mass function for each variable separately? It turns out that</p>

\[p_X(x) = \sum_{y} p_{XY}(x, y)\]

<p>and similarly for \(p_Y(y)\). In this case, we refer to \(p_X(x)\) as the marginal probability mass function of \(X\). In statistics, the process of forming the marginal distribution with respect to one variable by summing out the other variable is often known as “marginalization”.</p>

<h3 id="example-3">Example</h3>

<p>Consider a rat navigating through a maze. Let \(X\) be the number of turns it takes for the rat to reach the end of the maze, and let \(Y\) be the number of times the rat retraces its steps. The joint PMF \(p_{XY}(k, l)\) gives the probability that the rat will reach the end of the maze in \(k\) turns and retrace its steps l times. The marginal PMF \(p_X(k)\) gives the probability that the rat will reach the end of the maze in \(k\) turns, regardless of how many times it retraces its steps.</p>

<h2 id="joint-and-marginal-probability-density-functions">Joint and Marginal Probability Density Functions</h2>

<p>Let \(X\) and \(Y\) be two continuous random variables with joint distribution function \(F_{XY}\). In the case that \(F_{XY}(x, y)\) is everywhere differentiable in both \(x\) and \(y\), then we can define the joint probability density function as,</p>

\[f_{XY}(x, y) = \frac{\partial^2 F_{XY}(x, y)}{\partial x \partial y}\]

<p>Like in the single-dimensional case, \(f_{XY}(x, y) ≠ P(X = x, Y = y)\), but rather,</p>

\[\int_{x \in A} f_{XY}(x, y) dx dy = P((X, Y) ∈ A)\]

<p>Note that the values of the probability density function \(f_{XY}(x, y)\) are always nonnegative, but they may be greater than 1. Nonetheless, it must be the case that,</p>

\[\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f_{XY}(x, y) dx dy = 1\]

<p>Analogous to the discrete case, we define</p>

\[f_X(x) = \int_{-\infty}^{\infty} f_{XY}(x, y) dy\]

<p>as the marginal probability density function (or marginal density) of \(X\), and similarly for \(f_Y(y)\).</p>

<h3 id="example-4">Example</h3>

<p>Consider a rat navigating through a maze. Let \(X\) be the time it takes for the rat to reach the end of the maze, and let \(Y\) be the speed of the rat. The joint PDF \(f_{XY}(t, v)\) gives the probability density function of the time and speed it takes for the rat to reach the end of the maze. The marginal PDF \(f_X(t)\) gives the probability density function of the time it takes for the rat to reach the end of the maze, regardless of its speed.</p>

<h2 id="conditional-distributions">Conditional Distributions</h2>

<p>Conditional distributions seek to answer the question, what is the probability distribution over \(Y\), when we know that \(X\) must take on a certain value \(x\)? In the discrete case, the conditional probability mass function of \(X\) given \(Y\) is simply:</p>

\[p_{Y|X}(y|x) = \frac{p_{XY}(x, y)}{p_X(x)}\]

<p>assuming that \(p_X(x) ≠ 0\).</p>

<p>In the continuous case, the situation is technically a little more complicated because the probability that a continuous random variable \(X\) takes on a specific value \(x\) is equal to zero. Ignoring this technical point, we simply define, by analogy to the discrete case, the conditional probability density of \(Y\) given \(X = x\) to be:</p>

\[f_{Y|X}(y|x) = \frac{f_{XY}(x, y)}{f_X(x)}\]

<p>provided \(f_X(x) ≠ 0\).</p>

<h3 id="example-5">Example</h3>

<p>Consider a rat navigating through a maze. Let \(X\) be the time it takes for the rat to reach the end of the maze, and let \(Y\) be the speed of the rat. The conditional PDF \(f_{Y|X}(y|x)\)
gives the probability density function of the speed of the rat given that it reaches the end of the maze in time \(t\)</p>

<h2 id="independence">Independence</h2>

<p>Two random variables \(X\) and \(Y\) are independent if \(F_{XY}(x, y)= FX(x)FY (y)\) for all values of \(x\) and \(y\). Equivalently,</p>

<ul>
  <li>For discrete random variables, \(p_{XY}(x, y) = p_X(x)p_Y(y)\) for all \(x ∈ Val(X)\), \(y ∈ Val(Y)\).</li>
  <li>For discrete random variables,
\(p_{Y|X}(y|x) = p_Y(y)\) whenever \(p_X(x) ≠ 0\) for all \(y ∈ Val(Y)\).</li>
  <li>For continuous random variables, \(f_{XY}(x, y) = f_X(x)f_Y(y)\) for all \(x, y ∈ R\).</li>
  <li>For continuous random variables,
\(f_{Y|X}(y|x) = f_Y(y)\) whenever \(f_X(x) ≠ 0\) for all \(y ∈ R\).</li>
</ul>

<p>Independent random variables often arise in machine learning algorithms where we assume that the training examples belonging to the training set represent independent samples from some unknown probability distribution. To make the significance of independence clear, consider a “bad” training set in which we first sample a single training example \((x^{(1)}, y^{(1)})\) from some unknown distribution, and then add \(m - 1\) copies of the exact same training example to the training set. In this case, we have (with some abuse of notation)</p>

\[P((x^{(1)}, y^{(1)}), . . . .(x^{(m)}, y^{(m)})) ≠ \sum_{i=1}^{m} P(x^{(i)}, y^{(i)}).\]

<p>Despite the fact that the training set has size \(m\), the examples are not independent! While clearly the procedure described here is not a sensible method for building a training set for a machine learning algorithm, it turns out that in practice, non-independence of samples does come up often, and it has the effect of reducing the “effective size” of the training set.</p>

<h2 id="bayess-rule">Bayes’s Rule</h2>

<p>A useful formula that often arises when trying to derive expression for the conditional probability of one variable given another, is Bayes’s rule.</p>

<p>In the case of discrete random variables \(X\) and \(Y\),</p>

\[P_{Y|X}(y|x) = \frac{P_{XY}(x, y)}{P_X(x)} = \frac{P_{X|Y}(x|y)P_Y(y)}{\sum_{y' ∈ Val(Y)} P_{X|Y}(x|y')P_Y(y')}.\]

<p>If the random variables \(X\) and \(Y\) are continuous,</p>

\[f_{Y|X}(y|x) = \frac{f_{XY}(x, y)}{f_X(x)} = \frac{f_{X|Y}(x|y)f_Y(y)}{\int_{-\infty}^{\infty} f_{X|Y}(x|y')f_Y(y')dy'}.\]

<h3 id="example-6">Example</h3>

<p>In a Brain-Computer Interface (BCI) experiment, suppose \(X\) is the event that a rat successfully performs a task, and \(Y\) is the event that a specific pattern of neural activity is observed. We are interested in \(P(X|Y)\),
the probability that the rat successfully performs the task given that the specific pattern of neural activity is observed. According to Bayes’ rule, we can calculate this as:</p>

\[P(X|Y) = \frac{P(Y|X)P(X)}{P(Y|X)P(X) + P(Y|\neg X)P(\neg X)}\]

<p>Here, \(P(Y|X)\)
is the probability that the specific pattern of neural activity is observed given that the rat successfully performs the task, \(P(X)\)
is the prior probability that the rat successfully performs the task, \(P(Y|\neg X)\) is the probability that the specific pattern of neural activity is observed given that the rat does not successfully perform the task, and \(P(\neg X)\) is the prior probability that the rat does not successfully perform the task.</p>

<p>This formula allows us to update our belief about the rat’s performance based on the observed neural activity. If the specific pattern of neural activity is highly indicative of successful task performance and the rat is generally successful at performing the task, \(P(X|Y)\)
will be high. If the specific pattern of neural activity is not very indicative of successful task performance or the rat is generally not successful at performing the task, \(P(X|Y)\)
will be lower.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this post, we’ve covered the basics of probability theory, using examples from the field of computational neuroethology to illustrate key concepts. We’ve seen how these concepts can be applied to understand and analyze the behavior of rats in a maze and in a BCI experiment. Understanding these concepts is crucial for designing experiments, analyzing data, and building models in computational neuroethology. I hope that this post has provided a useful introduction to these topics and has sparked your interest in further study.</p>]]></content><author><name>Snawar Hussain</name></author><category term="Blog" /><category term="Probability Theory" /><category term="Computational Neuroethology" /><category term="Tutorial" /><category term="Probability Theory" /><category term="Computational Neuroethology" /><category term="Experimental Design" /><category term="Data Analysis" /><category term="Neural Systems Modeling" /><summary type="html"><![CDATA[This post covers the basics of probability theory, using examples from the field of computational neuroethology. It provides a fundamental understanding of designing experiments, analyzing data, and building/testing models of neural systems.]]></summary></entry></feed>